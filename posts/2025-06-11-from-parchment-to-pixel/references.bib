
@article{borjeson_transfiguring_2024,
	title = {Transfiguring the {Library} as {Digital} {Research} {Infrastructure}: {Making} {KBLab} at the {National} {Library} of {Sweden}},
	volume = {85},
	copyright = {Copyright Love Börjeson, Chris Haffenden, Martin Malmsten, Fredrik Klingwall, Emma Rende, Robin Kurtz, Faton Rekathati, Hillevi Hägglöf, Justyna Sikora},
	issn = {2150-6701},
	shorttitle = {Transfiguring the {Library} as {Digital} {Research} {Infrastructure}},
	url = {https://crl.acrl.org/index.php/crl/article/view/26325},
	doi = {10.5860/crl.85.4.564},
	abstract = {This article provides an account of the making of KBLab, the data lab at the National Library of Sweden (KB). The first part discusses the work involved in establishing a lab as both a physical and a digital site for researchers to use digital collections at previously unimaginable scales. The second part explains how the lab has deployed the library’s collections as data to produce high quality Swedish AI models, which constitute a significant new form of digital research infrastructure. We situate this work in the context of uneven AI coverage for smaller languages, and consider how the lab’s models have contributed to the making of important AI infrastructure for the Swedish language. The conclusion raises the possibilities and challenges involved in continuing this type of library-based AI development.},
	language = {en-US},
	number = {4},
	urldate = {2025-04-08},
	journal = {College \& Research Libraries},
	author = {Börjeson, Love and Haffenden, Chris and Malmsten, Martin and Klingwall, Fredrik and Rende, Emma and Kurtz, Robin and Rekathati, Faton and Hägglöf, Hillevi and Sikora, Justyna},
	month = may,
	year = {2024},
	note = {Number: 4},
	pages = {564},
	file = {Full Text PDF:C\:\\Users\\chrhaf\\Zotero\\storage\\GLN95W72\\Börjeson m. fl. - 2024 - Transfiguring the Library as Digital Research Infr.pdf:application/pdf},
}

@article{nockels_implications_2024,
	title = {The implications of handwritten text recognition for accessing the past at scale},
	volume = {80},
	issn = {0022-0418},
	url = {https://doi.org/10.1108/JD-09-2023-0183},
	doi = {10.1108/JD-09-2023-0183},
	abstract = {Purpose This paper focuses on image-to-text manuscript processing through Handwritten Text Recognition (HTR), a Machine Learning (ML) approach enabled by Artificial Intelligence (AI). With HTR now achieving high levels of accuracy, we consider its potential impact on our near-future information environment and knowledge of the past. Design/methodology/approach In undertaking a more constructivist analysis, we identified gaps in the current literature through a Grounded Theory Method (GTM). This guided an iterative process of concept mapping through writing sprints in workshop settings. We identified, explored and confirmed themes through group discussion and a further interrogation of relevant literature, until reaching saturation. Findings Catalogued as part of our GTM, 120 published texts underpin this paper. We found that HTR facilitates accurate transcription and dataset cleaning, while facilitating access to a variety of historical material. HTR contributes to a virtuous cycle of dataset production and can inform the development of online cataloguing. However, current limitations include dependency on digitisation pipelines, potential archival history omission and entrenchment of bias. We also cite near-future HTR considerations. These include encouraging open access, integrating advanced AI processes and metadata extraction; legal and moral issues surrounding copyright and data ethics; crediting individuals’ transcription contributions and HTR’s environmental costs. Originality/value Our research produces a set of best practice recommendations for researchers, data providers and memory institutions, surrounding HTR use. This forms an initial, though not comprehensive, blueprint for directing future HTR research. In pursuing this, the narrative that HTR’s speed and efficiency will simply transform scholarship in archives is deconstructed.},
	number = {7},
	urldate = {2025-05-22},
	journal = {Journal of Documentation},
	author = {Nockels, Joseph and Gooding, Paul and Terras, Melissa},
	month = jan,
	year = {2024},
	note = {Publisher: Emerald Publishing Limited},
	pages = {148--167},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2025-05-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\chrhaf\\Zotero\\storage\\EZFCEUY7\\Vaswani m. fl. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@incollection{bockerman_kungliga_2025,
	title = {Kungliga bibliotekets latinska medeltida handskrifter : {Proveniens} och katalogisering},
	shorttitle = {Kungliga bibliotekets latinska medeltida handskrifter},
	url = {https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-556596},
	abstract = {Kungliga bibliotekets latinska medeltida handskrifter : Proveniens och katalogisering},
	language = {swe},
	urldate = {2025-05-26},
	publisher = {Uppsala universitet},
	author = {Böckerman, Robin},
	year = {2025},
	pages = {11--34},
	file = {Full Text PDF:C\:\\Users\\chrhaf\\Zotero\\storage\\HGDACNM6\\Böckerman - 2025 - Kungliga bibliotekets latinska medeltida handskrif.pdf:application/pdf},
}

@misc{redmon_you_2015,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {https://arxiv.org/abs/1506.02640v5},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	language = {en},
	urldate = {2025-05-27},
	journal = {arXiv.org},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\chrhaf\\Zotero\\storage\\K8Y56CRJ\\Redmon m. fl. - 2015 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf},
}

@misc{li_trocr_2021,
	title = {{TrOCR}: {Transformer}-based {Optical} {Character} {Recognition} with {Pre}-trained {Models}},
	shorttitle = {{TrOCR}},
	url = {https://arxiv.org/abs/2109.10282v5},
	abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at {\textbackslash}url\{https://aka.ms/trocr\}.},
	language = {en},
	urldate = {2025-05-27},
	journal = {arXiv.org},
	author = {Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
	month = sep,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\chrhaf\\Zotero\\storage\\CAMFEW3T\\Li m. fl. - 2021 - TrOCR Transformer-based Optical Character Recogni.pdf:application/pdf},
}
