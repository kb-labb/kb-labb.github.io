{
  "hash": "af6cf2eeb8a5f2e8dd5b604d2024ce22",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Article segmentation in digitized newspapers\"\ndescription: |\n  When physical newspaper copies are digitized at KB, optical character \n  recognition (OCR) is applied to scanned images of newspaper pages. The\n  OCR software segments the page in to detected boxes of text and images. \n  However, these individual boxes are not further connected as part of articles. \n  This post summarizes a master thesis project undertaken at KBLab with the \n  goal of segmenting articles in newspaper pages.\nauthor:\n  - name: Gustav Henning\n    url: https://github.com/GustavHenning\n    affiliations: \n      - name: KTH\n  - name: Faton Rekathati\n    url: https://github.com/Lauler\n    affiliations: \n      - name: KBLab\n        url: https://www.kb.se/in-english/research-collaboration/kblab.html\ndate: 2022-02-10\nformat:\n  html:\n    embed-resources: false\n    toc: true\n    toc-depth: 3\n    toc-location: left\ndraft: true\n---\n\n\n\n\n\nConverting scanned images of physical newspaper copies to a machine readable format enables them to be explored and accessed in new ways. Automated processes involving OCR (optical character recognition) software are able to successfully detect regions of text in an image and read the printed text off of the page. Unfortunately, the discovered regions of text tend to be organized as individual paragraphs without any information on how they may be interconnected. When researchers and library users perform searches on these corpuses, the returned results are thus in the context of individual blocks of text belonging to a page, as opposed to blocks of text/images connected as part of articles with a suggested reading order. \n\nIn the fall of 2021 KBLab hosted a master thesis project written with the goal of segmenting articles in digitized newspapers. The thesis was written by Gustav Henning, who in this blog post will summarize some of his findings.\n\n## Introduction and purpose\n\n<!--- This is a html comment. \nI'm going to suggest some content for you to include, but you need to flesh it out.\n--->\n\nAs mentioned before, digitized newspaper content is organized in to segmented boxes detected by OCR software. One approach to connect these boxes would be to work with the segmented boxes directly and try to cluster or classify them in to groups. This, however, may have a disadvantage in that OCR software may change or improve in the future. The National Library of Sweden may at some point choose to redo their OCR. Therefore we ideally do not want our article segmentation method to be too dependent of the output of an OCR software.\n\nInstead, I/we have chosen to use object detection, instance/semantic segmentation as a method. \n\nAdvantages: \n\n* Independent of OCR software, does not require the boxes outputted by OCR. \n* Predicts directly on the page level.\n* Can incorporate data from OCR software in the form of text and positional info.\n\n\n### Multimodal input\n\nWhether a region of a newspaper page forms a cohesive article unit is not necessarily only determined by visual cues, but can also benefit from taking into account textual information. Therefore I/we decided to experiment with multimodal inputs. We extended the chosen object detection framework to incorporate textual input by appending... \n\n<!--- Fill in some text and expand on explanation of multimodal inputs --->\n\n### Research questions\n\n<!--- Fill it in so it sounds natural --->\nIn the thesis we were interested in investigating whether\n\n1. Multimodal neural network architectures can outperform unimodal NNs.\n2. There is a difference in how well they perform with changing design/time.\n3. Increasing the amount of annotated data also increases the generalizationperformance of the model.\n\n\n## Data\n\nDescribe the data split. We annotated a training dataset\n\nIn domain, near domain, out of domain. What was the purpose of splitting it this way? Relate to research question nr2: To test how well a model trained on one newspaper and time period can generalize on other similar and not-so-similar newspapers.\n\n<!--- Insert either some markdown tables here or send me csv/picture of table so I can input it --->\n\n### Labels and annotation\nDescribe the different classes/labels. Write something about treating every class as a single publication unit. Briefly describe the annotation process.\n\n<!--- Insert example pics of segmentation/annotation, send me the pics or upload to your git repo branch if you clone this --->\n\n### The effect of more data\n\nDescribe the setup with training only with subsets of the training data set.\n\n<!--- Insert either some markdown tables here or send me csv/picture of table so I can input it --->\n\n## Models\n\nDon't need to describe the models you have used in too much detail. Can be enough to just list them and write something very brief so people can understand the results later on.\n\n<!--- Insert pic/table of list of models --->\n\n## Results\n\n<!--- I'm not going to write anything here, I think you can write about results and relate them to the research questions \nInsert table or send me pics of diagrams you want to use either via mail or upload to git repo branch.\nIndicate where each picture should be placed. --->\n\n## Discussion and suggestions for future improvments\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}