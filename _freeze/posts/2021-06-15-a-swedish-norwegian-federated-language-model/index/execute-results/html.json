{
  "hash": "d24836b7c7cbd2fc9cfedb8fc21c6d4e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A Swedish-Norwegian Federated Language Model\"\ndescription: |\n    We trained a bilingual Swedish-Norwegian\n    [ELECTRA](https://github.com/google-research/electra) language model in\n    a federated setup, showcasing LM training when various corpora cannot be\n    shared directly.\n    The main goal of the project was to validate the feasibility\n    of the approch, as well as to study some key numerical\n    properties affecting the performance of the FL process. \nauthor:\n  - name: Robin Kurtz\n    affiliations: \n      - name: KBLab\n        url: https://www.kb.se/in-english/research-collaboration/kblab.html\ndate: 2021-06-15\nimage: images/HFedAvg.png\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    toc-location: left\n---\n\n\n\n\n\n\n\n## Introduction\n\nLarge transformer-based language models (LMs) have come to dominate the\nstate-of-the-art for many natural language processing (NLP) tasks.\nThese [models](https://huggingface.co/transformers/summary.html), such as BERT\nand GPT, require both large amounts of compute as well as large amounts of\ntextual data.\nLarge tech companies that have been the driving force in the development of \nthese large and steadily growing LMs, scrape the internet to gather huge text\ncorpora for many different genres.\nThese datasets however come with some problems.\nLanguages that are less common on the internet will be underrepresented and\nthe automatic classification of which language the text is actually in is not\nnecessarily very accurate either.\nDue to the size of the data, manual checking is not feasible.\nIncluding any type of text scraped from the internet without checking its \ncontent, will also include texts with undesirable views of racist,\nsexist, or similar nature that can induce certain biases into the final model.\nThe [National Library of Sweden](https://www.kb.se/) (Kungliga Biblioteket --\nKB) has access to vast amounts of digitized newspapers and other texts in\nSwedish, that we used to train a state-of-the-art [Swedish\nBERT](https://github.com/Kungbib/swedish-bert-models) language model.\nIn contrast to text scraped from the internet, our dataset is much more\ncontrolled.\nFor that reason it is a valuable asset for research on language\nmodeling, but  due to the copyright of the original owners of the\nindividual texts we are not able to directly share the data with\nexternal parties in the research community.\n\n<aside>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Electra at the Tomb of Agamemnon, Frederic Leighton c. 1869](images/electra.jpg){width=290}\n:::\n:::\n\n\n\n</aside>\n\n\nIn order to allow others to train new models with their own private and our\nprivate data, we are here exploring the use of _federated machine learning_\n(FL).\nFL is a recent strategy that allows the training of models without directly\nsharing or disclosing the data.\nSimply speaking, the data never leaves the administrative control of the data\nprovider, instead local model updates are computed and combined to form\na global, federated model. \n\nSuch a FL setup would allow multiple national libraries and other maintainers\nof private data, to collaborate in training multilingual LMs without having to\nsort out potential legal problems, as no data is shared.\nWe collaborate with [Scaleout](https://www.scaleoutsystems.com/) and use their\nopen-source FL framework [FEDn](https://github.com/scaleoutsystems/fedn) to\ntrain a Swedish-Norwegian ELECTRA language model.\n\nYou can read more about this project at\n[KBLab](https://www.kb.se/samverkan-och-utveckling/nytt-fran-kb/nyheter-samverkan-och-utveckling/2021-02-12-pilotstudie-om-federativt-tranade-sprakmodeller.html),\n[Scaleout](https://www.scaleoutsystems.com/post/federated-learning-and-language-models),\nand [AI Sweden](https://www.ai.se/en/node/81535/pilot-study-federated-language-models-swedish).\n\n## What is ELECTRA?\n\n\n[ELECTRA](https://arxiv.org/pdf/2003.10555.pdf) is a transformer-based _masked\nlanguage model_ (MLM) similar to its predecessor BERT.\nIn contrast to classical LMs, now often referred to as _causal language models_\n(CLMs), that are trained by predicting the next token in a sequence, an MLM is\ntrained by reconstructing the original sequence given a corrupted input\nsequence.\nIn the original BERT model this is done by randomly masking out 15% of the\ninput:\n\n\n> __Input:__ The `[MASK]` sat on the mat.\n>\n> __Output:__ The cat sat on the mat.\n\nBy learning to predict missing tokens, the model learns to imitate not only the\nstructure of language in form of fitting syntax, but also which words and\nphrases have similar meaning by the contexts they have been used in the\ndataset.\n\nGiven that only 15% of the input tokens are masked and thus used for training\nthe model, this approach is somewhat inefficient.\nWhile the network structure of ELECTRA is essentially the same as BERT's, its\ntraining objective promises to be more sample-efficient.\nInstead of training to predict some masked out tokens, ELECTRA learns to\npredict for each token whether it belongs to the original input sequence or\nif it was generated by a secondary model.\nThis secondary model, the _generator_, is trained in tandem with the primary\nmodel, the _discriminator_, quite similar to generative adversarial networks\n(GANs).\n\n\n\n> __Input:__ The dog sat on the mat.\n>\n> __Output:__  ‚úîÔ∏è ‚ùå ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è\n\nWith this new objective ELECTRA is able to outperform BERT, essentially \napplying the MLM objective to every input token.\n\n\n## What is Federated Machine Learning?\n\nFederated learning is a technique used when a model needs to be trained on\nmultiple datasets that cannot be pooled.\nThere are two general use-case scenarios for FL: Cross-silo and\ncross-device.\n\n__Cross-device__ is a scenario where there are too many small devices, such as\nmobile or edge devices, that provide a constant stream of outputs.\nIn contrast to this, __cross-silo__ involves few, more powerful machines that\nhandle datasets that cannot be shared due to privacy concerns or legal\nrestrictions.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Schema for the hierarchical federated learning architecture implemented in FEDn.](images/HFedAvg.png){width=848}\n:::\n:::\n\n\n\n\nThe FL framework [FEDn](https://arxiv.org/pdf/2103.00148.pdf) is designed to\nsupport scalable FL using a tiered, hierarchical architecture.\nIt is based on services taking four principal roles: i) controller,\nii) reducer, iii) combiner, and iv) client.\nAt the lowest level of this hierarchical structure, local models with local\ndata are trained on multiple geographically distributed _client_ nodes.\nThese local models are then, after a certain number of training updates, sent\nto one or more _combiners_ that coordinate the updates from their own subset of\n_clients_.\nThese partial model updates are then _reduced_ into a single global model and\nredistributed to clients for the next training round, according to a reducer\nprotocol (currently all-reduce).\nFinally, the _controller's_ responsibility is to coordinate the overall\ncomputation and to maintain the immutable trail of global models. \n\nThe update scheme used to combine the local models into one global model is\ncalled [federated averaging (FedAvg)](https://arxiv.org/pdf/1602.05629.pdf),\none of the most widely used methods for FL.\nIn each round the current version of the global model is distributed to the\nclients that continue training using each their own data.\nAfter one local round of training the distributed clients' model-weights are\nsent back to the server that simply averages the weights, while taking the\nnumber of local updates into account.\n\n## Experimental Setup\n\nWith the future goal to train a large Scandinavian transformer-based language\nmodel, we downscale the size of the model and data to be able to efficiently\ntest different hyper-parameter settings.\nWe choose to train a small ELECTRA model using publicly available data from the\n[OSCAR corpus](https://oscar-corpus.com/) and Wikipedia, for Swedish, and\nNorwegian _bokm√•l_ and _nynorsk_.\nThe Swedish corpus is 27 GB, about five times larger than the Norwegian corpus.\nThis uneven distribution allows us to additionally investigate whether an LM\nbuilt on little data can benefit from a similar language's data.\n\nDue to the rather small size of the ELECTRA model, we are able to train using\nstandard workstation GPUs.\nOur federated setup consists of three workstations plugged into the same\nnetwork, two of which serving as local clients, doing the majority of\ncomputational work training the local model instances on GPU, and one\nworkstation taking care of collecting, averaging, and redistributing the\nmodels.\n\nTraining large-scale transformer-based language models heavily relies on the\ncorrect choice of hyper-parameters, for the model as well as the optimizer.\nWe follow the settings of the original small ELECTRA models in English, and\nfocus only on choosing the correct federated learning strategy.\n\n### Convergence as a function of local update steps \n\nIn order to obtain good FL performance, we need to balance communication\noverhead and convergence.\nThis entails doing as many local model updates (i.e. gradient steps) as\npossible (more update steps means fewer global rounds), without letting the\nlocal models diverge from one another too far (large divergence before\naggregation leads to lower convergence rate).\nFor example, updating after 100 gradient steps will keep divergence to\na minimum and require fewer gradient steps in total to converge, but will, due\nthe communication overhead in global rounds, need much longer actual wall-time\nto reach a certain accuracy level, compared to models communicating their\nupdates after every 1000 local gradient steps.\nOn the other hand, taking too many local gradient steps will manage to do more\ngradient steps in a shorter amount of time, but need many more updates and thus\ntime to reach convergence.\n\n\n### The Role of the optimizer\n\nWith FedAvg we typically only consider model parameters, but large transformer\nneural networks generally need more advanced optimization methods than simple\nstochastic gradient descent.\nIn most cases the _Adam_ (Adaptive Moment Estimation) optimizer is used, which \ncomputes adaptive learning rates for each parameter, storing both the mean and\nthe variance of the gradients.\nThese additional parameters depend on the model parameters, meaning that they\nshould be averaged as well and redistributed to the clients.\nThis however increases the size of the data package that has to be sent by a\nfactor of three, which can be significant when larger models are trained that\n\"weigh\" multiple gigabytes.\nWe test how the development of the loss is affected by keeping the optimizer\nspecific parameters local versus averaging them the same way as regular model\nparameters.\n\n## Results\n\nTo evaluate the impact of changing various hyper-parameters, we focus on the\ndevelopment of the loss function during training.\nWhile it seems easy to evaluate large language models, as one can simply use\nthe [GLUE](https://gluebenchmark.com/) or\n[SuperGLUE](https://super.gluebenchmark.com/) benchmarks to get an overall\nperformance evaluation, there are many tricks one needs to apply to gain better\nscores.\nEven simply changing the random seed can [increase or decrease\nperformance](https://arxiv.org/pdf/2002.06305.pdf) by multiple points.\n\nWhile we do not evaluate downstream model performance, we clearly see how the\ntraining is affected.\n\n### Number of local updates\n\nIn our first set of experiments we investigate how various local round lengths\naffect the training progress.\nWe try four different local round lengths, with 100, 1000, 2000, and 5000 \ngradient steps before recombining the models.\n\n\n\n\n::: {.cell layout=\"l-page\"}\n::: {.cell-output-display}\n![](images/round_lengths_steps.png){width=684}\n:::\n:::\n\n\n\n\nWhile the loss decreases the most per steps taken when the model is updated as\noften as possible (i.e. 100 steps), it takes far longer than in the other\nsetups to reach the same loss values.\nIncreasing the local round length to 5000 gradient steps allows us to do the\nmost gradient steps in the shortest amount of time, but results in the loss\nnot decreasing as quickly as with for example 1000 steps per round.\n\n\n\n\n::: {.cell layout=\"l-page\"}\n::: {.cell-output-display}\n![](images/round_lengths_time_long.png){width=684}\n:::\n:::\n\n\n\n\nIn this scenario we finally settle for 1000 steps per round, giving us the best\nspeed-performance trade-off.\nWith real-world models being much larger than the one used in our experiments,\nit could be interesting to change the round length during training.\nLonger round length in the beginning allows the model to see more data, while\nshorter round lengths towards the end will help the model to converge.\n\n### Local vs. global optimizer\n\nUsing a more advanced optimizer such as\n[Adam](https://ruder.io/optimizing-gradient-descent/index.html#adam) is\nnecessary when training models with parameters now regularly surpassing\nmultiple billions.\nThis unfortunately means that the number of parameters that we need to\nfederate triples, which increases the communication overhead.\nIn order to test whether it is enough to only federate the model parameters\nthemselves while keeping the optimizer states local, we train our small\nELECTRA model with the additional Adam parameters retaining their local states,\nand averaging them just as the regular model parameters.\n\n\n\n\n::: {.cell layout=\"l-page\"}\n::: {.cell-output-display}\n![](images/local_v_global_steps.png){width=684}\n:::\n:::\n\n\n\n\nWe can see that averaging the optimization-specific parameters allows the loss\nto decrease further, without taking much more time.\nWhile keeping the optimization parameters local increases the speed a little\nbit (the green curve in the figure above is slightly longer), it is not enough\nto counteract the decrease in learning.\n\n\n\n\n::: {.cell layout=\"l-page\"}\n::: {.cell-output-display}\n![](images/local_v_global_time.png){width=684}\n:::\n:::\n\n\n\n\nThese results show that keeping outdated optimization parameters to increase \nthe overall speed is not desirable.\nFor larger models we might see a significant increase in speed, but it might\nthen be a better idea to change the optimization algorithm to regular\nstochastic gradient descent, to avoid faulty inputs.\nSimilarly to dynamically changing the round lengths, adding a smarter\noptimization algorithm towards the end can be a possibility.\n\n\n\n## Continuation\n\nThis project has given us some promising first results towards training large\nlanguage models such as ELECTRA.\nUsing a federated black-box approach as implemented in FEDn, gives us the \npossibility to train models with other non-public data holders, but also gives\nothers the possibility to train their models with our data.\n\nThe models we trained are however only of one type and relatively small.\nWe are working on implementing an interface to the [ü§ó\nTransformers](https://huggingface.co/transformers/) library, that will allow\nusers to train LMs from scratch in a federated fashion, but also fine-tune\nthese models using the same functionalities.\nWe hope that training models larger than our small ELECTRA, will give us more\ninsights into how long we should train locally and whether to change the \noptimization strategy.\n\nWith these pieces in place, we finally hope to train a large Scandinavian\nlanguage model that combines data sources that so far could not have been\ncombined.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}