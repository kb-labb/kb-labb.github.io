[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The KBLab Blog",
    "section": "",
    "text": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab\n\n\nHow well do current HTR models handle the complexity of medieval Latin handwriting? At KBLab, we‚Äôve been testing and comparing the state of the art in openly available‚Ä¶\n\n\n\nJustyna Sikora, Chris Haffenden, Robin B√∂ckerman\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome KB-Whisper, a new fine-tuned Swedish Whisper model!\n\n\nKBLab proudly presents KB-Whisper, a speech to text model fine-tuned using 50,000 hours of transcribed speech. This work reports an overall improvement across model sizes‚Ä¶\n\n\n\nLeonora Vesterbacka, Faton Rekathati, Robin Kurtz, Justyna Sikora, Agnes Toftg√•rd\n\n\nMar 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreserving the history of cultural heritage conservation\n\n\nWhile KBLab uses modern heritage data to produce cutting edge AI models, we also work with developing the library‚Äôs research infrastructure for older heritage material and‚Ä¶\n\n\n\nChris Haffenden, Emil Stenback\n\n\nJan 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of financial data at the Financial Supervisory Authority\n\n\nA guest post from the Swedish Financial Supervisory Authority (Finansinspektionen, FI), where FI describe how they use machine learning to analyze financial data and perform‚Ä¶\n\n\n\nJimmy Holl√©n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnearthing forgotten images with the help of AI\n\n\nHow can new AI methods be used to improve the searchability and accessibility of visual heritage collections? We‚Äôve taken advantage of the possibilities opened up by‚Ä¶\n\n\n\nChris Haffenden, Faton Rekathati, Emma Rende\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWords unboxed: discovering new words with Kubord\n\n\nKBLab together with Spr√•kbanken Text have developed 75 freely available datasets to support research in, but not limited to, lexicography. The datasets, marked as Kubord 2‚Ä¶\n\n\n\nMarkus Forsberg, Justyna Sikora, Emma Sk√∂ldberg\n\n\nAug 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor how long is a person recognisable by their voice?\n\n\nSearching a database of speakers by their voice presents a unique challenge, as speakers‚Äô voices change as they age. We can represent a speaker‚Äôs voice computationally with‚Ä¶\n\n\n\nMaya Nachesa\n\n\nJul 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nA robust, multi-label sentiment classifier for Swedish\n\n\nKBLab presents a robust, multi-label sentiment classifier trained on Swedish texts. The model is robust in the sense that it is trained on multiple datasets of different‚Ä¶\n\n\n\nHillevi H√§ggl√∂f\n\n\nJun 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwedish speech synthesis\n\n\nKBLab releases a neural network based text-to-speech model for Swedish. The model was trained on an open Swedish speech synthesis dataset from NST. We make our latest‚Ä¶\n\n\n\nFaton Rekathati\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nScientific discourse with BERTopic\n\n\nWe describe a typical topic modeling use case where BERTopic is applied to scientific abstracts in the research field of education. We discuss the limitations of BERTopic‚Ä¶\n\n\n\nHillevi H√§ggl√∂f, Justyna Sikora\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nRixVox: A Swedish Speech Corpus with 5500 Hours of Speech from Parliamentary Debates\n\n\nKBLab releases RixVox, a speech dataset comprised of 5500 hours of speech from parliamentary debates. The speeches have been aligned with transcripts from written protocols‚Ä¶\n\n\n\nFaton Rekathati\n\n\nMar 9, 2023\n\n\n\n\n\n\n\n\n\n\n\nFinding Speeches in the Riksdag‚Äôs Debates\n\n\nThe Riksdag is the Parliament of Sweden. It has made available twenty years of parliamentary debates through its website and open data platform. Each speech is accompanied‚Ä¶\n\n\n\nFaton Rekathati\n\n\nFeb 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwedish zero-shot classification model\n\n\nKBlab has released a BERT model fine-tuned on NLI tasks, which can be used for zero-shot text classification.\n\n\n\nJustyna Sikora\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwedish Sentence Transformer 2.0\n\n\nKBLab‚Äôs Swedish sentence transformer has been updated to a newer version. The new version features an increased maximum sequence length of 384 tokens, allowing users to‚Ä¶\n\n\n\nFaton Rekathati\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nBERTopic for Swedish: Topic modeling made easier via KB-BERT\n\n\nTopic modeling is an exciting option for exploring and finding patterns in large volumes of text data. While this previously required considerable programming skills, a‚Ä¶\n\n\n\nElena Fano, Chris Haffenden\n\n\nJun 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Swedish Language Models\n\n\nWe present OverLim, a new benchmark for evaluating large language models for Swedish, Danish, and Norwegian, created by translating a subset of the GLUE and SuperGLUE‚Ä¶\n\n\n\nRobin Kurtz\n\n\nMar 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwedish Bootleg model\n\n\nWe at KBLab have trained a Swedish version of an entity disambiguation model called Bootleg, developed by the Hazy Research Lab at Stanford. The model is trained on Swedish‚Ä¶\n\n\n\nElena Fano\n\n\nMar 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSUCX 3.0 - NER\n\n\nWe present a remix of the venerable SUC 3.0 dataset for Swedish Named Entity Recognition (NER), and explore the effect of Hyper Parameter Optimization (HPO) for this task‚Ä¶\n\n\n\nRobin Kurtz, Joey √ñhman\n\n\nFeb 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nKBLab publishes an article about AI in the library in C&RL\n\n\nWhat does AI mean for libraries and how could libraries pave the way for ethical AI? KBLab reflects on these questions in an article in the Open Access journal College &‚Ä¶\n\n\n\nElena Fano, Chris Haffenden\n\n\nJan 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nKBLab‚Äôs director Love B√∂rjeson nominated as AI Swede of the year 2021\n\n\nWe are very pleased to announce that our director Love B√∂rjeson has been nominated for the prestigious prize √Örets AI Svensk 2021 (AI Swede of the year 2021) which is‚Ä¶\n\n\n\nElena Fano\n\n\nNov 15, 2021\n\n\n\n\n\n\n\n\n\n\n\nIntroducing a Swedish Sentence Transformer\n\n\nWhile language models such as BERT are effective at many tasks, they have limited use when it comes to information retrieval and large scale similarity comparisons. In this‚Ä¶\n\n\n\nFaton Rekathati\n\n\nAug 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Swedish-Norwegian Federated Language Model\n\n\nWe trained a bilingual Swedish-Norwegian ELECTRA language model in a federated setup, showcasing LM training when various‚Ä¶\n\n\n\nRobin Kurtz\n\n\nJun 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic models for Statens Offentliga Utredningar\n\n\nWe built some topic models on the National Library‚Äôs SOU collection, as an example of what can be done with the Library‚Äôs materials. The curated dataset can be a useful‚Ä¶\n\n\n\nElena Fano\n\n\nMay 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nA multimodal approach to advertisement classification in digitized newspapers\n\n\nThe process of digitizing historical newspapers at the National Library of Sweden involves scanning physical copies of newspapers and storing them as images. In order to‚Ä¶\n\n\n\nFaton Rekathati\n\n\nMar 28, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-03-03-swedish-bootleg-model/index.html",
    "href": "posts/2022-03-03-swedish-bootleg-model/index.html",
    "title": "Swedish Bootleg model",
    "section": "",
    "text": "Named Entity Disambiguation (NED) is the task to determine which specific entity in a knowledge base a detected named entity refers to. For example, take the sentence ‚ÄúI bought a new Lincoln‚Äù: given Wikipedia as a knowledge base, the task of the NED system is to link the named entity ‚ÄúLincoln‚Äù to the page ‚ÄúLincoln Motor Company‚Äù and not to ‚ÄúLincoln, Nebraska‚Äù or ‚ÄúAbraham Lincoln‚Äù. This is a rather complex task, because there is very little context to base the disambiguation on, and nowhere in the sentence there is any reference to cars. It is especially tricky to disambiguate rare entities, i.e.¬†entities that appear very few times in the data or not at all.\nYet there is very little doubt in any human mind that I didn‚Äôt buy an American president or a city in Nebraska. So how do humans do it? We base our disambiguation on the fact that you don‚Äôt buy people or cities, you buy objects. In more abstract terms, we base our decision on the type of the entity, and other similar reasoning patterns.\nBootleg is NED system that aims to improve disambiguation on rare entities, so-called ‚Äútail‚Äù entities (because they constitute the long tail of the frequency distribution). Developed by researchers at Hazy Research Lab, Bootleg is a transformer-based system that introduces reasoning patterns based on entity type and relations in the disambiguation process. Even if an entity is rare, its type probably isn‚Äôt; for example names of different car brands tend to be used much the same way in natural language. Similarly, some adjectives only refer to specific types of entities: you wouldn‚Äôt say that a person is long, but you can say that they are tall.\n\n\n\n\n\n\nFigure¬†1: Bootleg architecture\n\n\n\nBootleg is trained on Wikidata and Wikipedia, that exist for a variety of the world‚Äôs languages, and achieves much better results on tail entities compared to a vanilla BERT-based entity disambiguation system (see Figure¬†1). It is an end-to-end system that can detect named entities in running text and disambiguate them, giving as output the unique identifiers of the candidate entities in the knowledge base.\nThe original Bootleg paper can be found here, as well the official repository and the blog post from the Stanford AI Lab."
  },
  {
    "objectID": "posts/2022-03-03-swedish-bootleg-model/index.html#what-is-bootleg",
    "href": "posts/2022-03-03-swedish-bootleg-model/index.html#what-is-bootleg",
    "title": "Swedish Bootleg model",
    "section": "",
    "text": "Named Entity Disambiguation (NED) is the task to determine which specific entity in a knowledge base a detected named entity refers to. For example, take the sentence ‚ÄúI bought a new Lincoln‚Äù: given Wikipedia as a knowledge base, the task of the NED system is to link the named entity ‚ÄúLincoln‚Äù to the page ‚ÄúLincoln Motor Company‚Äù and not to ‚ÄúLincoln, Nebraska‚Äù or ‚ÄúAbraham Lincoln‚Äù. This is a rather complex task, because there is very little context to base the disambiguation on, and nowhere in the sentence there is any reference to cars. It is especially tricky to disambiguate rare entities, i.e.¬†entities that appear very few times in the data or not at all.\nYet there is very little doubt in any human mind that I didn‚Äôt buy an American president or a city in Nebraska. So how do humans do it? We base our disambiguation on the fact that you don‚Äôt buy people or cities, you buy objects. In more abstract terms, we base our decision on the type of the entity, and other similar reasoning patterns.\nBootleg is NED system that aims to improve disambiguation on rare entities, so-called ‚Äútail‚Äù entities (because they constitute the long tail of the frequency distribution). Developed by researchers at Hazy Research Lab, Bootleg is a transformer-based system that introduces reasoning patterns based on entity type and relations in the disambiguation process. Even if an entity is rare, its type probably isn‚Äôt; for example names of different car brands tend to be used much the same way in natural language. Similarly, some adjectives only refer to specific types of entities: you wouldn‚Äôt say that a person is long, but you can say that they are tall.\n\n\n\n\n\n\nFigure¬†1: Bootleg architecture\n\n\n\nBootleg is trained on Wikidata and Wikipedia, that exist for a variety of the world‚Äôs languages, and achieves much better results on tail entities compared to a vanilla BERT-based entity disambiguation system (see Figure¬†1). It is an end-to-end system that can detect named entities in running text and disambiguate them, giving as output the unique identifiers of the candidate entities in the knowledge base.\nThe original Bootleg paper can be found here, as well the official repository and the blog post from the Stanford AI Lab."
  },
  {
    "objectID": "posts/2022-03-03-swedish-bootleg-model/index.html#training-and-evaluating-the-swedish-model",
    "href": "posts/2022-03-03-swedish-bootleg-model/index.html#training-and-evaluating-the-swedish-model",
    "title": "Swedish Bootleg model",
    "section": "Training and evaluating the Swedish model",
    "text": "Training and evaluating the Swedish model\nSince the only pre-trained Bootleg model available from Hazy Research is an English model, we decided to train a Swedish Bootleg on the Swedish part of Wikipedia. The authors of the paper provided us with data pre-processing and training scripts, and the system is also well documented here. Since Swedish Wikipedia is much smaller than the English (1.5 GB versus 20 GB compressed size), we were able to train the Swedish Bootleg on a single GPU in about two to three days, but we also expected to get a worse model.\nFor evaluation we also used the scripts provided by the authors and a test set consisting of Wikipedia articles that the model had never seen before. We obtained the following results:\nprecision = 2024911 / 2052099 = 0.987\nrecall = 2024911 / 2067105 = 0.979\nf1 = 0.983\nThese results might seem very good but they are somewhat misleading for a couple of reasons. First, this is exactly the same type of material that the model was trained on, so of course the style and register are very similar and this makes the inference task easier. Second, for any entity linking system there is a fair share of entities that only have one possible candidate in the knowledge base, which means that the model will be right every time on these entities regardless of how good it actually is.\nA more accurate sense of how well the model performs can be gained by manually testing it on different kinds of material and observing the resulting mistakes. The first step in predicting new data is isolating the mentions to be linked, and this is done through the mention extraction pipeline. This finds entities in the text that correspond to candidate entities in the knowledge base, and ensures that every entity selected for linking has at least one candidate to link to. We substituted the default English spaCy model with our own Swedish model and ran the inference on some test sentences. We observed that the model can indeed perform type disambiguation, as in the following example:\n\n\n\n\n\n\nFigure¬†2: Bootleg predictions for the sentence ‚ÄúI bought a new Lincoln‚Äù\n\n\n\nThe model understands that the first sentence must be about a person and links to ‚ÄúAbraham Lincoln‚Äù, whereas the second is probably about an object and picks the automobile brand ‚ÄúLincoln‚Äù, which is correct. We report some more examples of sentences and the predicted entities to show the model‚Äôs behavior and follow up with some comments.\nSentence: ‚ÄúDuplantis bel√∂nades med Svenska Dagbladets bragdguld 2020 f√∂r sina dubbla v√§rldsrekord och f√∂r att dessutom ha noterat det h√∂gsta hoppet n√•gonsin utomhus, 6,15.‚Äù\nEntities: [‚ÄòAndreas Duplantis‚Äô, ‚ÄòSvenska Dagbladets guldmedalj‚Äô, ‚ÄòFriidrottsrekord‚Äô, ‚ÄòHopp (dygd)‚Äô]\nSentence: ‚ÄúEkonomer vid internationella valutafonden IMF har r√§knat ut att varje storvuxen bardval enbart genom sin f√∂rm√•ga att binda kol utf√∂r en nytta v√§rd minst 20 miljoner kronor.‚Äù\nEntities: [‚ÄòInternationella valutafonden‚Äô, ‚ÄòInternationella valutafonden‚Äô, ‚ÄòBardvalar‚Äô, ‚ÄòKol (br√§nsle)‚Äô, ‚ÄòSvensk krona‚Äô]\nSentence: ‚ÄúElhandelsbolagen i Norden k√∂per sin el p√• den nordiska elhandelsb√∂rsen Nord Pool och vad det kostar beror p√• en m√§ngd olika faktorer.‚Äù\nEntities: [‚ÄòNorden‚Äô, ‚ÄòNord Pool‚Äô]\nClearly the main problem is that as long as there is an entry for it in Wikidata, anything is considered an entity. Depending on the application, we might not want everyday objects and concepts like ‚Äúkol‚Äù and ‚Äúfriidrottsrekord‚Äù to be treated as name entities. We have already applied a filter that removes from the entity database those aliases that appear very often in Wikipedia texts without being marked as mentions, the reasoning being that these aliases point to common words that are not named entities. Still, the results are a bit noisy. This issue could be probably solved by running a Named Entity Recognition model on the same data, for example kb-bert-ner, and ignoring all the entities in the Bootleg output that the NER model doesn‚Äôt recognize as such.\nThere are also some actual mistakes, for example predicting ‚ÄúAndreas Duplantis‚Äù instead of his brother Armand Duplantis, but it would require too much world knowledge for the model to know which one of the two pole-vaulting brothers recently set a world record. In the same sentence, ‚Äúhopp‚Äù is identified as the Christian virtue of ‚Äúhope‚Äù, which is incorrect, but maybe justified by the fact that ‚Äúh√∂gsta hoppet‚Äù (highest hope/jump) can be a good collocation for both senses of the word."
  },
  {
    "objectID": "posts/2022-03-03-swedish-bootleg-model/index.html#usage-and-examples",
    "href": "posts/2022-03-03-swedish-bootleg-model/index.html#usage-and-examples",
    "title": "Swedish Bootleg model",
    "section": "Usage and examples",
    "text": "Usage and examples\nWe are working on making the Swedish model and the entity database available for download, as well as providing ready-made scripts for inference in multilingual settings. Stay tuned for more information!"
  },
  {
    "objectID": "posts/2022-03-03-swedish-bootleg-model/index.html#acknowledgements",
    "href": "posts/2022-03-03-swedish-bootleg-model/index.html#acknowledgements",
    "title": "Swedish Bootleg model",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany thanks to Laurel Orr at the Hazy Research Lab for assisting in every step of the data processing and training, both with scripts and insightful tips."
  },
  {
    "objectID": "posts/2022-03-16-evaluating-swedish-language-models/index.html",
    "href": "posts/2022-03-16-evaluating-swedish-language-models/index.html",
    "title": "Evaluating Swedish Language Models",
    "section": "",
    "text": "In the last couple of years the field of natural language processing (NLP) has seen the rise of larger and larger language models (LMs), such as BERT, GPT, and others, based on the transformer architecture. These massive models are trained on a simple language modeling task, guessing words using the their previous or surrounding context. Due to the large number of parameters and the amounts of text the models see during training, they learn both syntactic and semantic patterns of words in contexts and thus to some extent language, resembling natural language understanding (NLU). From a practical point of view, these models serve as the basis for specialist models that are finetuned to solve a specific task, such as sentiment analysis, named entity recognition, and more. The GLUE and SuperGLUE benchmark suites are a collection of tasks designed to require some form of NLU to outperform a human baseline. With these benchmarks, model creators can easily test the usefulness of their new models on a wide range of tasks, while comparing the performance to other similar models. For the three Scandinavian languages Swedish, Danish, and Norwegian there are so far only few such tasks, that can be used to evaluate a transformer LM‚Äôs downstream performance. With the introduction of OverLim we hope to provide a simple benchmark to help assess the quality of the model, while comparing its performance to others. Due to its nature of being an automatically translated dataset, any performance differences should be interpreted carefully, as these models tend to learn with the help of data artifacts unrelated to NLU, of which the automatic translation might introduce even more. A more serious effort of a benchmark suite for Swedish is done in the SuperLim project, which is currently working on creating training data to accompany their test sets.\n\n\n\n\n\nQuality! Photographer: Jens Gustavsson/KB"
  },
  {
    "objectID": "posts/2022-03-16-evaluating-swedish-language-models/index.html#introduction",
    "href": "posts/2022-03-16-evaluating-swedish-language-models/index.html#introduction",
    "title": "Evaluating Swedish Language Models",
    "section": "",
    "text": "In the last couple of years the field of natural language processing (NLP) has seen the rise of larger and larger language models (LMs), such as BERT, GPT, and others, based on the transformer architecture. These massive models are trained on a simple language modeling task, guessing words using the their previous or surrounding context. Due to the large number of parameters and the amounts of text the models see during training, they learn both syntactic and semantic patterns of words in contexts and thus to some extent language, resembling natural language understanding (NLU). From a practical point of view, these models serve as the basis for specialist models that are finetuned to solve a specific task, such as sentiment analysis, named entity recognition, and more. The GLUE and SuperGLUE benchmark suites are a collection of tasks designed to require some form of NLU to outperform a human baseline. With these benchmarks, model creators can easily test the usefulness of their new models on a wide range of tasks, while comparing the performance to other similar models. For the three Scandinavian languages Swedish, Danish, and Norwegian there are so far only few such tasks, that can be used to evaluate a transformer LM‚Äôs downstream performance. With the introduction of OverLim we hope to provide a simple benchmark to help assess the quality of the model, while comparing its performance to others. Due to its nature of being an automatically translated dataset, any performance differences should be interpreted carefully, as these models tend to learn with the help of data artifacts unrelated to NLU, of which the automatic translation might introduce even more. A more serious effort of a benchmark suite for Swedish is done in the SuperLim project, which is currently working on creating training data to accompany their test sets.\n\n\n\n\n\nQuality! Photographer: Jens Gustavsson/KB"
  },
  {
    "objectID": "posts/2022-03-16-evaluating-swedish-language-models/index.html#creation-and-content",
    "href": "posts/2022-03-16-evaluating-swedish-language-models/index.html#creation-and-content",
    "title": "Evaluating Swedish Language Models",
    "section": "Creation and Content",
    "text": "Creation and Content\nFrom the 10 main GLUE tasks and the 8 SuperGLUE tasks (not counting the diagnostic datasets), we choose the following 11 tasks:\n\nMNLI ‚Äì Multi NLI\nMRPC ‚Äì Microsoft Reasearch Paraphrase Corpus\nQNLI ‚Äì Question-answering NLI\nQQP ‚Äì Quora Question Pairs\nRTE ‚Äì Recognizing Textual Entailment\nSST ‚Äì Stanford Sentiment Treebank\nSTS-B ‚Äì Semantic Textual Similarity Benchmark\nWNLI ‚Äì Winograd NLI\nBoolQ ‚Äì Boolean Questions\nCB ‚Äì Commitment Bank\nCOPA ‚Äì Choice of Plausible Alternatives\n\nThe translations were done using Marian-NMT and the models for Swedish, Danish, and Norwegian bokm√•l provided by Opus-MT.\nThe original GLUE and SuperGLUE datasets use a test set, which does not have its labels published, as competitors send in their results on the test set, while the benchmark maintainers then evaluate on the hidden labels to avoid cheating by pre-evaluating on the test set, or even training on it. This means that we only have labels for the training and development splits. We use the former development split as the new test split and divide the training split with an 80-20 distribution into a new training and development split. Comparing the results to the English GLUE and SuperGLUE results is therefore difficult, given the smaller training set as well as the different test set."
  },
  {
    "objectID": "posts/2022-03-16-evaluating-swedish-language-models/index.html#evaluation",
    "href": "posts/2022-03-16-evaluating-swedish-language-models/index.html#evaluation",
    "title": "Evaluating Swedish Language Models",
    "section": "Evaluation",
    "text": "Evaluation\nTogether this new dataset we want to give potential users some simple baselines and an easy way to evaluate their own models, and present some of our new and upcoming work-in-progress.\nWe evaluate the following models:\n\nAI-Sweden BERT-large\nmBERT-base\nELECTRA-base\nSentence-BERT\nKB-BART\nKB-BERT\nBERT ü§ó\nMegatron-BERT-base 125k\nMegatron-BERT-base 600k\nMegatron-BERT-large 110k\n\nKB-BART and the last four models in that list use the same amount of data, roughly 70GB. BERT ü§ó was trained using the huggingface ü§ó framework, concatenating multiple short documents into one long sequence, additionally to the traditional splitting of documents longer than 512 tokens into multiple sequences. The two Megatron-BERT-base models use the same setup, with one being trained for 125k steps and the other for 600k steps, to test the impact of longer training times. Finally, our large BERT model, also using the Megatron-LM framework, is only an intermediate model checkpoint. The intended training time is set to 500k steps and will be continued in the foreseeable future.\nEach model was trained and evaluated five times with varying seeds. The final models were chosen according to their performance on the development set. The results below on the test set can therefore in some cases be lower than if the best model had been chosen directly with respect to test-set performance. This does not change the order of the best models with the exception for the RTE task. Some of the sub-tasks return two evaluation measures, accuracy and F-score, which are averaged here for simplicity. For the moment we do not evaluate on COPA.\nThe training- and evaluation script can be downloaded via the git repository.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nmnli\nmrpc\nqnli\nqqp\nrte\nsst\nstsb\nwnli\nboolq\ncb\n\n\n\n\nAI-Sweden BERT-large\n83.49%\n86.67%\n90.10%\n68.69%\n70.40%\n91.28%\n87.61%\n46.48%\n70.28%\n59.14%\n\n\nmBERT-base\n78.40%\n81.73%\n86.97%\n69.17%\n67.15%\n89.22%\n81.93%\n43.66%\n65.96%\n58.95%\n\n\nELECTRA-base\n78.18%\n76.66%\n84.07%\n60.68%\n54.51%\n88.19%\n10.54%\n60.56%\n62.35%\n56.08%\n\n\nSentence-BERT\n81.60%\n76.22%\n86.73%\n69.67%\n51.26%\n90.25%\n82.81%\n42.25%\n67.43%\n59.16%\n\n\nKB-BART\n79.60%\n73.41%\n85.47%\n\n53.07%\n89.33%\n74.33%\n45.07%\n63.33%\n55.81%\n\n\nKB-BERT\n80.97%\n83.53%\n89.27%\n70.21%\n65.34%\n90.83%\n87.42%\n38.03%\n67.31%\n57.66%\n\n\nBERT ü§ó\n81.15%\n76.75%\n87.63%\n62.65%\n52.35%\n90.71%\n54.83%\n47.89%\n64.98%\n60.44%\n\n\nMegatron-BERT-base 125k\n80.23%\n78.40%\n88.38%\n73.73%\n65.34%\n88.88%\n83.61%\n50.70%\n64.98%\n59.19%\n\n\nMegatron-BERT-base 600k\n82.48%\n76.34%\n89.13%\n75.56%\n63.90%\n90.37%\n77.46%\n40.85%\n62.39%\n57.61%\n\n\nKB BERT-large 110k\n84.50%\n81.36%\n91.12%\n72.44%\n69.31%\n93.00%\n87.75%\n29.58%\n72.23%\n54.63%\n\n\n\n\nThe two large models mostly come out on top, with some exceptions achieved by the new BERT-base models. All models fail on the WNLI set, with the exception of ELECTRA-base, which in turn underperforms on everything else; performance on this sub-task should therefore be taken (even more than the others) with a grain of salt. The MRPC and RTE tasks show that our models do not perform, compared to the large AI-Sweden BERT and Google‚Äôs multilingual BERT, well there. We believe that this might be due to difference in training data used, which relies much less on data crawled from the web.\n\nDouble-Check Performance on other Datasets\nWhen evaluating models it is very important to test them on a wide variety of tasks. While it is tempting to test models on a test-suite like (Super)GLUE or OverLim, these in particular focus on one type of application. Large LMs also benefit other applications considered more basic such as tagging or parsing. Users should not choose their model because of some over-optimized aggregate of task-irrelevant scores, that favors larger models, which might even be too unwieldy for production purposes. When evaluating some of the models on our SUCX 3.0 - NER dataset, we see this clearly. For these experiments we again trained and evaluated each model five times, on the mixed-case variations of the SUCX 3.0 - NER dataset using both the original tags as well as the more stable simple ones. The reported F-scores on the test set were averaged to give a more even view.\n\n\n\nModel\nSimple\nOriginal\n\n\n\n\nAI-Sweden BERT-large\n88.73%\n86.07%\n\n\nKB-BERT\n89.35%\n86.71%\n\n\nBERT ü§ó\n89.80%\n87.43%\n\n\nMegatron-BERT-base 125k\n87.84%\n84.95%\n\n\nMegatron-BERT-base 600k\n88.60%\n86.11%\n\n\nMegatron-BERT-large 110k\n89.78%\n87.32%\n\n\n\nHere we can see that additional training time increases the performance for the two Megatron-BERT-base models, but even though they have been trained for longer and on more data, the small model differences let them stay behind the old KB-BERT. AI-Sweden‚Äôs Megatron-BERT-large model also performs worse than the KB-BERT, while our Megatron-BERT-large manages to barely outperform its smaller predecessor. The winner in this little competition however is the new BERT ü§ó which was also only trained for 125k steps, showing that there is no one best model.\nOne important aspect when comparing models has been omitted so far. Statistical significance testing is an important tool that is very much underused (such as here). With significance testing we can get a much better idea on which performance differences actually matter, or whether some model only performs consistently better, by already having seen five test-set examples during pretraining.\nWe invite everyone to test their models, optimize performance of already evaluated models, and compare, even across languages to better understand the strengths and weaknesses of language models."
  },
  {
    "objectID": "posts/2022-03-16-evaluating-swedish-language-models/index.html#conclusion",
    "href": "posts/2022-03-16-evaluating-swedish-language-models/index.html#conclusion",
    "title": "Evaluating Swedish Language Models",
    "section": "Conclusion",
    "text": "Conclusion\nWith the publication of this dataset we hope to give the Scandinavian NLP community a new tool for evaluating their language models. Even though we think that any results on this data should not be used to claim the state-of-the-art of your newest model, we hope to instill a little bit of healthy competition into the community, especially for everyone developing multi-lingual models."
  },
  {
    "objectID": "posts/2022-03-16-evaluating-swedish-language-models/index.html#acknowledgements",
    "href": "posts/2022-03-16-evaluating-swedish-language-models/index.html#acknowledgements",
    "title": "Evaluating Swedish Language Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe gratefully acknowledge the HPC RIVR consortium (https://www.hpc-rivr.si) and EuroHPC JU (https://eurohpc-ju.europa.eu) for funding this research by providing computing resources of the HPC system Vega at the Institute of Information Science (https://www.izum.si)."
  },
  {
    "objectID": "posts/2022-01-24-kblab-publishes-an-article-in-crl/index.html",
    "href": "posts/2022-01-24-kblab-publishes-an-article-in-crl/index.html",
    "title": "KBLab publishes an article about AI in the library in C&RL",
    "section": "",
    "text": "With the increasingly rapid appearance of new techniques over the past few years, the role of AI within the library is becoming a hot topic today. Yet despite a growing interest among librarians, there remains surpisingly little research devoted to the subject.\nWe have sought to fill this gap at KBLab by conducting research focused specifically upon AI in the context of libraries. The aim with our new article, Making and using AI in the library: creating a BERT model at the National Library of Sweden, is to show how AI techniques create new possibilities for processing and organizing information at scale in a library (e.g.¬†though automated classification and enriched metadata), but also how library collections might be used to make even better AI tools in the future.\nIn this way, we have sought to nuance the image that otherwise tends to predominate of the library as a passive space for the application of AI solutions developed elsewhere. On the contrary, we argue that libraries can have an important roll to play in AI development, especially in relation to language technology (NLP) for other languages than English and Chinese. We hope that our text can lead to new and exciting conversations about the future role of AI in the library!\n\n\nThe article has been accepted for publication in the Open Access journal College & Research Libraries, but is available to read online as a preprint now (link to a downloadable PDF on SocArXiv)."
  },
  {
    "objectID": "posts/2022-01-24-kblab-publishes-an-article-in-crl/index.html#ai-in-the-library",
    "href": "posts/2022-01-24-kblab-publishes-an-article-in-crl/index.html#ai-in-the-library",
    "title": "KBLab publishes an article about AI in the library in C&RL",
    "section": "",
    "text": "With the increasingly rapid appearance of new techniques over the past few years, the role of AI within the library is becoming a hot topic today. Yet despite a growing interest among librarians, there remains surpisingly little research devoted to the subject.\nWe have sought to fill this gap at KBLab by conducting research focused specifically upon AI in the context of libraries. The aim with our new article, Making and using AI in the library: creating a BERT model at the National Library of Sweden, is to show how AI techniques create new possibilities for processing and organizing information at scale in a library (e.g.¬†though automated classification and enriched metadata), but also how library collections might be used to make even better AI tools in the future.\nIn this way, we have sought to nuance the image that otherwise tends to predominate of the library as a passive space for the application of AI solutions developed elsewhere. On the contrary, we argue that libraries can have an important roll to play in AI development, especially in relation to language technology (NLP) for other languages than English and Chinese. We hope that our text can lead to new and exciting conversations about the future role of AI in the library!\n\n\nThe article has been accepted for publication in the Open Access journal College & Research Libraries, but is available to read online as a preprint now (link to a downloadable PDF on SocArXiv)."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "",
    "text": "The Riksdag is Sweden‚Äôs legislature. The 349 members of the Riksdag regularly gather to debate in the Chamber of the Parliament House. These debates are recorded and published to the Riksdag‚Äôs Web TV. For the past 20 years the Riksdag‚Äôs media recordings have been enriched with further metadata, including tags for the start and the duration of each speech, along with their corresponding transcripts. Speaker lists are added to each debate, allowing viewers to navigate and jump between speeches easier 1. This metadata also allows linking members of parliament and ministers with the debates they have participated in.\nHappening upon these recordings and seeing them linked to such rich metadata, we were curious to learn if the Riksdag had planned to make them available through its open data platform and APIs. We e-mailed them to inquire whether an audio/media file API was in the plans, to which they responded that such an API in fact already did and does exist, although they had yet to settle on a good way of communicating this service to the public.\nPart of our work here at KBLab involves training Swedish Automatic Speech Recognition (ASR) models capable of transcribing speech to text. We release these models freely and openly, see for example our wav2vec2-large-voxrex-swedish model (Malmsten, Haffenden, and B√∂rjeson 2022). Here, audio datasets with annotated transcriptions play an especially important part for quality. Unfortunately such datasets are hard to come by for Swedish. The combined total of current available datasets with annotated transcriptions number somewhere in the hundreds of hours.\nDecades of debates from the Riksdag present a golden opportunity to increase this quantity by a factor of tenfold or more. The wealth of dialects and accents, along with the breadth of metadata covering electoral districts, birth year and the gender of members of parliament may serve to not only improve ASR systems, but also to enable research on the weaknesses and biases of both current and future models.\nHowever, in order to get the speeches and transcripts properly aligned so they can be brought to a suitable and workable format, we first need to ensure the region of audio within a debate representing a speech actually matches the written transcript. Additionally no other speakers should ideally be present in this window."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#the-importance-of-metadata",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#the-importance-of-metadata",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "The importance of metadata",
    "text": "The importance of metadata\nWhen assessing the quality of the available metadata from debates, we found most of the material from 2012 and forward was generally accurate and of high quality. However, the required degree of precision of metadata always depends on one‚Äôs use case. And in our case it was important that only one speaker ‚Äì the one making the speech ‚Äì be present in the indicated window. With this in mind, it soon became evident that a certain level of adjustment of the existing metadata was required. Illustrating this with an example below, we see that the Riksdag‚Äôs metadata (right video) tends to include parts where the speaker of the house talks. The left video shows the automatically generated metadata from the method we developed for locating and segmenting speeches. We employed speaker diarization to more precisely pinpoint when a speaker starts and stops speaking.\n\n\nBoth videos below will update at the same time when pressing ‚ÄúNext debate‚Äù/‚ÄúPrevious debate‚Äù. The speeches are set to begin and end playing at the ‚Äústart‚Äù and ‚Äúend‚Äù of both metadata sources. Play them one after another to see how they compare in terms of speech segmentation.\n\n\n  \n    \n    \n    \n      \n        \n        \n      \n      Adjusted metadata (KBLab)\n      Beginning of official transcript:\n      Fru talman! Njutnings√§ktenskap l√•ter h√§rligt. Men Uppdrag granskning visade att det √§r religi√∂st sanktionerat koppleri. Det √§r sexhandel, det √§r...\n      Start: 00:01:37.5  End: 00:02:34.0\n      Speaker: Ann-Sofie Alm (M)\n      Source: Sveriges riksdag.\n    \n  \n\n  \n    \n    \n      \n        \n        \n      \n      The Riksdag's metadata\n      Beginning of official transcript:\n      Fru talman! Njutnings√§ktenskap l√•ter h√§rligt. Men Uppdrag granskning visade att det √§r religi√∂st sanktionerat koppleri. Det √§r sexhandel, det √§r...\n      Start: 00:01:34.0  End: 00:02:37.0\n      Speaker: Ann-Sofie Alm (M)\n      Source: Sveriges riksdag.\n    \n  \n\n\nModern Riksdag metadata, such as the debate above, serve as a good benchmark against which we can evaluate our fully automated method. Should our method ‚Äì using only official transcripts and audio ‚Äì be able to roughly match the segmentation quality of these more recent debates, we can be reasonably certain it can also fare well when applied on older materials.\nBelow we display speeches from 10 sampled debates from before 2012-01-01 ‚Äì a period where metadata quality tends to be shakier. The first and the second debate, ‚ÄúRegeringens sk√§rpning av migrationspolitiken‚Äù and ‚ÄúKvalitet i f√∂rskolan m.m.‚Äù, contain large errors in the Riksdag‚Äôs metadata when it comes to start and end times. It appears the metadata in these debates have shifted to be off by an entire speech. In addition to the above, we found mismatches between the indicated names of the speakers in text form and in the internal id-system that the Riksdag use to identify members of parliament. The more accurate field is intresent_id which lists the id number of the speaker, whereas the text field which lists the name in textual form at times can be misleading.\nLikely this is either an off by one error during data entry, a joining of disparate datasets gone wrong, or some post-processing mistake. Since we found the intressent_id field to be reliable, we used the id‚Äôs to fetch the names and information of parliament members from a separate data file the Riksdag provides in their open data platform 2.\n\n\n\n\n\n\n\n  \n    \n    \n    \n      \n        \n        \n        \n        \n      \n      Adjusted metadata (KBLab)\n      Beginning of official transcript:\n      Herr talman! Tyv√§rr tvingas jag notera att jag inte fick svar p√• de fr√•gor jag st√§llde, men jag kan upprepa...\n      Start: 00:12:35.4  End: 00:16:41.9\n      Speaker: Erik Ullenhag (L)\n      Source: Sveriges riksdag.\n    \n  \n\n  \n    \n    \n      \n        \n        \n        \n        \n      \n      The Riksdag's metadata\n      Beginning of official transcript:\n      Herr talman! Tyv√§rr tvingas jag notera att jag inte fick svar p√• de fr√•gor jag st√§llde, men jag kan upprepa...\n      Start: 00:00:00.0  End: 00:05:12.0\n      Speaker: Barbro Holmberg (S)\n      Source: Sveriges riksdag.\n    \n  \n\n\nA higher fraction of the earlier debates from 2003-2006 tend to start and end in the middle of speeches, looking like they may possibly have been automatically edited and spliced based on misaligned metadata. Several of the debates have issues with skips and cuts in the media, possibly resulting from video encoding errors upon conversion to web formats. Hopefully, the full debates are still available in an unedited format somewhere in the Riksdag‚Äôs archives."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#the-riksdags-speeches-in-numbers",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#the-riksdags-speeches-in-numbers",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "The Riksdag‚Äôs speeches in numbers",
    "text": "The Riksdag‚Äôs speeches in numbers\nThe valid downloadable audio files from the Riksdag‚Äôs debates have a total duration 6361 hours. There is a metadata field called debateseconds indicating the duration of each debate in seconds. If we sum the claimed duration of debates they amount to 6398 hours.\n\n\n\nSource\nTotal duration of debates (hours)\n\n\n\n\nThe Riksdag‚Äôs metadata\n6398.4\n\n\nAudio files\n6361.4\n\n\n\nHow many of those 6360 hours are actually speeches though? According to our speech segmentation, the debate files consist of a total of 5858 hours of speeches. However, when looking at the metadata field duration (for individual speech durations), the total duration of speeches exceed the total duration of the audio files. We note the start and end indications of many speeches overlap with other speeches for metadata prior to 2012.\n\n\n\nSource\nTotal speech duration of debates\n\n\n\n\nThe Riksdag‚Äôs metadata\n6742.15\n\n\nAdjusted metadata (KBLab)\n5858.36\n\n\n\nOur method for finding which debates had associated media files, was to first download the speeches in text form from ‚ÄúRiksdagen‚Äôs anf√∂randen‚Äù. We then queried the Riksdag‚Äôs media API, using the document ids of all those speeches. Out of more than 300000 available speeches from 1993/94 and forward:\n\n133130 speeches belonged to debates that had downloadable audio files in the Riksdag‚Äôs media API.\nOf the above only 122525 speeches had valid audio files, or were found to be at all present in the audio files.\nAfter applying additional quality filters 117725 speeches remained. These filters included removing:\n\nduplicate transcripts attributed to different speakers.\ntwo or more separate speeches being attributed with the same starting or ending time (indicating the model had failed making a valid prediction).\ndebates starting and ending in the middle of speeches.\nsudden jumps/cuts/edits in the audio while a speech was in progress.\nspeeches shorter than 25 seconds in duration (the margins of error are narrower for shorter speeches).\n\n\n\nMetadata statistics grouped by year\nTo get an overview of metadata quality over time, we calculated the difference between our adjusted ‚Äústart‚Äù and ‚Äúend‚Äù metadata, and the corresponding metadata from the Riksdag. The table below displays the total hours of audio per year, and the median of the difference between KBLab‚Äôs adjusted metadata and the Riksdag‚Äôs metadata. Negative start and end difference values reflect that KBLab‚Äôs speech starts or ends earlier in the debate audio file, whereas positive difference values reflects KBLab‚Äôs speech begins or ends later than the Riksdag‚Äôs.\n\n\n\n\n\nWe note how, for several years between 2006 and 2011, the adjusted KBLab metadata places the end of a speech about 85 seconds earlier than the Riksdag. The typical end indication from the Riksdag during these years appears to overshoot the speech by more than a minute. Looking at the same thing in graphical form below, where every point represents the mean difference within an entire debate, we see a systematic pattern of the ‚Äúend‚Äù metadata marker from the Riksdag overshooting the end of speeches (Figure¬†1 (b)). In contrast, most of the ‚Äústart‚Äù metadata markings (Figure¬†1 (a)) are more accurate. Perhaps this was a conscious choice, as capturing the start of a speech might have been deemed more important than spending a lot of time and resources capturing the exact ending of the speech.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The difference (delta) between ‚Äòstart‚Äô time metadata for KBLab and the Riksdag\n\n\n\n\n\n\n\n\n\n\n\n(b) The difference (delta) between ‚Äòend‚Äô time metadata for KBLab and the Riksdag\n\n\n\n\n\n\n\nFigure¬†1: Mean difference between KBLab adjusted metadata and the Riksdag. In general the Riksdag‚Äôs start markers tend to be biased towards beginning a few seconds before the actual speech, and ending later than the actual ending. The plot shows a zoomed in view between -200 and +200 seconds to emphasize general trends. There also exist debates for which the metadata is off by thousands of seconds.\n\n\n\n\n\n\nMost and least intelligible speakers\nAfter adjusting the metadata, we used the new metadata to split the debate audio file in to speech segments. These speech files were machine transcribed and evaluated against the official transcript using the BLEU score. High BLEU score generally indicate there‚Äôs a high correspondence or overlap between the output of the machine transcription and the official transcript.\nIn the table below, we display the 30 speakers with the lowest mean BLEU score, among those speakers with more than \\(5\\) speeches in the Riksdag‚Äôs debates. A low BLEU score may not necessarily imply the speaker is less intelligible for speech-to-text models, but may be the result of a combination of different factors:\n\nThe official transcription may have taken less or more liberties when transcribing the speech.\nThe segmentation (adjusted metadata) may have been systematically off for a particular speaker.\n\nInterestingly, the vast majority of the bottom \\(30\\) list are men (\\(28\\) out of \\(30\\)). Many of the members of parliament on this list have thick accents, or speak in dialectal varieties of Swedish. The low BLEU scores either indicate that speech-to-text models perform worse for this subset, or that the official transcriptions tend to rephrase a lot of what was uttered.\n\n\n\n\n\nThe top \\(30\\) list, in contrast, seems to skew towards high BLEU scores for women (\\(21\\) out of \\(30\\)). An interesting research question to examine would be whether this to a higher degree can be attributed to the speech-to-text model, how transcriptions are written, or the degree to which speeches are prepared or improvised.\n\n\n\n\n\nYou can use the search feature on the Riksdag‚Äôs Web TV to search for debates these speakers participated in."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#the-speech-finder-method",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#the-speech-finder-method",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "The speech finder method",
    "text": "The speech finder method\nWe‚Äôll provide a short summary of our method here, since the concept is likely better illustrated in graphical form.\n\nWe begin by using a speech-to-text model to transcribe an entire audio file. The speech-to-text model also outputs timestamps for when each of its output words were uttered. We use the model wav2vec2-large-voxrex-swedish (Malmsten, Haffenden, and B√∂rjeson 2022).\nWe use the official transcripts, and perform fuzzy string matching to locate what region of the automatic transcript they match against. The string matched region becomes an approximation of where the speech is located in the big audio file.\nWe perform speaker diarization. This gives us a more detailed view of when different speakers spoke during the debate. We use the tool pyannote.audio (Bredin et al. 2020) (Bredin and Laurent 2021) for speaker diarization.\nWe now still need to connect the speaker segments from the speaker diarization output to the official transcripts and the metadata associated with the transcripts. We therefore use the approximate predictions from the string matching method, and calculate the degre of overlap between different diarization speaker segments and the fuzzy string matched region of a speech.\nThe dominant (most overlapping) speaker segment(s) becomes our prediction.\n\nThe idea is that this method should be completely independent way to generate speech segments ‚Äì one that does not rely on the Riksdag‚Äôs already existing metadata. The only necessary components are i) a transcript, and ii) an audio file. The objective is essentially to find the needle (speech) in the haystack (audio file)."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#evaluation-of-metadata-quality",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#evaluation-of-metadata-quality",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "Evaluation of metadata quality",
    "text": "Evaluation of metadata quality\nHow do we know whether our method works or not? The benefit of the ‚Äúspeech finder‚Äù method being independent of the Riksdag‚Äôs official metadata, is that we can benchmark and compare against the portions of Riksdag data that maintain the best metadata quality. In general the best metadata quality is found in recent years, although most years since 2012 have had a fairly consistent and high quality.\nIn the plot below, we plot the average monthly BLEU scores over time (higher is better). KBLab‚Äôs adjusted metadata is of comparable quality the Riksdag‚Äôs metadata for, 2012 and forward. However, whereas the Riksdag‚Äôs BLEU scores drop for older debates, speech segmentations based on KBLabs adjusted metadata maintains an even and similar quality throughout the entire period.\n\n\n\n\n\n\n\nFigure¬†2: BLEU score is often used as a scoring metric for determining how well a translation overlaps with a reference, source or ground truth text. In our case we split the debate audio file in to speeches based on ‚Äòstart‚Äô and ‚Äòend‚Äô metadata. We then use the wav2vec2-voxrex-swedish speech-to-text model to automatically transcribe any speech within the split speeches. The automatic model transcription can be seen as a form of ‚Äòtranslation‚Äô. Finally, we compare the automated transcription against the official transcript. A high BLEU score indicates what was spoken within the segmented region overlaps to a high degree with the official transcript."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#the-benefits-of-open-data",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#the-benefits-of-open-data",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "The benefits of open data",
    "text": "The benefits of open data\nWe like open data at KBLab! The Riksdag‚Äôs open data is a wonderful resource that we believe will benefit research for many years to come. Making it easy for many people to acccess and use the Riksdag‚Äôs data creates positive spinoff effects. The WeStAc project with their curation of protocols from the Riksdag spanning back to the 1920s is one such example (see riksdagen-corpus). While KBLab‚Äôs primary objective in doing speech segmentation was to create an ASR dataset, the method and metadata output can be shared and used by both the Riksdag and other projects interested in curating Riksdag materials and possibly connecting curated protocols to audio and video recordings.\nThe audiovisual materials currently exposed in the Riksdag‚Äôs APIs has made it possible for us to create RixVox: a 5500 hour audio dataset with aligned text transcripts. From the point of view of KBLab, we benefit greatly from other organizations and research projects curating datasets, protocols and transcripts in this manner. We believe the method described in this article can likely successfully be applied to Riksdag debates from the 1960s to the 2000s. The prerequisite is simply having access to the audio, and a set of curated and well segmented textual transcripts from protocols. Luckily, since the Riksdag has done such a splendid job with their open data platform, there are research projects that have taken on the challenge!"
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#code-and-data",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#code-and-data",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "Code and data",
    "text": "Code and data\nWe plan on making RixVox, the 5500 hour speech-to-text dataset consisting of parliamentary speeches, freely and openly available on Huggingface. A new post will appear on this blog announcing the news when it is released!\nThe code for reproducing the speech segmentations, the adjusted metadata, and RixVox is available. See the Code and Data sections below."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#acknowledgements",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#acknowledgements",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPart of this development work was carried out within the frame of the infrastructural project HUMINFRA."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#code",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#code",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "Code",
    "text": "Code\n\nThe code for reproducing results in this article can be found on https://github.com/kb-labb/riksdagen_anforanden."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#data",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#data",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "Data",
    "text": "Data\n\nThe resulting metadata can be downloaded, and has been shared with the Riksdag. You can find the metadata here: https://github.com/kb-labb/riksdagen_anforanden/tree/main/metadata."
  },
  {
    "objectID": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#footnotes",
    "href": "posts/2023-02-15-finding-speeches-in-the-riksdags-debates/index.html#footnotes",
    "title": "Finding Speeches in the Riksdag‚Äôs Debates",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn example can be seen here: https://www.riksdagen.se/sv/webb-tv/video/partiledardebatt/partiledardebatt_HAC120230118pd‚Ü©Ô∏é\nThe file Sagtochgjort.csv.zip here: https://data.riksdagen.se/data/ledamoter/‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-05-04-topic-models-for-sous/index.html",
    "href": "posts/2021-05-04-topic-models-for-sous/index.html",
    "title": "Topic models for Statens Offentliga Utredningar",
    "section": "",
    "text": "Statens Offentliga Utredningar, or the State‚Äôs Public Investigations, are a collection of governmental reports commissioned by Swedish departments on a variety of different topics. The National Library owns physical copies of all SOUs published between 1922 and 1999, as well as their digital versions, available here. The SOUs are an interesting collection in their own right because of their historical and political value, but they also constitute a perfect open-source dataset for Swedish NLP. In this post we want to showcase an example of quantitative analysis which allows us to gain insights from a large amount of text, namely topic models. The analysis was done in python and the code can be found here."
  },
  {
    "objectID": "posts/2021-05-04-topic-models-for-sous/index.html#gathering-and-compiling-the-data",
    "href": "posts/2021-05-04-topic-models-for-sous/index.html#gathering-and-compiling-the-data",
    "title": "Topic models for Statens Offentliga Utredningar",
    "section": "Gathering and compiling the data",
    "text": "Gathering and compiling the data\nAs we said, the National Library has digitized all the physical SOUs in its possession covering the period 1922-1999. These are available as pdf, as images or as OCR:ed plain text. Plain text format is the most convenient for further processing, so we downloaded and stored them in a .csv file as plain text along with year and issue metadata.\nFrom the year 2000, the Swedish government started publishing its reports directly in digital form, so the digitization process was no longer necessary. The newer issues were downloaded directly from the Government‚Äôs Open Data page, where they are available in a variety of different formats. We chose the .json format as it is a structured data format that is easy to manipulate in python, and like for the older SOUs, we extracted plain text as well as year and issue metadata.\nIn order to build a topic model, a whole SOU issue with over a hundred pages is too long to be considered a single document and the results wouldn‚Äôt be very good. So we decided to further process the data and divide each issue into pages. For the older SOUs, the page information is easily available since they come from a physical format and they have been digitized by page. For the newer editions, we used the HTML version of the file, where the page numbers were indicated. Both datasets, by issue and by page, can be downloaded here.\nFinally, we thought it would be useful to add department information to each SOU, in order to be able to build topic models by department as well as by decade. This proved to be trickier than expected, for a couple of different reasons. For older SOUs, especially from the ‚Äô20s and ‚Äô30s, the first page (where the department information is), is often very worn out by time and the quality of the OCR is consequently rather poor. Conversely, for newer SOUs, especially in the beginning of the 21st century, the department information is absent altogether, as they are addressed to the politician that was Head of that department at the time. We used a combination of techniques to compile the department information automatically and then did some manual cross-checking to ensure the best possible results, but the department metadata should still be taken with a grain of salt. The corresponding departments.csv file can also be found here."
  },
  {
    "objectID": "posts/2021-05-04-topic-models-for-sous/index.html#cleaning-and-pre-processing",
    "href": "posts/2021-05-04-topic-models-for-sous/index.html#cleaning-and-pre-processing",
    "title": "Topic models for Statens Offentliga Utredningar",
    "section": "Cleaning and pre-processing",
    "text": "Cleaning and pre-processing\nLet‚Äôs say we want to build a topic model for the ‚Äô50s, and see what Swedish politics was most concerned with at that time. We start by loading the dataset and having a look at our data.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"sou_1950-1959_pages.csv\")\nprint(df.groupby('year')['issue'].nunique())\n\n\n\n\nYear\nNumber of Issues\n\n\n\n\n1950\n48\n\n\n1951\n59\n\n\n1952\n53\n\n\n1953\n36\n\n\n1954\n40\n\n\n1955\n52\n\n\n1956\n62\n\n\n1957\n51\n\n\n1958\n46\n\n\n1959\n46\n\n\n\nWe can see that we have between 36 and 62 issues for each year from 1950 to 1959. Let‚Äôs have a look at the number of pages per year:\n\nprint(df.value_counts(subset=['year']))\n\n\n\n\nYear\nNumber of Pages\n\n\n\n\n1956\n14266\n\n\n1951\n12737\n\n\n1955\n12121\n\n\n1952\n11045\n\n\n1957\n10780\n\n\n1958\n9034\n\n\n1959\n8886\n\n\n1950\n8834\n\n\n1954\n8195\n\n\n1953\n7596\n\n\n\nWe have thousands of pages for each year, but there is certain degree of variation: 1956 has almost double as many pages as 1953. For a political scientist, this could already be telling them something about the Swedish government in the 50s.\nNow let‚Äôs see what the actual text looks like, to have an idea of how to proceed with the cleaning. We want to see a typical page, not the table of contents or an appendix full of tables, so we choose one random page in the middle of an issue, for example page 40 in the first SOU of 1952:\n\nprint(df[(df['issue']==1) & (df['year']==1952)].iloc[40]['text'])\n\n\n\n\nRaw text\n\n\nIt looks pretty good, but we need to remove unnecessary whitespace characters and numbers. We don‚Äôt want to remove punctuation at this step because we will do sentence segmentation and part-of-speech tagging at a later stage, and the punctuation really helps with that.\n\n# turn all consecutive whitespace characters into one space\ndf['text'] = df['text'].map(lambda x: re.sub(r'\\s+', ' ', x))\n# remove numbers\ndf['text'] = df['text'].map(lambda x: re.sub(r'\\d+', '', x))\n\nNext we might want to remove the pages that contain less than a given number of words, for example we can set the threshold at 20 words. Those pages most likely do not contain regular text, because if you look at the previous sentence, that is already 27 words not counting punctuation.\n\ndf = df[df['text'].str.split().str.len() &gt;= 20]\n\nNow we can start the actual pre-processing. We want to base our topic model only on nouns and verbs, as other word classes rarely carry the type of topical semantic information that we are interested in. In order to do that, we need to segment each document into sentences, tokenize the words, remove stopwords and assign POS tags. We also need to lemmatize to reduce the size of the vocabulary and make the model cleaner. Since Swedish is not a major language like English, we have more limited options in the choice of NLP tools. In this example we use the python library Stanza by the Stanford NLP group. While it is really accurate and provides a great lemmatizer for Swedish, it does take several hours on a GPU to process a decade of SOUs. This might be complete overkill or totally reasonable depending on the use case. We decided to go with this tool because we have access to a GPU and it provides sentence segmentation, tokenization, POS-tagging and lemmatization in a few lines of python code. Other possibilities are NLTK (which only has a stemmer for Swedish), efselab or spaCy-UDPipe. Our own spaCy models still don‚Äôt have a good lemmatizer because it isn‚Äôt trainable, so we couldn‚Äôt consider them as an option.\n\nimport stanza\n\npages = list(df.text)\n\n# load stanza model\nnlp = stanza.Pipeline(lang='sv', processors='tokenize,pos,lemma')\n\nallowed_tags=['NOUN', 'VERB']\ndata = []\n\nfor page in pages:\n  doc = nlp(page)\n  data.append([word.lemma for sent in doc.sentences for word in sent.words if word.pos in allowed_tags\n                    and word.lemma not in stopwords and re.search(r'\\w{2}', word.lemma) is not None])\n\nLet‚Äôs see what is going on in the code snippet above. First we need to convert the text column of our dataframe into a list. Then we load the stanza model (if it‚Äôs the first time you use Stanza you will need to download it first) and we define the word classes that we want to include. Next we loop though each page (or document) in our dataset and we run the Stanza model on it. For each sentence we only want to keep the lemmas of the words that are nouns or verbs, provided that they are not stopwords and that they are at least two letters long (this is an arbitrary number, we picked two because Swedish has many meaningful two-letter nouns). With this step we automatically lowercase the words and we get rid of isolated punctuation. Some punctuation marks that have been tokenized in the same token as a word might still appear (for example ‚Äúkr.‚Äù), but it shouldn‚Äôt bother us too much in training our model and we can always remove them later if we want to. We use a stopword list that can be downloaded here.\nThe result of the step above is a list where every document is represented by a list of words. Now we need to create the vocabulary and the corpus that we will use in our topic model. Often in topic modeling a great deal of time is spent tweaking the stopword list with domain-specific stopwords. That gives the researcher a greater degree of control, but we can also use word frequencies and occurrence statistics to help us prune our vocabulary much faster.\n\nid2word = gensim.corpora.Dictionary(data)\nid2word.filter_extremes(no_below=10, no_above=0.4)\n\ncorpus = [id2word.doc2bow(text) for text in data]\n\nThe vocabulary is just a look-up table where an index is assigned to every word in our data. We create it with a built-in gensim function. In order to make sure that we keep only relevant words, we would like to get rid of the extremes: words that occur in very few documents are probably OCR errors or context-specific names that would only make our model messier, whereas words that occur in many documents are not likely to be very informative in regard to our topics. We use gensim‚Äôs built in filter_extremes() function to get rid of all the words that occur in less than 10 pages or more than 40% of all documents. These numbers are a judgment call and should be adjusted according to the specific problem. In this case, before filtering we have 461253 words in the dictionary and after filtering there are only 47119, which is a much more reasonable number. The corpus is a also a list where each document is represented by a list of tuples. Each tuple contains a word ID and the frequency of that word in the document."
  },
  {
    "objectID": "posts/2021-05-04-topic-models-for-sous/index.html#building-a-topic-model",
    "href": "posts/2021-05-04-topic-models-for-sous/index.html#building-a-topic-model",
    "title": "Topic models for Statens Offentliga Utredningar",
    "section": "Building a topic model",
    "text": "Building a topic model\nNow we have all the pieces to build a topic model, but we still have a problem: we have no idea what the number of topics k should be. There are a few different ways to measure the quality of a topic model, for example perplexity and coherence. In this example we use coherence as our metric to compare models with different values of k. The code below trains a separate topic model for each k between 20 and 100 topics with a step of 5, and plots the resulting coherence. We use a gensim wrapper for the java-based tool Mallet which is considered one of the best options for topic models in python. The algorithm that we use is a standard Latent Dirichlet Allocation. It should also be mentioned that with this amount of documents it takes 12-13 minutes to train one topic model, so this process of finding the best value of k is rather time consuming.\n\nmallet_path = \"Mallet/bin/mallet\"\ncoherence = []\n\nfor k in tqdm(range(20,105,5)):\n    print('Topics: '+str(k))\n    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=k, id2word=id2word)\n    \n    coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data, dictionary=id2word, coherence='c_v')\n    coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n    coherence.append((k,coherence_ldamallet))\n    \nx_val = [x[0] for x in coherence]\ny_val = [x[1] for x in coherence]\n\nplt.plot(x_val,y_val)\nplt.scatter(x_val,y_val)\nplt.title('Number of Topics vs. Coherence')\nplt.xlabel('Number of Topics')\nplt.ylabel('Coherence')\nplt.xticks(x_val)\nplt.show()\n\n\n\n\nNumber of Topics vs.¬†Coherence\n\n\nFrom the image above we can see that coherence improves steadily until about k=50, and after that it stays more or less between 0.60 and 0.62. We could pick k=90 because in absolute terms it has the highest coherence, but the correlation between coherence and perceived quality of the model is not so strict, and for interpretability it is usually easier to have a smaller number of topics. We can settle on k=50 in order to have good coherence and a reasonable number of topics to work with.\nNow we can have a look at our topics, and one way to do that is to put the top 20 words for each topic into a dataframe. Here is what it looks like for our data, down to the 20th topic:\n\n\n\nSeveral topics - for example Topics 1, 8, 13 and 19 - seem to be mostly connected to the activities of the government and the investigative process itself. The words that represent them are quite general, so we don‚Äôt get many insights from them. On the other hand, we have topics that are clearly dominated by a political issue that was relevant at the time and might still be relevant today. For example, Topic 2 is about the pension system, Topic 4 is about selling alcohol and its consequences, Topic 15 is about the organization of the Church in relation to the state, and Topic 18 is about higher education and research. An interesting phenomenon manifests itself in Topic 35:\n\nIt seems to be a mix of words that are likely to appear in a table and OCR errors. We can guess that such pages with a complex layout are not easy for the OCR software and therefore more likely to contain errors. Luckily for us, they have grouped themselves neatly into a single topic and we don‚Äôt even need to hunt them down and exclude them manually!\nAnother common way to visualize topic models is the library pyLDAvis. First we need to convert our Mallet model into a gensim model to make it compatible with the library. Then we can run the visualization with pyLDAvis and we obtain the widget below:\n\ndef convert_mallet_to_gensim(mallet_model):\n    model_gensim = LdaModel(\n        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,\n        alpha=mallet_model.alpha) \n    model_gensim.state.sstats[...] = mallet_model.wordtopics\n    model_gensim.sync_state()\n    return model_gensim\n\nldagensim = convert_mallet_to_gensim(ldamallet)\n\nvis_data = gensimvis.prepare(ldagensim, corpus, id2word, sort_topics=False)\npyLDAvis.display(vis_data)\n\n\n\n\nThe widget shows our topics as circles. Their position on the map is an indication of how distinct they are and the size of the circles represents the predominance of each topic in the dataset. We did a pretty good job of having evenly sized topics, and while we do have some overlap on the left side around the horizontal axis, that is to be expected with a high number of topics. We could definitely work on our stopword list to get the topics to diversify themselves even more, by manually eliminating common bureaucratic words that are likely to occur in many documents but didn‚Äôt make the cut in our filter. We can also lower the threshold for eliminating a word, say to 5 occurrences in different documents, in order to keep more infrequent words, but we would run a higher risk of including lemmatization and digitization errors.\nOn the right side of our widget we see a list of words. Those are the most common words in the corpus overall. If we click on a topic we can see the words that define that topic. On the slider at the top we can control the value of Œª: a value close to 0 will show the most relevant words for that topic regardless of how frequent they are in the whole corpus, while a value close to 1 will give us words that are important for our topic but also frequent in the rest of the corpus. This allows us to name and interpret the topics, as well as to determine what distinguishes them from one another. The next step would be to go back to the texts and pick up our close-reading from there.\nDepending on the research question, you might want to dig deeper and use auxiliary methods like seeded topic models or global vectors to investigate the problem space. Even with regular topic models there are a lot of hyperparameters to tweak in order to improve the results, so go ahead and experiment. We hope you find this example and the corresponding datasets useful, don‚Äôt hesitate to share your work with us if you did something cool with them!"
  },
  {
    "objectID": "posts/2022-06-14-bertopic/index.html",
    "href": "posts/2022-06-14-bertopic/index.html",
    "title": "BERTopic for Swedish: Topic modeling made easier via KB-BERT",
    "section": "",
    "text": "The emergence of transformer-based language models has significantly enhanced the potential for machine learning to be used for extracting meaning from large volumes of text. Yet this potential has hitherto been limited to the select few with the requisite knowledge in data science. To the vast majority without prior experience of programming in Python, the promise of cutting-edge language processing from a BERT model remains just that‚Äîa distant promise.\nHowever, this has now changed with the development of BERTopic. Recent work by the Dutch data scientist, Maarten Grootendorst makes it possible to harness the impressive performance of BERT language models without first needing to become an expert in data science. More specifically, it enables the sophisticated and flexible NLP capacities of a BERT model to serve as the basis for a more accessible form of topic modeling. By offering a new and simpler way of using KBLab‚Äôs language models, BERTopic brings a cutting-edge yet previously technically challenging method within reach of a broader range of researchers and other users working with Swedish material.\nIn this post, we provide a brief introduction to using BERTopic for topic modeling with Swedish text data. We explain how BERTopic harnesses KBLab‚Äôs models to produce state-of-the-art topic models, and we outline some tips on how to get started. If you‚Äôre interested in finding out more about this particular method, or about topic modeling in general, check out the links at the bottom of the page!\n\n\n\n\n\n\nFigure¬†1: Intertopic distance map for a topic model of Swedish governmental reports (SOUs) on education between 1970-2021\n\n\n\n\nWhat is topic modeling and why might it be helpful for researchers?\nTopic modeling is a quantitative method for text analysis that allows researchers to gain insights on large text corpora without manually going through all the data themselves. The principal idea is to find a number of major topics in the collection and to be able to retrieve documents that are relevant for these topics.\nThe output of a topic model is a list of topics represented by words that define them. It is possible to visualize the topics in charts to see their size and distance to each other (see Figure¬†1).\nTopic models can be used in many ways, for example to follow the development of a concept over time or to analyze subtopics of a given issue (see Figure¬†2 and Figure¬†3). The main insight is in utilizing data mining algorithms to make vast amounts of data searchable‚Äîand thus legible‚Äîaccording to the particular themes of which it is comprised.\n\n\n\n\n\n\nFigure¬†2: Topics over time for model of Swedish governmental reports (SOUs) on education between 1970-2021\n\n\n\n\n\nWhat‚Äôs new with BERTopic and how does it make the method more accessible?\nBERTopic is a new topic modeling tool that builds on powerful neural networks. At the base there is a model called sentence-BERT, which can be considered an all-purpose language model that has some understanding of the meaning of a sentence or paragraph. The documents are grouped into clusters and then some words are extracted from each cluster to label the topic.\nThe main advantage of BERTopic over traditional topic models is that there is next to no pre-processing of the documents required prior to modeling: the pre-trained transformer model, sentence-BERT, takes care of identifying the meaningful parts of the text, which means that we don‚Äôt need to do it by hand. On top of that, the process is highly automated, so there are relatively few operative decisions that can influence the final result and that need to be accounted for.\n\n\nWhat do you need to do to get started with this method?\nTo build a BERTopic model you need to have a collection of relatively short documents, for example paragraphs or abstracts. While you don‚Äôt need to be an expert, you do need some experience with Python programming, since you need to be able to load your documents into Python to run the modeling on them. If you are planning to use the method on Swedish data, then you may want to use the sentence transformer model we have released, which can be found on KBLab‚Äôs HuggingFace page (see link below).\nThere are excellent tutorials available via the BERTopic repository on Github (link below), which provide the best starting point for testing out the method.\n\n\nWhat are the computational requirements of this approach?\nThe powerful language model that BERTopic is based on needs to run on a GPU, which not everyone has at their disposal. There are two possible solutions if you do not have access to a GPU at your institution: either you can run BERTopic with less computation-intensive embedding models like Flair, spaCy or Gensim, or you can use Google Colab to have temporary free access to a GPU.\nBoth options have pros and cons. Using a lighter model that runs on CPU means giving up some of the complex language understanding of a transformer model: BERT and similar models have consistently proven to be superior to other methods in many NLP tasks, and a BERTopic without an underlying BERT would be falling short of its full potential. Using Google Colab, on the other hand, means that you need to have a Google account, you need to have permission to load your documents online and you might not get enough GPU memory and/or time for your modelling needs. But having said that, experimenting with this latter option would certainly be a good place to start!\n\n\n\n\n\n\nFigure¬†3: Hierarchical clustering of topics for a model of Swedish governmental reports (SOUs) on education between 1970-2021\n\n\n\n\n\nLinks about BERTopic:\nTutorials on GitHub: https://github.com/MaartenGr/BERTopic\nComplete documentation: https://maartengr.github.io/BERTopic/\nKBLab‚Äôs sentence BERT for Swedish: https://huggingface.co/KBLab/sentence-bert-swedish-cased\nGoogle Colab: https://colab.research.google.com/\n\n\nFurther reading on topic modeling as a research method:\n‚ÄúTopic Modeling with BERT,‚Äù Medium article by Maarten Grootendorst: https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\n‚ÄúInteractive Topic Modeling with BERT,‚Äù Medium article by Maarten Grootendorst https://towardsdatascience.com/interactive-topic-modeling-with-bertopic-1ea55e7d73d8\n‚ÄúProbabilistic Topic Models,‚Äù David M. Blei article: https://www.cs.columbia.edu/~blei/papers/Blei2012.pdf\n‚ÄúThe Digital Humanities Contribution to Topic Modeling‚Äù, Journal of Digital Humanities special edition: http://journalofdigitalhumanities.org/2-1/\n‚ÄúTopic Modeling: What Humanists Actually Do With It,‚Äù University of California blog post by Teddy Roland: https://digitalhumanities.berkeley.edu/blog/16/07/14/topic-modeling-what-humanists-actually-do-it-guest-post-teddy-roland-university\n\n\n\n\nCitationBibTeX citation:@online{fano2022,\n  author = {Fano, Elena and Haffenden, Chris},\n  title = {BERTopic for {Swedish:} {Topic} Modeling Made Easier via\n    {KB-BERT}},\n  date = {2022-06-14},\n  url = {https://kb-labb.github.io/posts/2022-06-14-bertopic/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFano, Elena, and Chris Haffenden. 2022. ‚ÄúBERTopic for Swedish:\nTopic Modeling Made Easier via KB-BERT.‚Äù June 14, 2022. https://kb-labb.github.io/posts/2022-06-14-bertopic/."
  },
  {
    "objectID": "posts/2024-01-30-finansinspektionen/index.html",
    "href": "posts/2024-01-30-finansinspektionen/index.html",
    "title": "Analysis of financial data at the Financial Supervisory Authority",
    "section": "",
    "text": "Analyzing financial data from various market actors presents significant challenges. It‚Äôs not just about analyzing the data itself but also about handling the vast amount of data received regularly, both quantitatively and qualitatively. The data arrives on a spectrum from daily to yearly intervals.\nUpon receiving the data, we meticulously analyze and categorize each institute into different risk categories. Subsequently, we devise a tailored supervisory plan for each institute, with the most high-risk institutions receiving the most comprehensive action plans.\nQuantitative data undergoes processing and analysis through relevant software, while qualitative data, typically in PDF format, is scrutinized mostly by supervisors. Analyzing qualitative data is resource-intensive and prone to oversights.\nTo streamline the supervision of qualitative data, we‚Äôve developed a machine learning model with support from the European Commission, DG Reform, and the Research Institute in Sweden (RISE). During the model‚Äôs development, we utilized KBLab‚Äôs Named Entity Recognition model based on KB-BERT to minimize noise. More precisely KBLab‚Äôs model helped us anonymize the data to minimize spurious correlations. The model has proven effective, prompting us to further enhance its capabilities. So instead of roughly 20 people reading little less than 200 reports we nowadays spend roughly one hour for the same work.\nAs we progress, our commitment to refining our analytical capabilities remains steadfast. By leveraging cutting-edge technologies and collaborative partnerships, we strive to enhance the efficacy of financial market supervision and regulation.\n\n\n\nCitationBibTeX citation:@online{holl√©n2024,\n  author = {Holl√©n, Jimmy},\n  title = {Analysis of Financial Data at the {Financial} {Supervisory}\n    {Authority}},\n  date = {2024-01-30},\n  url = {https://kb-labb.github.io/posts/2024-01-30-finansinspektionen/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHoll√©n, Jimmy. 2024. ‚ÄúAnalysis of Financial Data at the Financial\nSupervisory Authority.‚Äù January 30, 2024. https://kb-labb.github.io/posts/2024-01-30-finansinspektionen/."
  },
  {
    "objectID": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html",
    "href": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html",
    "title": "RixVox: A Swedish Speech Corpus with 5500 Hours of Speech from Parliamentary Debates",
    "section": "",
    "text": "Automatic Speech Recognition (ASR) systems that convert spoken language to text rely heavily on annotated data to produce the best possible results. Such datasets are unfortunately not widely available for Swedish. The combined total of currently available audio datasets with annotated transcripts for the Swedish language number somewhere in the hundreds of hours.\nTo this end, KBLab releases Rixvox, a new Swedish ASR dataset consisting of \\(5500\\) hours of speech. The data originates from parliamentary debates between the years of \\(2003\\) to \\(2023\\), which were made available via the Swedish Parliament‚Äôs open data initiative. KBLab used written protocols to segment speeches from the debates, and to subsequently force align the the written transcripts with audio from the speeches. In addition to audio and transcripts, metadata such as the name, gender, birth year, political party, and electoral district of speakers is also available.\nRixVox is free and open for anyone to download and use. The dataset can be reached on the following link: https://huggingface.co/datasets/KBLab/rixvox ."
  },
  {
    "objectID": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html#rixvox-dataset-statistics",
    "href": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html#rixvox-dataset-statistics",
    "title": "RixVox: A Swedish Speech Corpus with 5500 Hours of Speech from Parliamentary Debates",
    "section": "RixVox dataset statistics",
    "text": "RixVox dataset statistics\nThe RixVox dataset was constructed from parliamentary debates. You can read more about how we segmented speeches from debates and determined their precise start and end location within a debate in a previous article on this blog: ‚ÄúFinding Speeches in the Riksdag‚Äôs debates‚Äù (Rekathati 2023).\nThe dataset has chunked the audio from speeches in to smaller snippets suitable for training ASR models. Each observation is up to 30 seconds in length, and consists of either a single or several sentences from the written transcript of a speech, along with the corresponding audio. The dataset consists of a total of \\(5493.6\\) hours of speech. There are \\(1194\\) different speakers represented in the data. The average duration of an observation is \\(23.68\\) seconds. In the table below, we present the distribution of observations of the different train, validation and test split of RixVox, along with some summary statistics for each split.\n\n\n\n\n\n\n\n\n\n\n\nDataset Split\nObservations\nTotal duration of speech (hours)\nAverage duration obs. (seconds)\nNumber of speakers\n\n\n\n\nTrain\n818227\n5383\n23.69\n1165\n\n\nValidation\n7933\n52\n23.50\n18\n\n\nTest\n8884\n59\n23.74\n11\n\n\n\n\nThe dataset splits were created by sampling speakers until a threshold was reached in terms of total duration of speech. For the training set, we randomly sampled speakers until \\(98\\%\\) of the total duration of the RixVox dataset was reached (\\(5384\\) hours). For the test and validation set, we randomly sampled speakers until each filled up to a bucket of \\(1\\%\\) of the total duration of the entire dataset.\nLet‚Äôs also take a look at the gender distribution of speakers. We have \\(602\\) men, \\(519\\) women, and \\(73\\) speakers for whom this metadata is missing.\n\n\n\nLooking at the total duration of speech for each gender, we have a similar distribution to above. \\(46.3\\%\\) of the individual speakers were women, and \\(44.3\\%\\) of the total duration of speeches in RixVox is made up of women speaking.\n\n\n\n\n\n\nMost and least intelligible electoral districts\nEach observation in our dataset belongs to a speech in a debate. After segmenting the speeches from debate audio files, we machine transcribed every speech using KBLab‚Äôs wav2vec2-large-voxrex-swedish model (Malmsten, Haffenden, and B√∂rjeson 2022). We then calculated the BLEU score to measure the correspondence between the machine generated transcription and the official written transcript. A high BLEU score indicates there‚Äôs a higher correspondence, or overlap, between the machine generated transcript and the official transcript. This may indicate that ASR systems find certain regions easier to transcribe, or may alternatively indicate that the people who transcribe the speeches tend to rephrase or reword written transcripts of speeches from these districts.\n\n\n\n\n\nThe speakers with the highest score are for those whom thedistrict is missing (NA). These are mostly government ministers who have never been members of parliament. The least intelligible electoral districts are southern Sk√•ne, Gotland, and Malm√∂ municipality (also southern Sk√•ne).\n\n\nLongest total duration speaker\nWhich speakers have spent the most time on the Riksdag Chamber‚Äôs podium? The table below shows that Morgan Johansson is the undisputed \\(\\#1\\) debater in terms of total duration of speech."
  },
  {
    "objectID": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html#method-of-creation",
    "href": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html#method-of-creation",
    "title": "RixVox: A Swedish Speech Corpus with 5500 Hours of Speech from Parliamentary Debates",
    "section": "Method of creation",
    "text": "Method of creation\nBefore RixVox could be created, we needed to accurately segment speeches from debates. In other words: locate where the speech started and ended within the debate audio file. The most cumbersome parts of the preliminary work undertaken to segment speeches from debates is described in our previous article ‚ÄúFinding Speeches in the Riksdag‚Äôs Debates‚Äù (Rekathati 2023). We recommend reading this article for background on the speech segmentation.\n\nQuality filtering\nOnce the speeches were segmented, the remaining work consisted of performing some quality filtering based on simple heuristics, aligning the written transcripts with the audio on a sentence level, adding metadata about the speakers, and finally converting the alignments to short snippets up to 30 seconds in length (a suitable format for training ASR models).\nThe first round of quality filters applied on speeches can be found in the following lines of code. These include:\n\nWe keep only speeches \\(&gt; 25\\) seconds in duration as predicted by speaker diarization (see the linked article for context). The reliability of our speech segmentation method improves with speech length.\nWe keep only speeches \\(&gt; 15\\) seconds in duration as predicted by fuzzy string matching between machine transcription and official transcripts.\nWe calculate a ‚Äúlength ratio‚Äù, which is the predicted duration of the speech by speaker diarization, divided by the predicted duration of the speech by fuzzy string matching. We only keep the speech if this length ratio is between \\(0.8\\) and \\(1.5\\). Otherwise, we deem our two methods to be in too much of a disagreement.\nWe calculate an ‚Äúoverlap ratio‚Äù, which is the ‚Äúduration where speaker diarization and fuzzy string matching predictions overlap‚Äù divided by the total predicted duration of the fuzzy string matching method. If this ratio is \\(&gt;0.8\\) we keep the speech.\nWe only keep speeches where \\(1\\) single speaker was identified as speaking within the predicted regions.\nWe only keep speeches where the difference in predicted start time between a future and previous speech is \\(&gt;5\\) seconds.\n\nThe second round of quality filters were applied after another fuzzy string matching sanity check was performed. This time, instead of fuzzy string matching the text of a written transcript against the machine transcription of an entire debate, we fuzzy string match the text of the written transcript against the machine transcription of the segmented speech, as predicted by our speaker diarization. A short summary of the second round of quality filters follows:\n\nRemoving speeches where the official written protocol starts matching the machine transcription only after a threshold of \\(X\\) words in the machine transcribed version. This was a possible indication that the speech segmentation had predicted a too early start location for the speech, erroneously including parts of other speakers.\n\nRemoving speeches where the official written protocol stops matching hte machine transcription too early.\nWe adjust the \\(X\\) threshold based on different dates the debates were held on, and whether the debate was the first and/or the last speech of the debate (the first and last speeches of debates were more likely to be cut off in the middle of the speech before the year \\(2012\\)).\n\nSee the following lines of code for a full list of the filtering conditions.\nThe above filtering procedures reduced the number of speeches to be included in RixVox from about 122k speeches to 115k speeches.\n\n\nForced alignment\nOnce we had high confidence in the remaining set of predictions, we proceeded to align the written protocols with the audio. This was done by:\n\nSentence tokenizing the written transcripts.\nUsing the aeneas library to force align the audio with the text on the sentence level.\n\nThe aeneas library gives an output in the form of predicing the start and end location of the sentence within the speech.\nWe can recommend reading the masters thesis ‚ÄúAutomatic Annotation of Speech: Exploring Boundaries within Forced Alignment for Swedish and Norwegian‚Äù (Biczysko 2022) for an excellent review of available forced alignment tools for Swedish and Norwegian.\n\n\nCreating 30s observation snippets.\nIn the final step, we concatenate sentences from the same speech that follow one another up to a maximum length of \\(30\\) seconds per observation. The observations in RixVox are thus composed of either a single sentence, or several sentences in order within a speech up until the ‚Äúbucket‚Äù fills up to the threshold of \\(30\\) seconds.\nWe remove the first sentence of each speech, as transcriptions tend to add a ‚ÄúFru talman!‚Äù or ‚ÄúHerr Talman‚Äù here as a matter of formality, regardless of whether this was uttered by the speaker or not.\n\n\nDataset card\nRixVox has a dataset card on Huggingface, where you can find more details about the dataset, its features, and how to download and use it. You can also preview the first 100 observations of the train, validation and test sets in the dataset viewer."
  },
  {
    "objectID": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html#acknowledgements",
    "href": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html#acknowledgements",
    "title": "RixVox: A Swedish Speech Corpus with 5500 Hours of Speech from Parliamentary Debates",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPart of this development work was carried out within the frame of the infrastructural project HUMINFRA."
  },
  {
    "objectID": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html#code",
    "href": "posts/2023-03-09-rixvox-a-swedish-speech-corpus/index.html#code",
    "title": "RixVox: A Swedish Speech Corpus with 5500 Hours of Speech from Parliamentary Debates",
    "section": "Code",
    "text": "Code\n\nThe code for reproducing results in this article can be found on https://github.com/kb-labb/riksdagen_anforanden."
  },
  {
    "objectID": "posts/2021-03-28-ad-classification/index.html",
    "href": "posts/2021-03-28-ad-classification/index.html",
    "title": "A multimodal approach to advertisement classification in digitized newspapers",
    "section": "",
    "text": "The digital archives at The National Library of Sweden hold millions digitized of historical newspaper pages. Researchers accessing these collections for quantitative research purposes commonly inquire whether it is possible to search or filter the contents based on their research needs. One such request is to filter out all advertisement content in a query and select only the editorial content.\nWhile OCR digitizes the text in scanned images of newspaper pages, it is unfortunately not able to determine whether the text it has digitized is an article title, the body text of an article or an advertisement. Such contextual information is lost in the process of digitization. An important aspect of our work at KBLab is thus to develop automated methods to recover the information that is implicitly present in the structure and layout of newspaper pages, but cannot as of yet adequately be captured by digitization tools.\nA human reader looking at a newspaper page is generally able to distinguish advertisements from editorial content quickly and accurately at a glance. Advertisements tend to be visually distinct, featuring bright colorful designs, all capitalized text, large and distinctive fonts, and inverted text colors (light color text against dark backgrounds). A quick visual pass is therefore more often than not all that is required. Wherever any ambiguity persists, it can be resolved by reading a portion of the text. The presence of named commercial entities and logos along with advertisement-specific language tends to clear up any lingering uncertainty.\nIdeally, a machine learning model built to classify advertisements should be able to incorporate information from an array of senses similar to the ones we are using when making a determination. And since the way we classify content is through a combination of visual and contextual language clues, we aim for our model to do the same."
  },
  {
    "objectID": "posts/2021-03-28-ad-classification/index.html#objective",
    "href": "posts/2021-03-28-ad-classification/index.html#objective",
    "title": "A multimodal approach to advertisement classification in digitized newspapers",
    "section": "Objective",
    "text": "Objective\nOur objective with this article is to propose and evaluate an advertisement classifier trained as a single joint model using three diverse set of features: images, text and metadata. In order to further investigate the contribution of each component part of this model, ablation studies are performed where selected model components are omitted in the training."
  },
  {
    "objectID": "posts/2021-03-28-ad-classification/index.html#data",
    "href": "posts/2021-03-28-ad-classification/index.html#data",
    "title": "A multimodal approach to advertisement classification in digitized newspapers",
    "section": "Data",
    "text": "Data\nThe dataset consists of 35 sampled editions of the newspaper Svenska Dagbladet from 2017-03-29 to 2019-05-25 previously used for newspaper section classification (Rekathati, Hurtado Bodell, and Magnusson n.d.). Stratified sampling was performed based on weekday, meaning 7 editions were randomly sampled for each day of the week (Monday, Tuesday, ‚Ä¶) during the period.\nWhen OCR procedures are applied to newspaper page images they perform a segmentation of the contents of the page. Figure¬†1 below illustrates how the procedure may look like. The segmentation process first attempts to detect article zones (orange). The key word here is attempts, as it seldom is completely successful. Within the detected article zones it performs another pass to detect text blocks (blue) and image blocks (green). The unit of observation, i.e.¬†the data we are going to train and predict on in this work, consists of the text and image blocks (blue and green boxes).\n\n\n\n\n\n\n\nFigure¬†1: OCR segmentation of newspaper content results in identified blocks of text (blue) and images (green). These are the observational unit we are trying to classify as either editorial or advertisement content.\n\n\n\n\n\nAnnotation of training and evaluation sets\nThe 35 sampled editions were manually annotated. Every observation (segmented text or image box) was assigned a label of either ad or editorial. Ads include sponsored, paid for and commercial content including advertisements for charity organizations and including the newspaper‚Äôs own campaigns for subscription services, wine tastings, workshops, etc. However, links and information referring readers to extra coverage of news stories online were annotated as editorial.\nA visual annotation tool was used to label the observations. This tool highlighted existing observations when the annotator hovered over them in an image. A label was assigned by clicking over the highlighted area. In order to annotate the data as efficiently as possible, the annotation task was divided in the following steps:\n\nPerform a first pass over all pages in the sampled editions. If the whole page consists of editorial content, mark the entire page to be annotated as editorial. If the page consists of a full page ad, mark all the observations to be annotated as ad. If the page contains a mix of editorial and commercial content, mark the page as mixed.\nPerform a second pass over the pages tagged as mixed. In these pages all segmented boxes containing commercial content are annotated as ad using a visual annotation tool (see Figure Figure¬†2) to the right.\nProgrammatically assign all the remaining observations which are yet to receive a label to the class editorial.\n\n\n\n\n\n\n\n\n\nFigure¬†2: Segmented boxes containing ads highlighted in red.\n\n\n\nAs a general rule this process was quick and yielded mostly accurate results. However, since the annotator was required to hover over an area of the picture for the segmented box to be highlighted, it was possible for some segmented boxes to go undetected and unlabeled in step 2 of the annotation procedure. A future more flexible and fool proof method of annotating would be to select the the entire area covered by ads and then later ‚Äì if necessary ‚Äì assign all segmented boxes which fall into that area to the corresponding class. We will likely switch to this annotation strategy in the future, as it works independently of whatever OCR software was used. That is, even if the OCR and its segmentation were to change, the annotations would still be valid. Labeling the area as opposed to the OCR boxes would further allow us to train object detection and semantic segmentation models.\n\n\nDescriptive statistics\nThe dataset was divided into a train and validation set using a time series split. The training set consisted of editions from 2017-04-08 to 2018-10-16, and the validation set editions from 2019-01-08 to 2019-04-05.\n\nMost of the segmented boxes were identified as text boxes (around \\(88\\%\\)), whereas only roughly \\(12\\%\\) were detected as images by the OCR software. A quarter to a third of the content in the newspaper is ads.\n\n\n\n\n\n\n\nFigure¬†3: Histograms over the distribution of segmented boxes (i.e.¬†observations) per newspaper page (Fig. 3a), and the distribution of character lengths over observations (Fig. 3b)\n\n\n\n\nThe mean number of segmented boxer per newspaper page in the samples was 44. The majority of these boxes contained less than 50 characters of text all in all.\n\n\nFeatures\nThe majority of features used for classification were extracted from pretrained vision and language models. This feature extraction process is described in the method. Regular tabular features consisted of metadata relating to the segmented boxes. The OCR process logs positional information of each segmented box. In other words the \\(x\\) and \\(y\\) coordinates of the upper leftmost corner of the segmented box in terms of pixel position, as well as width extending rightwards from \\((x, y)\\) as well as height extending downwards from \\((x,y)\\). Additionally, weekday (Monday, Tuesdary, ‚Ä¶) was used as a categorical feature.\n\n\n\n\n\n\n\n\nFigure¬†4: Metadata associated with a segmented box."
  },
  {
    "objectID": "posts/2021-03-28-ad-classification/index.html#method",
    "href": "posts/2021-03-28-ad-classification/index.html#method",
    "title": "A multimodal approach to advertisement classification in digitized newspapers",
    "section": "Method",
    "text": "Method\nThree different neural networks are used to extract useful features from images and text. Their respective outputs are concatenated and connected in a fully connected (FC) classifier. Additional metadata features from the OCR process are also concatenated to the feature vector fed as input to this FC classifier. In particular, positional information is added, since CNNs are designed to be (approximately) invariant against translations (movement) of objects within an image. That is, they are designed to output similar activations regardless of where an object happens to appear in an image. However, for our advertisement model, we want it to be able to reason spatially.\n\n\n\n\n\n\nFigure¬†5: The full advertisement classifier model. In the results section we remove and isolate certain components of the model to investigate the contribution of each part.\n\n\n\n\n\nModel code can be found here.\nAll model parameters in the above model are set to be trainable in each of the models. Thus, they are all jointly ‚Äúfine tuned‚Äù for the classification task of detecting advertisements.\n\nCNN models\nTwo, initially identical, CNN models are trained side by side. One to extract features from a zoomed out ‚Äúglobal‚Äù level, and the other to focus on learning ‚Äúlocal‚Äù zoomed in features. The local CNN model takes as input images of the cropped segmentation boxes depicted in Figure¬†4 and Figure¬†1. We crop the segmented boxes by using the available positional information: \\((x, y)\\), width and height. Thus, one such cropped image box exists for every observation in our dataset.\nThe second CNN model, in contrast, receives an image of the entire newspaper page where the observation in question is located. Here, the rationale being that the context surrounding an observation is important in determining whether the observation is an or not. A consequence of this is that the same newspaper page image is passed as input to the model for however many segmented boxes exist on said page. Recall from the histograms in Figure Figure¬†3, that a mean of 44 segmented boxes (observations) exist for each newspaper page. Each time the global image is fed to the model, it is therefore paired with a different local segmented box however.\nThe backbone model used for both CNNs is an Efficientnet-B2 (Tan and Le 2019) with pretrained Imagenet weights. The classification head is discarded and instead the output of the convolutional layers are captured. This output is a 1408-dimensional vector.\n\n\nKB-BERT\nSwedish BERT base (Malmsten, B√∂rjeson, and Haffenden 2020) is applied as a feature extractor for any text associated with the segmented boxes. Commonly the embedding of the [CLS] token is extracted and used to fine tune models for downstream classification tasks (Devlin et al. 2018). Here, we follow this procedure and extract the 768-dimensional embedding corresponding to the [CLS] token and concatenate it to the previously extracted image features.\nThe observations in our data do not always have text content associated with them. Images, for example, have no OCR text fields. Segmented text detected in observations also vary in length all the way from single characters to series of paragraphs. In this work the maximum sequence length is set to 64. Padding is applied to the sequences of all observations whose number of tokens do not reach 64. This means that [PAD] tokens are added until we reach a sequence length of 64. Conversely, we truncate all observations exceeding a sequence length of 64 tokens.\nNo preprocessing or cleaning of the text is performed. KB-BERT is a cased model. However, ‚Äúcased‚Äù does not take into account all caps text as well as it does uncased text. In advertisements all caps text occur fairly common. The unrecognized tokens then often gets split up in many subtoken components or tokenized as [UNK] (unknown).\n\n\nTraining and validation split\nA time series split was chosen for the train and validation sets. This decision was informed by two main reasons:\n\nA simple random sample (SRS) of observations risked introducing test set leakage into the model. Since the global CNN model took the same image of the entire newspaper page as input multiple times, a SRS would have resulted in the same input data being exposed both to the train and validation sets.\nA time series split increases the difficulty of the prediction task. The model has to predict future data as opposed to data for which it has seem recent examples.\n\n\n\nMetadata features\nOnly five variables were added from OCR metadata. Positional information: x, y, width, height; and the weekday which was treated as a categorical variable for which a separate embedding layer was created. Categorical variables can be treated similar to word/token embeddings in language models. We ask our model to learn useful embeddings for the weekdays, just like BERT learns token embeddings for its vocabulary.\nIn the end a metadata feature vector of 7 dimensions was concatenated to the BERT and image features.\n\n\nFC classifier\nA fully connected classifier was built on top of the previously mentioned methods. The FC classifier consisted of a single hidden layer with 512 neurons. As input it took the \\(1408+1408+768+7\\) features from images, text and metadata respectively. The hidden layer applied a ReLU activation and a dropout layer with a dropout fraction of \\(0.3\\). The last output layer was a single neuron with sigmoid activation and binary cross entropy loss.\n\n\nHyperparameters, optimizer and hardware\nThe Adam optimizer was used with an initial learning rate of 0.00002. This learning rate was decreased by a factor of 0.65 every epoch. Models were trained 2 to 8 epochs, depending on when validation performance was observed to level off. A batch size of 16 was used for the full model. The smaller models were trained with larger batch sizes, and in general the learning rate was adjusted higher with increasing batch size.\nAll models were trained on a GeForce RTX 2080 TI. Total training time varied from circa 15 minutes for the smaller models to about 2 hours for the largest model."
  },
  {
    "objectID": "posts/2021-03-28-ad-classification/index.html#results-and-discussion",
    "href": "posts/2021-03-28-ad-classification/index.html#results-and-discussion",
    "title": "A multimodal approach to advertisement classification in digitized newspapers",
    "section": "Results and discussion",
    "text": "Results and discussion\nResults seem to indicate that all component models are useful. Some (global image features) provide a bigger boost than others, but all of them do contribute to a better overall performance of the full prediction model. The results displayed in the table below show that the full model reaches an accuracy of \\(97.2\\%\\). The model‚Äôs sensitivity, the proportion of actual ads it detects and labels as ad is \\(94.9\\%\\), whereas its specificity (proportion of actual editorial content it detects as editorial) reaches \\(98.4\\%\\). We have used a decision threshold of \\(0.5\\) to decide whether an observation should be classified as ad (\\(&gt;0.5\\) gets classified as ad). Researchers using these results can themselves adjust the decision threshold to boost either specificity or sensitivity depending on what is most important to them. Most researchers likely want a high specificity, i.e.¬†for the model to retain as much of the actual editorial content as possible.\nThe CNN Global model was a particularly strong feature extractor. It seemingly was succesful in incorporating spatial information when generating predictions. Since this model only saw the entirety of the newspaper page as input, it should not be able to reason very well about the spatial position of ads. Its performance seems to suggest that either i) the metadata features allow the model to combine image features with spatial information, or ii) the CNN architecture is not entirely invariant to objects being appearing at different locations of an image (translation invariance).\n\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(reactable)\n\ndf &lt;- arrow::read_feather(\"df_preds.feather\")\n\nmetrics_by_txtlength &lt;- function(txt_len,\n                                 df, \n                                 preds_col, \n                                 probs_col,\n                                 decision_threshold = 0.5, \n                                 observation_type = NA){\n  \n  df &lt;- df %&gt;%\n    filter(text_length &gt;= txt_len)\n  \n  if (!is.na(observation_type)){\n    df &lt;- df %&gt;%\n      filter(type == observation_type)\n  } else{\n    observation_type = \"All\"\n  }\n  \n  df[, preds_col] &lt;- 0\n  df[df[, probs_col] &gt; decision_threshold, preds_col] &lt;- 1\n  conf_mat &lt;- table(df %&gt;% pull(preds_col), df %&gt;% pull(label))\n  conf_mat &lt;- caret::confusionMatrix(conf_mat, positive=\"1\")\n  \n  return(tibble(accuracy = conf_mat$overall[\"Accuracy\"],\n                sensitivity = conf_mat$byClass[\"Sensitivity\"],\n                specificity = conf_mat$byClass[\"Specificity\"],\n                threshold = decision_threshold,\n                text_length = txt_len,\n                model = preds_col,\n                type = observation_type))\n}\n\npreds_col &lt;- c(\"preds_bertgloballocal\", \"preds_bertglobal\", \"preds_global\",\n                \"preds_local\", \"preds_bertlocal\", \"preds_bert\")\nprobs_col &lt;- c(\"probs_bertgloballocal\", \"probs_bertglobal\", \"probs_global\",\n                \"probs_local\", \"probs_bertlocal\", \"probs_bert\")\n\nobservation_type &lt;- c(NA, \"Image\", \"Text\")\n\nres &lt;- vector(mode = \"list\", length = length(observation_type) * length(preds_col))\ni &lt;- 1\nfor (j in 1:6){\n  \n  for (type in observation_type){\n    res[[i]] &lt;- metrics_by_txtlength(txt_len=0,\n                                   df, \n                                   preds_col = preds_col[j], \n                                   probs_col = probs_col[j],\n                                   decision_threshold = 0.5,\n                                   observation_type = type)\n    \n    i &lt;- i + 1\n  }\n}\ndf_table &lt;- do.call(bind_rows, res)\n\ndf_table[\"model\"] &lt;- recode(df_table$model, \n                             preds_bertgloballocal = \"Full model\",\n                             preds_bertglobal = \"Bert + Global\",\n                             preds_global = \"Global\",\n                             preds_local = \"Local\",\n                             preds_bertlocal = \"Bert + Local\",\n                             preds_bert = \"Bert\")\n\nreactable(df_table %&gt;% select(model, accuracy, sensitivity, specificity, type),\n          defaultSorted = c(\"type\", \"accuracy\"),\n          highlight = TRUE,\n          defaultPageSize = 9,\n          # searchable = TRUE,\n          columns = list(\n            model = colDef(\"name\" = \"Model\"),\n            accuracy = colDef(name = \"Accuracy\",\n                             format = colFormat(digits = 3),\n                             defaultSortOrder = \"desc\"),\n            sensitivity = colDef(name = \"Sensitivity\",\n                             format = colFormat(digits = 3)),\n            specificity = colDef(name = \"Specificity\",\n                             format = colFormat(digits = 3)),\n            type = colDef(name = \"Data subset\",\n                             format = colFormat(digits = 3))\n            ),\n          rowStyle = JS(\"function(rowInfo) {\n                            if (rowInfo.row['model'] == 'Full model') {\n                              return { background: 'rgba(0, 0, 0, 0.05)' }\n                            }\n                          }\"\n           ))\n\n\n\n\n\n\n\nIn general specificity is higher than sensitivity. This can be seen as a positive trait of all models as researchers tend to care more about the model retaining as much editorial content as possible (even if this means some ads slip through). Specificity also tends to increase with when more text is available in the segmented OCR box. Most long text sequences tend to be paragraphs from editorial content. It is comparatively rare for ads to feature long cohesive text paragraphs. The models also generally perform better on text data as opposed to image data.\nUpon further inspection of the model predictions, it became apparent that about one third to one half of the faulty predictions were in fact cases where the annotator had mislabeled examples. Most annotation mistakes occurred as a result of not finding all segmented advertisement boxes on a given page. This was in part due to annotator error, but in part ‚Äì we discovered ‚Äì also because it was not possible to highlight some of the smaller segmented boxes with the annotation tool.\n\n\nCode\nlibrary(gridExtra)\nmetrics_list &lt;- vector(\"list\", length = 6)\n\ni &lt;- 1\nfor (j in 1:length(metrics_list)){\n  metrics_list[[i]] &lt;- purrr::map_dfr(.x = 0:500, \n                                     df = df,\n                                     .f = metrics_by_txtlength,\n                                     preds_col = preds_col[j], \n                                     probs_col = probs_col[j],\n                                     decision_threshold = 0.5,\n                                     observation_type = NA)\n  i &lt;- i + 1\n}\n\ndf_metrics &lt;- do.call(bind_rows, metrics_list)\n\ndf_metrics &lt;- df_metrics %&gt;%\n  mutate(model = stringr::str_remove(model, \"preds_\"),\n         model = recode(model,\n                        \"bertgloballocal\" = \"full_model\",\n                        \"bertglobal\" = \"bert + global\",\n                        \"bertlocal\" = \"bert + local\"))\n\n\np1 &lt;- ggplot(df_metrics, aes(x = text_length, \n                             y = sensitivity, \n                             color = model,\n                             linetype = model,\n                             size = model)) +\n  geom_line(size = 0.3) +\n  geom_point(data = df_metrics[df_metrics$text_length == 0, ],\n             size = 0.7, \n             show.legend = FALSE) +\n  theme_light(base_size = 5) +\n  scale_linetype_manual(values = c(\"full_model\" = 1, \"bert + global\" = 1, \n                                   \"global\" = 2, \"bert + local\" = 1, \n                                   \"local\" = 2, \"bert\" = 1),\n                        breaks = c(\"full_model\", \"bert + global\", \"global\", \"bert + local\",\n                                   \"local\", \"bert\")) + \n  scale_color_manual(values = c(\"full_model\" = \"grey10\", \"bert + global\" = \"steelblue\", \n                                 \"global\" = \"steelblue2\", \"bert + local\" = \"firebrick\", \n                                 \"local\" = \"firebrick2\", \"bert\" = \"orange\"),\n                     breaks = c(\"full_model\", \"bert + global\", \"global\", \"bert + local\",\n                                \"local\", \"bert\")) +\n  scale_size_manual(values = rep(0.5, 6),\n                    breaks = c(\"full_model\", \"bert + global\", \"global\", \"bert + local\",\n                               \"local\", \"bert\")) + \n  labs(x = \"&gt;= Character Length\",\n       y = \"Sensitivity\", \n       title = \"Proportion of actual advertisement content detected (sensitivity) \nevaluated on all observations equal to or greater than the character length\",\n       color = \"Model\",\n       linetype = \"Model\") +\n  guides(color = guide_legend(override.aes = list(size=0.3))) +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5))\n\n\np2 &lt;- ggplot(df_metrics, aes(x = text_length, \n                             y = specificity, \n                             color = model,\n                             linetype = model,\n                             size = model)) +\n  geom_line(size = 0.3) +\n  geom_point(data = df_metrics[df_metrics$text_length == 0, ],\n             size = 0.7, \n             show.legend = FALSE) +\n  theme_light(base_size = 5) +\n  scale_linetype_manual(values = c(\"full_model\" = 1, \"bert + global\" = 1, \n                                   \"global\" = 2, \"bert + local\" = 1, \n                                   \"local\" = 2, \"bert\" = 1),\n                        breaks = c(\"full_model\", \"bert + global\", \"global\", \"bert + local\",\n                                   \"local\", \"bert\")) + \n  scale_color_manual(values = c(\"full_model\" = \"grey10\", \"bert + global\" = \"steelblue\", \n                                 \"global\" = \"steelblue2\", \"bert + local\" = \"firebrick\", \n                                 \"local\" = \"firebrick2\", \"bert\" = \"orange\"),\n                     breaks = c(\"full_model\", \"bert + global\", \"global\", \"bert + local\",\n                                \"local\", \"bert\")) +\n  scale_size_manual(values = rep(0.5, 6),\n                    breaks = c(\"full_model\", \"bert + global\", \"global\", \"bert + local\",\n                               \"local\", \"bert\")) + \n  labs(x = \"&gt;= Character Length\",\n       y = \"Specificity\", \n       title = \"Proportion of actual editorial content detected (specificity) \nevaluated on all observations equal to or greater than the character length\",\n       color = \"Model\",\n       linetype = \"Model\") +\n  guides(color = guide_legend(override.aes = list(size=0.3))) +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5))\n\ngrid.arrange(p1, p2, ncol=2)\n\n\n\n\n\n\n\n\n\nFigure¬†6: The plots above display the sensitivity and specificity respectively for all observations of equal or greater length than the specified character length on the x-axis. In general researchers tend to be interested in conducting research on editorial content. Detecting and assigning correct labels to actual editorial content (specificity) is therefore of importance.\n\n\n\n\n\nA closer look at predictions\nThe majority of predictions made by the full model are accurate. Below is an example where ads and editorial content are mixed on the same newspaper page.\n\n\n\n\n\nTip: Hover over the image with your mouse to view the model‚Äôs predictions.\nAd (red)\nEditorial (yellow)\nOur multimodal advertisment classifier did not have much trouble separating the ads on the right side from the editorial content on the left. It made only a handful of small mistakes.\nNext, we chose to display predictions on a page where our full model performed uncharacteristically poorly. These predictions can be seen in Figure Figure¬†7 below. The example also works great to display how the global image features model manages to spatially reason about the location of advertisements on a page.\n\n\n\n\n\n\n\nFigure¬†7: The predictions of three different models. Ad (red) and editorial (yellow). The global image features model seems to be able to incorporate spatial information in its predictions. Possibly from the metadata features that were added, but possibly also from learning specific ads-on-the-right-side or ads-on-the-left-side type of neuron signals whenever certain parts of the image has lots of color and images.\n\n\n\n\nThe Global image features model occasionally displays problematic behavior when paired up with the BERT model. We haven‚Äôt been able to figure out exactly why, but the combination of these two models occasionally seems to cause confusion in spatial reasoning. From all of the individual component models, we see that the Global image feature model is the one most unsure in its predictions about whether the editorial article paragraphs are actually editorial content. This uncertainty gets magnified for some reason when it is paired up with Bert. Further studies would be useful to investigate what causes this behavior. An especially interesting approach would be to feed parts of pages as input to the network as opposed to the entire page (expanding the area around the existing observation and cropping more of it).\nAdvertisement observations generally exhibit strong spatial correlation, as do editorial content. Ad observations tend to be found close to other ad observations, and vice versa. Thus we want to incorporate information from the area surrounding any given observation, while at the same time potentially tempering any potential problematic behavior from including an entire newspaper page. Either case, the occasional spatial confusion of the global model warrants further study.\n\n\n\n\n\n\n\nFigure¬†8: Example of output confidence score of the different component models in classifying a given observation to the class ad.\n\n\n\n\nWe can see that the combination of BERT+Global sometimes causes inconsistent and problematic predictions. All predictions displayed in Figure¬†8 and Figure¬†9 belong to the editorial class. However, the BERT+Global model confidently predicts them all as ad despite its two component models‚Äô scores being below 0.5 for all observations.\n\n\n\n\n\n\n\nFigure¬†9: Model score for predicting ad.\n\n\n\n\n\n\nModels without positional metadata variables\nWere the positional metadata features at all useful for model prediction? An easy way to investigate is by checking the BERT model‚Äôs predictions on images. BERT should not be able to predict images without metadata features. In fact, we can see in the table below that BERT without positional metadata features resorts to predicting editorial for every single image it encounters. This makes sense as editorial is the majority class. When faced with a complete absence of useful information, the model defaults to guessing the most frequently occurring outcome. The distribution of predictions of the BERT model with and without metadata features is as follows:\n\n\n\n\n\n\nSimilarly, the global CNN model should have trouble knowing what to predict when it is fed the same image of the entire newspaper page as input for every observation within a page. The performance does deteriorate somewhat, but it still performs surprisingly well in away that is quite difficult to explain."
  },
  {
    "objectID": "posts/2021-03-28-ad-classification/index.html#future-improvements",
    "href": "posts/2021-03-28-ad-classification/index.html#future-improvements",
    "title": "A multimodal approach to advertisement classification in digitized newspapers",
    "section": "Future improvements",
    "text": "Future improvements\nThe most obvious avenue for improvement is switching over to an alternate annotation method where we label the area covered by ads as opposed to explicitly labeling the OCR segmentation boxes. First and foremost because it is quicker and less error prone, but also because the OCR segmentation boxes can indirectly be annotated once we have area annotations. We can check whether the area of the OCR segmentation intersects with the labeled advertisement area beyond some threshold. Furthermore, a changed annotation approach allows for more flexibility in model choice. It opens up opportunities for applying object detection and semantic segmentation models.\nAnother easy way to boost the score of the model is to perform some post processing of the predictions. If an editorial prediction is completely surrounded by advertisement predictions, it is quite likely that the editorial prediction label was mistaken. Same goes for ad predictions surrounded on all sides by editorials. Simple postprocessing of can probably yield a boost in predictive performance.\nFinally, the ultimate advertisement model should be context aware, much like modern transformer-based language models. Currently, we crudely incorporate the context by supplying an image of the entire page as input to one of the CNN models. This can most likely be improved via some kind of a self-attention approach applied on the image embeddings of observations within a newspaper page."
  },
  {
    "objectID": "posts/2021-03-28-ad-classification/index.html#code",
    "href": "posts/2021-03-28-ad-classification/index.html#code",
    "title": "A multimodal approach to advertisement classification in digitized newspapers",
    "section": "Code",
    "text": "Code\nCode (without data) can be found at https://github.com/kb-labb/ad_classification ."
  },
  {
    "objectID": "posts/2023-08-29-kubord/index.html",
    "href": "posts/2023-08-29-kubord/index.html",
    "title": "Words unboxed: discovering new words with Kubord",
    "section": "",
    "text": "Since the last century, linguists have gradually become increasingly interested in quantitative research. One of the most well-known examples of how linguistics can benefit from quantitative methods is the story of the American linguist George Kingsley Zipf. In the 1930s, Zipf broadened his scientific focus from linguistics to statistics. This fascination led to the formulation of two laws: the law of abbreviation and a law addressing frequency distribution. The latter, known simply as Zipf‚Äôs law, states that the distribution of words in a natural language is inversely proportional to their rank in the frequency distribution. For example, the second most common word in a corpus occurs only half as frequently as the first one, the third word is half as frequent as the second one, and so on.\nThis law has been used in various research projects, including the analysis of the Voynich manuscript and, more recently, the investigation of AI-generated texts.\nAdvancements in quantitative research such as language technology have also proven to be extremely helpful in other fields of language research such as lexicography. Language researchers have, for a long time ago, discovered the potential of exploring language statistics, as they provide us with important information about language use, such as word formation and language change. In this regard, newspaper data seem to be an excellent source for identifying the emergence of new words or tracking changes in the meaning of existing ones, since this data typically reflect the language used at the time it was published. Books, on the other hand, while providing vast amounts of high-quality data, the lengthy publishing process and the specific nature of literary language make them less anchored in time, hence not as fitting for the aforementioned purposes compared to newspapers.\n\nKubord and Kubord 2\nWith this in mind, KBLab together with Spr√•kbanken Text, has been working towards the development of freely available datasets to support research in, but not limited to, lexicography. We have aimed at maximizing the usefulness of the data for researchers while at the same time respecting our copyright protection laws, and the result of these efforts are Kubord ‚Äì a data set containing annotated word frequencies from modern Swedish newspapers. To conduct the analysis, the texts have been processed using the text analysis tool Sparv, developed at Spr√•kbanken Text. In addition to calculating word statistics, the newspaper data has been automatically annotated with additional linguistic information such as part-of-speech tags and lemmas.\nThe original Kubord data set consists of 84 datasets from Dagens Nyheter, Svenska Dagbladet, Aftonbladet, Expressen, √ñstg√∂ta Correpondenten, and G√∂teborgsposten. The majority of the data spans from 2010 to 2021; however, word frequencies for Dagens Nyheter from 2001-2009 are also available.\nExtending the work on Kubord, 75 new datasets, marked as Kubord 2, have recently been developed and made publicly available. Kubord 2 can be seen as an enriched version of the original corpora. The novelty lies in that the analyses are supplemented with information about the frequency of the pairs of syntactically related words, such as a word acting as the subject paired with a word acting as the main verb, for all the words present in the data. Similar to Kubord, Kubord 2 covers the years 2010-2021, and the aim is to update the collection with newer data as it becomes available.\n\n\n\n\n\nKubord 2 data available in the research tool Korp.\n\n\n\n\nMaking the most of Kubord\nThe usefulness of having Kubord available for lexicographers has been described with examples in the Spr√•kbanken blog post titled ‚ÄúHur f√•ngar vi upp svenskans nya ord med hj√§lp av Kubord?‚Äù (How do we capture new Swedish words with the help of Kubord?).\nAs highlighted in the article, tracking changes in word popularity (measured by frequency counts) can provide insight into changes in society - how our eating habits evolve, how we spend our free time, and more. It can therefore serve as a valuable source of knowledge for humanistic researchers such as linguists, historians, or sociologists. Additionally, it sheds light on which words come into use and when, which is especially useful in the context of lexicographical projects such as the Swedish Academy contemporary dictionary project (Svenska Akademiens samtidsordb√∂cker).\nThanks to the annotated word statistics obtained from Kubord, we can conclude that some words that became more prominent during 2020-2021 were:\nbarnfridsbrott, charkbricka, freeskiing, fucking, gastropub, glamping, hj√§rndimma, h√∂gh√∂jdsbana, incel, intimitetskoordinator, j√§tteloka, kakuro, klassikerskydd, kulturtolk, lockdown, magnetfiskare, mikrostat, minoritetsstress, miso, mockument√§r, mobildata, mobilitetshus, nagelsalong, naturhus, nettonollutsl√§pp, n√§ringsterapeut, parallellsamh√§lle, powerwalk, preklinisk, prosecco, reduktionsplikt, r√§ttspsyk, r√∂stassistent, salsiccia, sexsomni, skidalpinism, skills, skyddsperson, sportswashing, streetfood, syrah, trygghetspension, uppl√§ggningskostnad, utsl√§ppssiffra, verkst√§llighetshinder, villkorstrappa, yes.\nThe majority of these words will most likely be incorporated in future editions of the Swedish Academy‚Äôs contemporary dictionaries.\n\n\nUsing the research tool Korp for Kubord\nKorp, a research tool developed at Spr√•kbanken Text, is a tool that supports quantitative research of (sequences of) annotated words, and is heavily used by the Swedish lexicographers. Since Kubord data set is restricted to words rather than texts, not all of Korp‚Äôs functionalities are available, but some of the most important ones are, such as the statistics and the word picture mode. As an example, when a lexicographer investigates different kinds of word combinations, a word picture is useful, as it gives an overview of selected syntactical environments of a word. For nouns the word picture contains typical verbs, prepositions, pre-modifier, and post-modifiers, etc.\nBelow you find the word picture of the Swedish noun avtal, based on Kubord 2.\n\n\n\nWord picture of the Swedish noun avtal\n\n\nSome of the words in the first column of the word picture represent more or less free combinations that include avtal, e.g.¬†enligt (ett) avtal och med st√∂d av (ett) avtal. In the column with pre-modifiers you find (ett) nytt/skriftligt/muntligt/bilateralt/internationellt avtal, and more. This information is useful for a lexicographer when looking for good and typical language examples of the current word, which must support the meaning descriptions and, at the same time, demonstrate how the word is typically used.\nSome of the words and phrases that appear in the word picture also support a lexicographer‚Äôs work on providing valency information concerning avtal (see e.g.¬†the post-modifier including med).\nFinally, the word picture provides a lexicographer with recurrent word combinations including the noun and a verb, e.g.¬†teckna/sluta/ing√•/underteckna/l√∂pa ut/s√§ga upp ett avtal (see the columns 4‚Äì5).\nNow, let‚Äôs explore how we can supplement this example with information from the Kubord 2 data set by examining some of the new Swedish words mentioned above:\n\nstreetfood\n\nThe word picture of streetfood illustrates that this particular noun is often preceded by adjectives such as asiatisk, mexikansk, kantonesisk and vegansk. These adjectives clearly show the large selection, the international elements and the variety when it comes to street food of today. Furthermore, in written texts streetfood often is preceded by verbs such as servera, laga, s√§lja, √§ta och avnjuta. Obviously, both the seller‚Äôs and the buyer‚Äôs perspective on the sale of this type of food are represented in the list.\n\nlockdown\n\nA large number of more or less new words became more established in Swedish during the pandemic. One of those words is lockdown. (The more Swedish-sounding nedst√§ngning was also established but was not much used.) When it comes to lexicographic work, it can be discussed which of these pandemic related words are temporary and which are here to stay. A search in all texts in KB‚Äôs Svenska dagstidningar shows that the use of the word lockdown has decreased dramatically this year.\nA word picture for lockdown based on Kubord 2 gives a richer view of the word and shows that the actual shutdown is often specified by attributes such as total, (sten)h√•rd, strikt, fullskalig och fullst√§ndig, but that a lockdown can also be partiell, and in newspaper texts, the word is often a subject to the verb inf√∂ra and an object to the verb combination tr√§da i kraft. All this relevant information if the word ever finds its way into one of the Swedish dictionaries.\n\npreklinisk\n\nAmong the words that are more frequently used in recent years you also find the adjective preklinisk. A search in KB‚Äôs Svenska dagstidningar shows that the word has been used since at least 1945 in Swedish texts. The adjective also has different meanings in modern Swedish. For example, when preklinisk is followed by words like l√§kemedelsutveckling, studie(r) och test(er) in its word picture, it is with the meaning ‚Äòrelating to drug testing‚Äô. When preklinisk, on the other hand, is used together with the noun fas(er), it is with the meaning ‚Äòwhich applies to the early stage of a certain disease‚Äô. In other words, the word picture view gives support in sense differentiation.\nA Korp search also shows that it is, above all, the word form prekliniska (as in prekliniska tester) that is used in Kubord 2 and it is also something that a lexicographer takes into account when giving typical examples of how the word is used.\n\nverkst√§llighetshinder\n\nA final example word is verkst√§llighetshinder. It has increased strongly in use in Swedish texts since around 2012. The word is primarily used in the context of migration, and this context is clearly evident as that the name Migrationsverk is present in the word picture. Its word picture further reveals that the noun is used in, at least, two different contexts. On the one hand, a person can anm√§la or √•beropa obstacles to verkst√§llighetshinder. There may also exist a verkst√§llighetshinder, as in f√∂religga verkst√§llighetshinder.\nThe word is typically followed by the word combination f√∂r utvisning but it can also be av skyddsk√§l, which should also be included in a dictionary article. Consequently, the findings in Kubord 2 make both the description of the semantics of the words and its phraseological behavior more comprehensive.\n\n\nFinding the data set\nThe data sets can be downloaded from the Spr√•kbanken Text (Kubord 1 and Kubord 2) or accessed through Korp‚Äôs Kubord mode Korp‚Äôs Kubord mode. You can also conveniently search and explore the data through Korp‚Äôs API.\nKubord offers intriguing material for humanistic research, which would otherwise only be accessible to researchers within library premises due to copyright protection. While the context of sentences in which words occur is not available in these data sets, the annotated word frequencies and syntactically co-occurring words can still be useful for researchers interested in tracking and analyzing trends in new Swedish words.\nWith this in mind, there is a wealth of data waiting to be explored. Feel free to dive in and explore it yourself!\n\n\nAcknowledgements\nPart of this development work was carried out within HUMINFRA infrastructure project.\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{forsberg2023,\n  author = {Forsberg, Markus and Sikora, Justyna and Sk√∂ldberg, Emma},\n  title = {Words Unboxed: Discovering New Words with {Kubord}},\n  date = {2023-08-29},\n  url = {https://kb-labb.github.io/posts/2023-08-29-kubord/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nForsberg, Markus, Justyna Sikora, and Emma Sk√∂ldberg. 2023. ‚ÄúWords\nUnboxed: Discovering New Words with Kubord.‚Äù August 29, 2023. https://kb-labb.github.io/posts/2023-08-29-kubord/."
  },
  {
    "objectID": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html",
    "href": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html",
    "title": "Scientific discourse with BERTopic",
    "section": "",
    "text": "Topic modeling is a great way to discover and characterize the contents of a large text collection. It can help us understand what themes are present in a corpus. They can uncover trends and provide insights where a human, even an expert, would struggle. There are, however, limitations to topic modeling. The main one lies in the interpretation. The results are not always easily understood. Some topics may lack quality. There might be uncategorized documents. Other times the results do not align with a researcher‚Äôs expectation or intuition. The most important thing to understand is that topic models alone do not provide all the answers to the contents of a corpus. It is simply a tool to navigate large amounts of texts, and should be used as such.\nIn this post, we will describe a typical topic modeling use case using the transformer-based BERTopic on scientific abstracts. We will do this with the specific aim of highlighting the problems that might arise during the interpretation of the results. For further interested, an introduction to BERTopic and usage with KBLab‚Äôs BERT-models can be found here."
  },
  {
    "objectID": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#the-bertopic-pipeline",
    "href": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#the-bertopic-pipeline",
    "title": "Scientific discourse with BERTopic",
    "section": "The BERTopic pipeline",
    "text": "The BERTopic pipeline\nThe process of topic modeling with BERTopic is roughly as follows: collect the data ‚Üí transform the data into numerical representations ‚Üí reduce the dimensionality of these representations ‚Üí group data points into clusters ‚Üí describe the content of the clusters.\nHere, we‚Äôre working with a collection of scientific abstracts from the research field of education. We have downloaded abstracts alongside some metadata (for example: author names, publishing years and author affiliation). The abstracts were scraped from the Science Citation Index Expanded, which is a citation index of over 9000 scientific journals included in the Web of Science (WoS). 144 567 abstracts were collected between the years 1990-2022. Since scientific abstracts are short and distinct (this specific dataset has an average word count of 200), they are an excellent candidate for topic modeling.\nThe next step of the process is to convert the abstracts into a numerical representation. There are many ways of doing this but the preferred method is by using a pre-trained BERT model to generate embeddings. These are vector representations of the texts that capture the context of words and phrases. Since the data collection was limited to abstracts from papers written in English, we are able to use one of the many pre-trained English language models. In this case, we used DistilBERT, which is a small and fast Transformer model trained by distilling the bloated BERT base. DistilBERT provides a good tradeoff between performance and speed and is a good option, especially if you want to run the model on a laptop. The resulting embeddings are designed to capture complex relationships, making them high in dimension and thus difficult to work with. In order to perform the actual topic modeling, we need to reduce the dimensionality. This is done by using UMAP, which creates a low-dimensional representation of the abstract embeddings.\nThe goal of topic modeling, simply put, is to group documents into meaningful categories that ‚Äì hopefully ‚Äì represent some underlying topics. BERTopic does this by applying a density-based clustering algorithm called HDBSCAN. Very briefly, HDBSCAN groups together dense regions of data points in some feature space using a minimum-spanning tree (a type of graph) in order to connect all vertices of the tree with the minimum total weight.\nAt this point in the process, we have a set of clusters ‚Äì groups of documents that are similar in some way. But how do we understand those clusters? What do they actually have in common? The BERTopic pipeline solves this by applying a class-based Term Frequency-Inverse Document Frequency (TF-IDF) matrix. TF-IDF in its original form basically estimates the importance of a word in a document by comparing the frequency of a word in that document to the frequency of the same word in all other documents. The class-based TF-IDF does this on a cluster level by treating all documents in a cluster as one document. This results in a list of the top n most important words in a cluster. We let that list represent a topic.\nEt voil√†, our topic model is done. Let‚Äôs have a look at the results."
  },
  {
    "objectID": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#the-scientific-education-discourse",
    "href": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#the-scientific-education-discourse",
    "title": "Scientific discourse with BERTopic",
    "section": "The scientific education discourse",
    "text": "The scientific education discourse\nThe intertopic distance map is an interactive way of exploring the topics. It is quite intuitive ‚Äì the closer the topics are on the map, the more similar they are in terms of their content. The scientific educational discourse is here represented by 107 topics (as determined automatically by BERTopic as the optimal number of topics), organized into 20 larger clusters. In general, the bigger the cluster (and the topic), the more heterogeneous that cluster or topic will be. Take for example the large cluster in the top-left quadrant of the map. This cluster holds a dozen or so subtopics where some are obviously related: topic 86 and 91, for example, are clearly covid-19 topics. These are closely related to the online learning topic 25 and 38. This is no accident. The online learning topics, however, are at an even greater proximity to topic 0, which is the largest topic in the model. This topic is represented by the words ‚Äúinternal‚Äù, ‚Äúcultural‚Äù, ‚Äúinternational students‚Äù, ‚Äúhigher education‚Äù and ‚Äúexpatriates‚Äù. The relationship between these topics are less clear, which emphasizes the need of thorough examination of the quality of the topics at hand.\n\nSome patterns in the data are obvious, even to non-experts such as ourselves. As expected, there are several clusters of topics that represent teaching subjects: biology, medicine, accounting, physical education, programming, robotics and many more. There‚Äôs also several clusters of topics concerning various facets of being a teacher with topics characterized by words and phrases such as black teachers, teacher identity, mentors, coaching and so on. We can also identify different levels of the educational system, from schools for small children to grad school. There‚Äôs several socio-economical clusters that discuss, for example, class, intergenerational mobility, financial aid, segregation and charter schools. These are just some examples but they help give an indication that the topic model is, in fact, relatively sound. There is almost certainly more to this map that an expert could shed some light on. Interested parties are welcome to contribute.\nAnother way to get an overview of the topics and their internal relationship is by visualizing the potential hierarchical structure of the topic model. The number of topics can be overwhelming, especially since many topics are overlapping. By creating a hierarchical structure, we can easily trace the relationships between topics. As we can see in the graph, some of them are very similar, as demonstrated by for example topics 86 and 61, both concerned with covid. The two topics below, 33 and 17, are connected to medicine and nursing, which is a good fit with the articles about the pandemic. In the same branch we can also identify topics related to online learning and online discussions. Note the close proximity to the covid topics. Looking at the hierarchy can be especially useful if we would like to reduce the number of topics. The hierarchical clustering graph gives us an overview of how similar topics are and therefore can be a good indicator of how many topics could be potentially merged.\n\n\n\n\n\n\n\nFigure¬†1: Hierarchical topic structure of scientific abstracts in the field of education 1990-2022\n\n\n\n\nWe can also explore the topic evolution by providing timestamps for all documents present in the model. In this case, years. The plot represents the frequencies of documents within a topic for a specific year. We can also limit the number of topics we are interested in. Once again, following the steps of the covid topics, we can clearly see the emergence of massive open online courses around 2008. Due to the pandemic, the topic frequency spikes again.\n\n\n\n\n\n\n\nFigure¬†2: Online learning topics over time."
  },
  {
    "objectID": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#taking-bertopic-further",
    "href": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#taking-bertopic-further",
    "title": "Scientific discourse with BERTopic",
    "section": "Taking BERTopic further",
    "text": "Taking BERTopic further\nAnother aspect of topic models to keep in mind as a researcher is that a topic model alone does not constitute research. The topic model provides overview and helps us navigate large quantities of text but it is only of many tools in the quest for new knowledge. It might shine a new light on a dataset but in order to actually produce new knowledge, further analysis is needed. We provide an example of this below by correlating the topics with metadata found in the dataset, namely the nationality of authors. (This variable is not easily available in the dataset but can be extracted through some text wrangling methods that we will not elaborate further upon here.) With information not only about topics but also author affiliation at hand, we can examine what type of research emerges from specific cultural contexts. We do this by performing a correspondence analysis (CA) on our enriched topic model. Correspondence analysis (CA) is a statistical method for analyzing categorical data, conceptually similar to the more well-known principal components analysis (PCA). Using CA, we can visualize associations between topics and affiliations. The analysis is performed using Prince, a Python library for multivariate exploratory data analysis in Python. To reduce the amount of data points, only the 50 most well-represented countries in the dataset are included in the graph. The graph is still very busy, indicating that further reduction of data might be necessary.\n\n\n\n\n\n\n\nFigure¬†3: Correspondence analysis visualization over topics and author affiliation.\n\n\n\n\nBefore looking at the plot, there are some things to take into consideration. Interpreting the results of a CA is not exactly straightforward and a more in-depth guide can be found here, but the most important things to know is that (1) datapoints close to the origin are less distinct than data points further away and (2) a small angle between a topic and affiliation, indicates an association. The interpretation of the actual plotted data should ideally be done by a domain expert to be meaningful. We‚Äôll give it a go nonetheless.\nIn the left-lower quadrant, we can see that China is closely related to the covid-19 topics, which, to our knowledge, aligns well with the general Chinese discourse. China also appears to have a negative association to the USA, we can be found in the right-lower quadrant. USA stands out as a distinct scientific discourse in the right-lower quadrant, with very subject focused topics such as STEM, psychology and engineering. Other finds, that at least partially, validates our analysis is a cluster of nordic countries in the upper-right quadrant. This indicates a shared scientific educational discourse. Some topics that are distinct to the nordic countries are PISA, reflective practices and ethical values."
  },
  {
    "objectID": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#conclusions",
    "href": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#conclusions",
    "title": "Scientific discourse with BERTopic",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post, we have revisited BERTopic and provided some useful tools to mitigate the limitations of the method. Demonstrated here is a non-exhaustive list of ways to validate your topic model, as well as examples on how to perform further explorative data analysis using BERTopic as a base."
  },
  {
    "objectID": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#acknowledgements",
    "href": "posts/2023-03-17-scientific-discourse-with-bertopic/index.html#acknowledgements",
    "title": "Scientific discourse with BERTopic",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPart of this development work was carried out within HUMINFRA infrastructure project.\n\n\n\n\n\n\n\n\n\n\n\nHUMINFRA"
  },
  {
    "objectID": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html",
    "href": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html",
    "title": "Unearthing forgotten images with the help of AI",
    "section": "",
    "text": "Memory institutions like KB can often be faced with a palpable sense of overload. Partly this is an effect of the proliferation of new material now being produced by digital culture. With the number of items delivered to the library via electronic legal deposit rapidly increasing, how is any sort of order to be maintained? But it is also a question of dealing with inherited archival blindspots, where previous historical moments of mass media expansion created the conditions for parts of the collections to remain undescribed. Without the metadata that would make them discoverable, such items have now largely been consigned to the realm of forgetting that Aleida Assmann has described as ‚Äúthe passively stored memory that preserves the past‚Äù (Assmann 2010).\n\nThe image search demo can be accessed at https://lab.kb.se/bildsok\n\nA pertinent example of a material that, while preserved, lacks description - perhaps due to its perceived ephemerality historically and the limited valuation this has granted in terms of archival resources and attention - is visual heritage collections from the nineteenth and twentieth century. Descriptive cataloguing has long been a central part of KB‚Äôs making its collections accessible and searchable, but it has been impossible for each and every incoming item to be manually catalogued, given that legal deposit legislation dictates the library receives a copy of everything published in Sweden. Instead, certain items such as postcards or adverts have tended to be grouped together and classified under collective catalogue entries that often preclude the detailing of any specific information about the individual object per se.\nAlthough KB has a rich and diverse collection of c.¬†600,000 postcards, the lack of navigability and overview entrenched by scarce metadata has made the material hard for users to access - or even to be aware of its existence. Despite being preserved as part of our shared cultural heritage, such items are thus at risk of disappearing from view altogether."
  },
  {
    "objectID": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#forgotten-in-the-archive",
    "href": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#forgotten-in-the-archive",
    "title": "Unearthing forgotten images with the help of AI",
    "section": "",
    "text": "Memory institutions like KB can often be faced with a palpable sense of overload. Partly this is an effect of the proliferation of new material now being produced by digital culture. With the number of items delivered to the library via electronic legal deposit rapidly increasing, how is any sort of order to be maintained? But it is also a question of dealing with inherited archival blindspots, where previous historical moments of mass media expansion created the conditions for parts of the collections to remain undescribed. Without the metadata that would make them discoverable, such items have now largely been consigned to the realm of forgetting that Aleida Assmann has described as ‚Äúthe passively stored memory that preserves the past‚Äù (Assmann 2010).\n\nThe image search demo can be accessed at https://lab.kb.se/bildsok\n\nA pertinent example of a material that, while preserved, lacks description - perhaps due to its perceived ephemerality historically and the limited valuation this has granted in terms of archival resources and attention - is visual heritage collections from the nineteenth and twentieth century. Descriptive cataloguing has long been a central part of KB‚Äôs making its collections accessible and searchable, but it has been impossible for each and every incoming item to be manually catalogued, given that legal deposit legislation dictates the library receives a copy of everything published in Sweden. Instead, certain items such as postcards or adverts have tended to be grouped together and classified under collective catalogue entries that often preclude the detailing of any specific information about the individual object per se.\nAlthough KB has a rich and diverse collection of c.¬†600,000 postcards, the lack of navigability and overview entrenched by scarce metadata has made the material hard for users to access - or even to be aware of its existence. Despite being preserved as part of our shared cultural heritage, such items are thus at risk of disappearing from view altogether."
  },
  {
    "objectID": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#multimodal-ai-as-novel-entrance-point",
    "href": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#multimodal-ai-as-novel-entrance-point",
    "title": "Unearthing forgotten images with the help of AI",
    "section": "Multimodal AI as novel entrance point",
    "text": "Multimodal AI as novel entrance point\nRecent developments within AI offer a promising strategy for libraries and other GLAM institutions with large collections of undescribed visual heritage to counter this archival forgetting and make such material more visible. The emergence of powerful multimodal AI models makes it possible to interact with huge amounts of images in new ways, with or without preexisting metadata. This is evident, for example, in the computer vision technology underpinning Google‚Äôs image search function, which we now tend to take for granted on the phones in our pockets.\n\nGLAM: Galleries, Libraries, Archives and Museums.\n\nBy connecting the visual and textual domains in an innovative manner, multimodal AI has enabled more effective online search techniques, both in terms of text-to-image (i.e.¬†image retrieval from a textual description) and image-to-image search (otherwise known as reverse image search, i.e.¬†image retrieval from an image). The Open AI model CLIP was trained on a dataset of 400 million matching text-image pairs to learn to recognize visual concepts and their associated names - e.g.¬†an image of a cat and ‚Äúa photo of a cat‚Äù (Radford et al. 2021). This works via the use of vector space: through transforming the input text or image to a numerical representation (called embeddings), the AI model can return results based upon images with a similar representation (see Figure¬†1). When ‚Äúchurch‚Äù appears in a text, for instance, the model will generate a similar number representation for both the word description and the visual manifestation of a church, thus collapsing the distinction between the different modes and making them directly comparable. Such a technique can be used to identify all the images containing a church in a large collection of images, regardless of whether these images have previously been described as such. In short, it offers content-based image search independent of prior metadata.\n\n\n\n\n\n\nFigure¬†1: Matching similar images and texts within vector space. Image: Federico Bianchi."
  },
  {
    "objectID": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#applying-multimodal-search-at-the-library",
    "href": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#applying-multimodal-search-at-the-library",
    "title": "Unearthing forgotten images with the help of AI",
    "section": "Applying multimodal search at the library",
    "text": "Applying multimodal search at the library\nWe chose to explore the potential of this method for heritage material through a pilot project focused upon a selection of KB‚Äôs postcards. While the original version of CLIP was trained upon English text data, we turned to the Swedish adaptation of the model produced by (Carlsson et al. 2022), Swe-CLIP 2M, to enable free text search in Swedish. Since only part of the library‚Äôs postcard collection has been digitized, we could include 17,409 postcards in the project. Given that the back side of each postcard could include personal details such as names and addresses, we opted to employ only the front sides. Using this digital material as a dataset, we sought to investigate the relevance of cutting-edge AI search techniques in a library context.\n\n\n\n\n\n\n\nFigure¬†2: Interface for postcard image search.\n\n\n\n\nAfter close collaboration with the library‚Äôs developers, the project resulted in an image search demo, which, thanks to a recent licensing agreement, can now be openly accessed here. This amounts to an interface that allows these postcards to be searched according to either text or image search terms (see Figure¬†2), making a previously largely undescribed - and therefore undiscoverable - material amenable to the sort of granular search possibilities we are familiar with from online search engines.\nThe demo works by exploiting the multimodal capabilities of CLIP outlined above. So any text or image entered into the search box is run through the model, transformed into a numerical representation, and then compared with the equivalent representations for all of the postcards that have been previously processed and stored in a database. The search results are ranked according to (cosine) similarity, with the postcards with the closest representation to the search term appearing at the top of the list. Insofar as it prototypes a new entrance point to the collections, our demo provides a striking example of the transformative possibilities of vector databases for memory institutions."
  },
  {
    "objectID": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#test-and-explore",
    "href": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#test-and-explore",
    "title": "Unearthing forgotten images with the help of AI",
    "section": "Test and explore!",
    "text": "Test and explore!\nAs part of our wider mission at KBLab to contribute to wider use and discussion of AI tools for the heritage sector and beyond (B√∂rjeson et al. 2023), we encourage you to try out the image search demo and play around with its capabilities. Here we would like to offer a few brief pointers about how to use the demo and understand the results.\nThe first option is free text search: you can provide the search box with either a general term - i.e.¬†‚Äúkyrka‚Äù (Figure¬†3) - or a more specific set of terms, if you would like to find something more particular - i.e.¬†‚Äúkyrka och bl√• himmel bilar‚Äù (Figure¬†4). The second option is image search. Here you can either click on a given image in order to be provided with a results list of the most similar images in the collection (Figure¬†5), or you can upload your own image to find out which of the postcards are most closely related. (This latter option is particularly useful if you are a researcher interested in tracing the reception history and transmission of a particular image.) As the green tags at the top of Figure¬†5 suggest, the images have also been selectively enriched with various tags to enhance searchability: you can click on a tag to see which postcards share the same description, i.e.¬†the category ‚ÄúBorgholms kyrka‚Äù.\n\n\n\n\n\n\n\nFigure¬†3: Free text search in the postcard collection for the Swedish term ‚Äúchurch‚Äù.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†4: Search results for the Swedish search string: ‚Äúchurch and blue sky cars‚Äù.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5: Image-to-image search to find most closely related postcards.\n\n\n\n\nWhen it comes to understanding the search results, an important feature that needs to be considered is the production of relative similarity. This means that the model will always return a result of the 100 ‚Äúclosest results‚Äù, even if there are no exact matches for the given search term in the database. If we search for the Swedish term ‚Äúcat‚Äù, for example, the top results include postcards of a lynx, a goat and a bear, but no cats (fig.¬†@ref(fig:katt)). This is not because the model has misunderstood the search term, but rather that the particular data in the database happens not to include any such items, and that these other animals were the closest visual concepts identifiable in the data.\n\n\n\n\nSearching for the non-existent ‚Äúcat‚Äù in the postcard collection."
  },
  {
    "objectID": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#a-glimpse-of-the-future",
    "href": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#a-glimpse-of-the-future",
    "title": "Unearthing forgotten images with the help of AI",
    "section": "A glimpse of the future?",
    "text": "A glimpse of the future?\nWith this postcard demo project, we have illustrated the capacity for multimodal AI and vector databases to improve the searchability and accessibility of cultural heritage collections. As our specific case has shown, these techniques are particularly relevant as a means of transforming access to heritage material lacking metadata, with the multimodal dimension especially pertinent for visual heritage.\nThere are certain practical preconditions for the adoption of such technology in a heritage setting, most of which can be related to questions of funding: that the material has been digitized, that there are sufficient resources available for computation, data science and developer expertise, and that there are licensing agreements in place. But in broad brushstrokes, our pilot project still suggests a captivating vision of what the future of an AI-underpinned memory institution might look like with a little help from machine learning."
  },
  {
    "objectID": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#acknowledgements",
    "href": "posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai/index.html#acknowledgements",
    "title": "Unearthing forgotten images with the help of AI",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nA major part of the development and design of the postcard demo was carred out by KB‚Äôs developers, without whose efforts it would look nowhere near as pretty and be nowhere near as efficient. We would like to thank Matthias Nilsson, Krzysztof Bergendahl and Ebrima Faye for their work on the project!"
  },
  {
    "objectID": "cite.html",
    "href": "cite.html",
    "title": "How to cite KBLab",
    "section": "",
    "text": "If you have used the Lab‚Äôs resources or need to refer to the Lab in your research, please use the following reference:\nB√∂rjeson, L., Haffenden, C., Malmsten, M., Klingwall, F., Rende, E., Kurtz, R., Rekathati, F., H√§ggl√∂f, H., & Sikora, J. (2024). Transfiguring the Library as Digital Research Infrastructure: Making KBLab at the National Library of Sweden. College & Research Libraries, 85(4), 564‚Äì582. https://doi.org/10.5860/crl.85.4.564\nOr for BibTeX:"
  },
  {
    "objectID": "cite.html#acknowledgements",
    "href": "cite.html#acknowledgements",
    "title": "How to cite KBLab",
    "section": "Acknowledgements:",
    "text": "Acknowledgements:\nIf your project has been based at the Lab, please use the following formulation in any related publications:\n\n‚ÄùComputational resources and support for this research was provided by KBLab at the National Library of Sweden.‚Äù"
  },
  {
    "objectID": "cite.html#citing-specific-models",
    "href": "cite.html#citing-specific-models",
    "title": "How to cite KBLab",
    "section": "Citing specific models",
    "text": "Citing specific models\nIf you have used our models and need to cite the documentation we have produced for them, please use the following references:\n\nKB-BERT\n@misc{malmsten2020playing,\n  title={Playing with Words at the National Library of Sweden -- Making a Swedish BERT},\n  author={Martin Malmsten and Love B√∂rjeson and Chris Haffenden},\n  year={2020},\n  eprint={2007.01658},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n\nWav2vec2\n@misc{malmsten2022hearing,\n    title={Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language}, \n    author={Martin Malmsten and Chris Haffenden and Love B√∂rjeson},\n    year={2022},\n    eprint={2205.03026},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n\n\nOther models\nIf there is a post on this blog about the model you can find a BibTeX reference in the specific post."
  },
  {
    "objectID": "posts/2022-02-07-sucx3_ner/index.html",
    "href": "posts/2022-02-07-sucx3_ner/index.html",
    "title": "SUCX 3.0 - NER",
    "section": "",
    "text": "Figure¬†1: A simple NER example with three entities of two types.\n\n\n\nNamed Entity Recognition (NER), the task of automatically recognizing named entities, such as persons, companies, organizations, etc., is a staple Natural Language Processing (NLP) application. For Swedish, the Stockholm Ume√• Corpus (SUC) has been the biggest resource for training NER models, containing more than 30,000 sentences with manually annotated named entities and part-of-speech (POS) tags. Additionally, Spr√•kbanken has further enhanced the corpus with additional syntactic annotations and alternative annotations for NER, both created automatically. This enhanced version of the corpus is called SUCX 3.0. The new named entities largely match the manually annotated ones, use however slightly different categories and introduce two new categories for measurements and time. These new entity annotations are done by a rule-based NER system, creating regular and predictable annotations. A comparison of the entity categories is given below.\n\n\n\nManually Annotated\nAutomatically Annotated\n\n\n\n\nperson\nPRS\n\n\nplace\nLOC\n\n\ninst\nORG\n\n\nwork\nWRK\n\n\nproduct\nOBJ\n\n\nanimal\nPRS\n\n\nevent\nEVN\n\n\nmyth\nPRS\n\n\n-\nMSR\n\n\n-\nTME\n\n\nother\n-\n\n\n\nWith large transformer language models (LM) such as BERT becoming the de-facto standard for most NLP tasks, they have also shown their worth for somewhat simpler tasks such as NER.\n\n\nIn order to compare the performance of various models, it is common in NLP practice to create a canonical split of the training data into training-, development-, and test-data, that everyone uses to train and evaluate their models to provide a fair comparison. While this practice comes with a heap of problems, it is nonetheless an easy way to quickly compare models with the same setup to give an intuition about their performance. We therefore split the SUC 3.0 corpus into these three parts at random, while keeping the distribution of sentences with and without annotations, and the number of named entities per category the same across the three splits.\nThe original SUC 3.0 corpus uses XML to structure its various types of annotations. We use a much friendlier json-based format that only contains the sentence split into tokens, the POS annotations, and the NER annotations in the BIO format.\ne245ac6c-e24b4fe4\n[ \"I\", \"dag\", \"√§r\", \"han\", \"ingenj√∂r\", \"p√•\", \"vetenskapsakademins\",\n                    \"kemisk-tekniska\", \"institution\", \"i\", \"Vilnius\", \".\" ]\n[ \"PP\", \"NN\", \"VB\", \"PN\", \"NN\", \"PP\", \"NN\", \"JJ\", \"NN\", \"PP\", \"PM\", \"MAD\" ]\n[ \"B-TME\", \"I-TME\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\" ] \nThe data and its variations can be downloaded either with git or directly as a huggingface ü§ó dataset here https://huggingface.co/datasets/KBLab/sucx3_ner.\n\n\n\nThe original annotations of SUC are sometimes criticized to be somewhat inconsistent compared with other datasets (should titles be included in the named entity: kungen [Waldemar Atterdag] vs.¬†[kung Carl Gustaf]), to contain needlessly specific categories (animal, myth), and a dangerously confusing (for a machine) other category. In some instances one would therefore prefer the tags automatically annotated by the tagger, over the manual annotations.\nIn our first dataset variation we only take sentences with annotations, which do not contain the other category, and where the manual and automatic annotations match according to the mapping above. The new measurement (MSR) and time (TME) annotations are included as well. This means that this new dataset is somewhat smaller, as all sentences where the annotations did not match for each token are removed.\n\n\nDue to the custom in Swedish (and a lot of other European languages), to write named entities with a leading capital letter, NER systems quickly learn to rely on this simple feature. This improves performance as it is a clear indicator, when the data consists of properly formatted text, but leads systems to near absolute failure, when the data does not use case, as is often the case in web-text resources or chat.\nWe therefore also add a completely lower-cased (uncased) version of the dataset, and a cased-uncased-mixed version, that can be used to train and evaluate systems that are supposed to handle more noisy data. The new dataset variations uses the suffix ‚Äú_lower‚Äù at the sentence-id, to indicate that the sentence has been lower-cased. This allows users that wish to train and/or evaluate a model on a dataset where each sentence exists twice, cased and uncased, by simply combining the instances of each dataset into a new one."
  },
  {
    "objectID": "posts/2022-02-07-sucx3_ner/index.html#introduction",
    "href": "posts/2022-02-07-sucx3_ner/index.html#introduction",
    "title": "SUCX 3.0 - NER",
    "section": "",
    "text": "Figure¬†1: A simple NER example with three entities of two types.\n\n\n\nNamed Entity Recognition (NER), the task of automatically recognizing named entities, such as persons, companies, organizations, etc., is a staple Natural Language Processing (NLP) application. For Swedish, the Stockholm Ume√• Corpus (SUC) has been the biggest resource for training NER models, containing more than 30,000 sentences with manually annotated named entities and part-of-speech (POS) tags. Additionally, Spr√•kbanken has further enhanced the corpus with additional syntactic annotations and alternative annotations for NER, both created automatically. This enhanced version of the corpus is called SUCX 3.0. The new named entities largely match the manually annotated ones, use however slightly different categories and introduce two new categories for measurements and time. These new entity annotations are done by a rule-based NER system, creating regular and predictable annotations. A comparison of the entity categories is given below.\n\n\n\nManually Annotated\nAutomatically Annotated\n\n\n\n\nperson\nPRS\n\n\nplace\nLOC\n\n\ninst\nORG\n\n\nwork\nWRK\n\n\nproduct\nOBJ\n\n\nanimal\nPRS\n\n\nevent\nEVN\n\n\nmyth\nPRS\n\n\n-\nMSR\n\n\n-\nTME\n\n\nother\n-\n\n\n\nWith large transformer language models (LM) such as BERT becoming the de-facto standard for most NLP tasks, they have also shown their worth for somewhat simpler tasks such as NER.\n\n\nIn order to compare the performance of various models, it is common in NLP practice to create a canonical split of the training data into training-, development-, and test-data, that everyone uses to train and evaluate their models to provide a fair comparison. While this practice comes with a heap of problems, it is nonetheless an easy way to quickly compare models with the same setup to give an intuition about their performance. We therefore split the SUC 3.0 corpus into these three parts at random, while keeping the distribution of sentences with and without annotations, and the number of named entities per category the same across the three splits.\nThe original SUC 3.0 corpus uses XML to structure its various types of annotations. We use a much friendlier json-based format that only contains the sentence split into tokens, the POS annotations, and the NER annotations in the BIO format.\ne245ac6c-e24b4fe4\n[ \"I\", \"dag\", \"√§r\", \"han\", \"ingenj√∂r\", \"p√•\", \"vetenskapsakademins\",\n                    \"kemisk-tekniska\", \"institution\", \"i\", \"Vilnius\", \".\" ]\n[ \"PP\", \"NN\", \"VB\", \"PN\", \"NN\", \"PP\", \"NN\", \"JJ\", \"NN\", \"PP\", \"PM\", \"MAD\" ]\n[ \"B-TME\", \"I-TME\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\" ] \nThe data and its variations can be downloaded either with git or directly as a huggingface ü§ó dataset here https://huggingface.co/datasets/KBLab/sucx3_ner.\n\n\n\nThe original annotations of SUC are sometimes criticized to be somewhat inconsistent compared with other datasets (should titles be included in the named entity: kungen [Waldemar Atterdag] vs.¬†[kung Carl Gustaf]), to contain needlessly specific categories (animal, myth), and a dangerously confusing (for a machine) other category. In some instances one would therefore prefer the tags automatically annotated by the tagger, over the manual annotations.\nIn our first dataset variation we only take sentences with annotations, which do not contain the other category, and where the manual and automatic annotations match according to the mapping above. The new measurement (MSR) and time (TME) annotations are included as well. This means that this new dataset is somewhat smaller, as all sentences where the annotations did not match for each token are removed.\n\n\nDue to the custom in Swedish (and a lot of other European languages), to write named entities with a leading capital letter, NER systems quickly learn to rely on this simple feature. This improves performance as it is a clear indicator, when the data consists of properly formatted text, but leads systems to near absolute failure, when the data does not use case, as is often the case in web-text resources or chat.\nWe therefore also add a completely lower-cased (uncased) version of the dataset, and a cased-uncased-mixed version, that can be used to train and evaluate systems that are supposed to handle more noisy data. The new dataset variations uses the suffix ‚Äú_lower‚Äù at the sentence-id, to indicate that the sentence has been lower-cased. This allows users that wish to train and/or evaluate a model on a dataset where each sentence exists twice, cased and uncased, by simply combining the instances of each dataset into a new one."
  },
  {
    "objectID": "posts/2022-02-07-sucx3_ner/index.html#hyper-parameter-optimization",
    "href": "posts/2022-02-07-sucx3_ner/index.html#hyper-parameter-optimization",
    "title": "SUCX 3.0 - NER",
    "section": "Hyper-Parameter Optimization",
    "text": "Hyper-Parameter Optimization\nWhile we already achieve good performance with our KB-BERT model finetuned to do NER, we want to see if this can be improved by choosing a different set of hyper-parameters. With Hyper-Parameter Optimization (HPO) methods we can test combinations of different values for a chosen set of hyper-parameters that we believe can impact the performance of our final NER system.\n\nBaselines\nFirst we use the standard set of parameters of the huggingface ü§ó training API to create our baselines. We refer to the manually annotated tags as original (org) and the automatically annotated ones as simple. The measurement used for NER is F1-score, a measure that combines (for each tag) both the precision (to annotate only when it truly is of some type) and recall (to annotate all instances of some type).\nEach column illustrates one setting of NER tag & case type with batch size 64 and standard hyperparameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\norg/cased\norg/uncased\norg/mixed\nsimple/cased\nsimple/uncased\nsimple/mixed\n\n\n\n\nF1-Dev\n0.8901\n0.8683\n0.866\n0.9359\n0.912\n0.9111\n\n\nF1-Test\n0.8901\n0.867\n0.8687\n0.9346\n0.9017\n0.9118\n\n\n\n\nNote that these experiments use a version for the simple tags that does not use BIO-encoding in combination with the NE-tags. The dataset we publish uses BIO-tags, which can be removed if necessary.\n\nWhile there are many hyperparameters that one can vary, we only vary the learning rate, the weight decay, and the warmup ratio, three closely interconnected training parameters. The hyperparameter space that is shown below, was initially determined through inspiration from published related work and adapted after initial experiments.\nThe learning rate controls how much parameters are changed at every optimization step. Weight decay denotes a parameter that controls the impact of the regularizing L2-norm, favouring models with weights closer to zero. Finally, the warmup ratio controls the length of a warmup period during training in which the learning rate is increased to its initial maximum, aiming to avoid instability during early updates when the model weights are not yet aligned and the learning rate is too large.\nOriginal Hyperparameters:\nLearning Rate: 5e-5\nWeight Decay:  0.0\nWarmup Ratio:  0.0\nHyperparameter Search Space:\nLearning Rate: [2e-5, 3e-5, ..., 8e-5]\nWeight Decay:  [0.00, 0.05, 0.10, 0.15]\nWarmup Ratio:  [0.00, 0.04, 0.08, 0.12]\n\n\nMethods\nChoosing which hyperparameters to use when training machine learning models often requires a good understanding of the model, the dataset, but also the impact of each of the parameters as well, and how they are connected to each other. Additionally to that, a certain experience is needed as well, and the knowledge of the dark arts of optimization: specific parameter settings that generally work well out of the box, without necessarily being published in some way. For the HPO experiments we used the ray tune library, which is easily integrated within the huggingface ü§ó ecosystem.\nBut even then, there are simply too many possible settings that one can reasonably test manually while making informed decisions based on previous results. One way to solve this is by using grid-search, a method that simply checks every combination of hyperparameters. If the search space is too large, one can instead only search a random sub-space with random search. The following figure illustrates some ways of visualizing the results of hyperparameter optimization, which is useful to gain intuition of the hyperparameter behaviors.\n\n\n\n\n\n\n\nFigure¬†2: Weights and biases overview over the results from an HPO experiment with the simple lower mix variant of the dataset.\n\n\n\n\nWhile more advanced methods such as ASHA, BOHB, and PBT are applicable in our setting (and were tested to some degree), it is sufficient to employ random search when choosing a discrete set of values for each hyperparameter, instead of letting the algorithms explore the search space on their own.\n\nAdvanced HPO Methods\nThe more advanced HPO algorithms excel in different settings and can provide more efficient optimization. For example, ASHA and BOHB are both scalable and robust and can rapidly search through a large hyperparameter space by heavily utilizing early termination of non-promising runs. Furthermore, Bayesian Optimization (BO) models estimate the objective function and can draw informed samples of hyperparameter configurations, yielding successively better samples throughout the HPO session. While BO and early stopping sound promising, they possess the most value with a large number of runs and when early iterations of training consistently indicate end-of-run performance. In our case, we used a limited number of runs and searched for hyperparameters that clearly do not benefit from early stopping. For instance, a low learning rate with a large warmup ratio might perform poorly after training only for an epoch but may result in good performance after the full run.\nThe figure below demonstrates the early stopping behavior used in ASHA, terminating the lowest performing runs after roughly 2 epochs. Note that mechanisms as early stopping makes it difficult to visualize results and gain intuition about hyperparameters, as the final F1-scores become strongly biased towards runs that make it through the early phases of training.\n\n\n\n\n\n\n\nFigure¬†3: The validation F1 scores for an HPO session using ASHA, searching hyperparameters for the original lower case variation of the dataset.\n\n\n\n\nDeepMind‚Äôs PBT proved highly resource-demanding in our experiments and showed little to no performance gain. Increasing the population size and tweaking with other settings may yield a strong hyperparameter schedule if one has access to compute and wants to push the most out of a model/dataset. We did not investigate this further. Our experiments showed no significant advantage to using these advanced methods over random search.\nWe also conducted some modest experiments tuning the attention dropout rate, hidden dropout rate, and random seed as well, together with the more advanced HPO methods. Those results indicated that we can squeeze out a small amount of performance through more intricate HPO, but the procedure becomes expensive and prone to overfitting on the validation set without observing any performance gains on the test set.\n\n\n\nResults\nWe report our results using random search with 30 trials training on both the original and simple tags. For each tag-family we train the taggers on cased, uncased, and a mixed data set, while evaluating on all three development and test sets plus an additional set where only the named entity has been lower-cased. The columns labelled uncased-cased-both and ne-lower-cased-both denote data sets, in which every sentence appears both cased and uncased.\nMost notably in these results is the performance of a system trained on regular cased data, when evaluated on uncased or partially lowercased data. At the same time we see that the system trained on a mix of cased and uncased data performs only slightly worse than their pure counterparts on the pure evaluation sets, while clearly outperforming them on the mixed evaluation sets.\n\nDevelopment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTag Family\nTrained on\nHPO Alg\ncased\nuncased\nuncased-cased-mix\nuncased-cased-both\nne-lower\nne-lower-cased-mix\nne-lower-cased-both\n\n\n\n\nOriginal\ncased\nRS\n0.8951\n0.4067\n0.7054\n0.6987\n0.4110\n0.7045\n0.6985\n\n\nOriginal\nuncased\nRS\n0.7847\n0.8713\n0.8278\n0.8293\n0.8695\n0.8263\n0.8285\n\n\nOriginal\nuncased-cased-mix\nRS\n0.8821\n0.8573\n0.8702\n0.8698\n0.8504\n0.8671\n0.866\n\n\nSimple\ncased\nRS\n0.9345\n0.3037\n0.7035\n0.6974\n0.3038\n0.6995\n0.6941\n\n\nSimple\nuncased\nRS\n0.8361\n0.9157\n0.8753\n0.8774\n0.9154\n0.8754\n0.8773\n\n\nSimple\nuncased-cased-mix\nRS\n0.9275\n0.9078\n0.9185\n0.9177\n0.9029\n0.9155\n0.9153\n\n\n\n\n\n\nTest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTag Family\nTrained on\nHPO Alg\ncased\nuncased\nuncased-cased-mix\nuncased-cased-both\nne-lower\nne-lower-cased-mix\nne-lower-cased-both\n\n\n\n\nOriginal\ncased\nRS\n0.8978\n0.4053\n0.6940\n0.7000\n0.4103\n0.6924\n0.6998\n\n\nOriginal\nuncased\nRS\n0.7811\n0.8656\n0.8248\n0.8245\n0.8649\n0.8245\n0.8242\n\n\nOriginal\nuncased-cased-mix\nRS\n0.8833\n0.8523\n0.8661\n0.8680\n0.8489\n0.8650\n0.8663\n\n\nSimple\ncased\nRS\n0.9304\n0.2963\n0.6940\n0.6929\n0.2902\n0.6879\n0.6861\n\n\nSimple\nuncased\nRS\n0.8299\n0.9075\n0.8687\n0.8702\n0.9074\n0.8685\n0.8702\n\n\nSimple\nuncased-cased-mix\nRS\n0.9219\n0.8988\n0.9083\n0.9104\n0.8950\n0.9064\n0.9085\n\n\n\n\n\n\nBaselines Comparison\nIn order to see how much HPO actually helps for this task we compare with our unoptimized baseline systems. Each column illustrates one setting of tag & case type with the performance difference after HPO: F1(HPO) - F1(baseline)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg/cased\norg/uncased\norg/mixed\nsimple/cased\nsimple/uncased\nsimple/mixed\n\n\n\n\nF1-Dev\n+0.005\n+0.003\n+0.0042\n-0.0014\n+0.0037\n+0.0074\n\n\nF1-Test\n+0.0077\n-0.0014\n-0.0026\n-0.0042\n+0.0058\n-0.0035\n\n\n\n\nUnfortunately these differences are not what we hoped for, pointing to either a bad choice of hyperparameters and values to optimize or the stability of the model being trained regardless of the chosen hyperparameters (up to a certain degree of reasonable values). One such hyperparameter might be the warmup ratio, given that the model is already stable and is only being finetuned to a relatively simple task. We speculate that a more consistent gain in validation set performance could be achieved by taking the stochastic elements into account, i.e.¬†running each experiment with multiple seeds, which argues for the insignificance of the HPO results.\n\n\nSuccessful Hyperparameters\n\n\n\n\n\n\n\n\n\n\n\n\nTag Family\nTrained on\nHPO Alg\nlearning rate\nweight decay\nwarmup ratio\n\n\n\n\nOriginal\ncased\nRS\n7e-05\n0.15\n0.04\n\n\nOriginal\nuncased\nRS\n5e-05\n0.10\n0.08\n\n\nOriginal\nuncased-cased-mix\nRS\n8e-05\n0.15\n0.12\n\n\nSimple\ncased\nRS\n5e-05\n0.05\n0.04\n\n\nSimple\nuncased\nRS\n8e-05\n0.05\n0.04\n\n\nSimple\nuncased-cased-mix\nRS\n6e-05\n0.05\n0.12\n\n\n\n\n\n\n\nModels\nWe publish two models for our simple tags without BIO-encoding, trained on cased data and mixed cased-uncased data data for anyone to try, with more models to follow. If you feel that the model underperforms, feel free to continue training it on the validation and test data, or your own personal data. Let us know how the models perform in your projects and how you improved them."
  },
  {
    "objectID": "posts/2022-02-07-sucx3_ner/index.html#conclusion",
    "href": "posts/2022-02-07-sucx3_ner/index.html#conclusion",
    "title": "SUCX 3.0 - NER",
    "section": "Conclusion",
    "text": "Conclusion\nWe have taken the venerable SUC 3.0 dataset and given it a little refresher for people wanting to use its named entity annotations for training and evaluating NER taggers. We hope that the new format and its availability via the huggingface ü§ó ecosystem, together with a suggested train-development-test split will encourage more people to evaluate their models on this task, simply as a downstream finetuning task for large language models or for small specialist models trained to only do NER. With our little excursion to hyperparameter optimization we have learned to use the existing tools to easily find a better fitting set of hyperparameters, while also realizing that the results do not necessarily have to be better than when using the standard set of parameters.\n\nThe scripts to generate the SUCX 3.0 - NER data from the original data supplied by Spr√•kbanken, as well as the code for HPO and some additional information can be accessed at https://github.com/kb-labb/sucx3_ner."
  },
  {
    "objectID": "posts/2022-02-07-sucx3_ner/index.html#acknowledgements",
    "href": "posts/2022-02-07-sucx3_ner/index.html#acknowledgements",
    "title": "SUCX 3.0 - NER",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe gratefully acknowledge the HPC RIVR consortium (https://www.hpc-rivr.si) and EuroHPC JU (https://eurohpc-ju.europa.eu) for funding this research by providing computing resources of the HPC system Vega at the Institute of Information Science (https://www.izum.si)."
  },
  {
    "objectID": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html",
    "href": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html",
    "title": "A Swedish-Norwegian Federated Language Model",
    "section": "",
    "text": "Large transformer-based language models (LMs) have come to dominate the state-of-the-art for many natural language processing (NLP) tasks. These models, such as BERT and GPT, require both large amounts of compute as well as large amounts of textual data. Large tech companies that have been the driving force in the development of these large and steadily growing LMs, scrape the internet to gather huge text corpora for many different genres. These datasets however come with some problems. Languages that are less common on the internet will be underrepresented and the automatic classification of which language the text is actually in is not necessarily very accurate either. Due to the size of the data, manual checking is not feasible. Including any type of text scraped from the internet without checking its content, will also include texts with undesirable views of racist, sexist, or similar nature that can induce certain biases into the final model. The National Library of Sweden (Kungliga Biblioteket ‚Äì KB) has access to vast amounts of digitized newspapers and other texts in Swedish, that we used to train a state-of-the-art Swedish BERT language model. In contrast to text scraped from the internet, our dataset is much more controlled. For that reason it is a valuable asset for research on language modeling, but due to the copyright of the original owners of the individual texts we are not able to directly share the data with external parties in the research community.\n\n\n\n\n\nElectra at the Tomb of Agamemnon, Frederic Leighton c.¬†1869\n\n\nIn order to allow others to train new models with their own private and our private data, we are here exploring the use of federated machine learning (FL). FL is a recent strategy that allows the training of models without directly sharing or disclosing the data. Simply speaking, the data never leaves the administrative control of the data provider, instead local model updates are computed and combined to form a global, federated model.\nSuch a FL setup would allow multiple national libraries and other maintainers of private data, to collaborate in training multilingual LMs without having to sort out potential legal problems, as no data is shared. We collaborate with Scaleout and use their open-source FL framework FEDn to train a Swedish-Norwegian ELECTRA language model.\nYou can read more about this project at KBLab, Scaleout, and AI Sweden."
  },
  {
    "objectID": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#introduction",
    "href": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#introduction",
    "title": "A Swedish-Norwegian Federated Language Model",
    "section": "",
    "text": "Large transformer-based language models (LMs) have come to dominate the state-of-the-art for many natural language processing (NLP) tasks. These models, such as BERT and GPT, require both large amounts of compute as well as large amounts of textual data. Large tech companies that have been the driving force in the development of these large and steadily growing LMs, scrape the internet to gather huge text corpora for many different genres. These datasets however come with some problems. Languages that are less common on the internet will be underrepresented and the automatic classification of which language the text is actually in is not necessarily very accurate either. Due to the size of the data, manual checking is not feasible. Including any type of text scraped from the internet without checking its content, will also include texts with undesirable views of racist, sexist, or similar nature that can induce certain biases into the final model. The National Library of Sweden (Kungliga Biblioteket ‚Äì KB) has access to vast amounts of digitized newspapers and other texts in Swedish, that we used to train a state-of-the-art Swedish BERT language model. In contrast to text scraped from the internet, our dataset is much more controlled. For that reason it is a valuable asset for research on language modeling, but due to the copyright of the original owners of the individual texts we are not able to directly share the data with external parties in the research community.\n\n\n\n\n\nElectra at the Tomb of Agamemnon, Frederic Leighton c.¬†1869\n\n\nIn order to allow others to train new models with their own private and our private data, we are here exploring the use of federated machine learning (FL). FL is a recent strategy that allows the training of models without directly sharing or disclosing the data. Simply speaking, the data never leaves the administrative control of the data provider, instead local model updates are computed and combined to form a global, federated model.\nSuch a FL setup would allow multiple national libraries and other maintainers of private data, to collaborate in training multilingual LMs without having to sort out potential legal problems, as no data is shared. We collaborate with Scaleout and use their open-source FL framework FEDn to train a Swedish-Norwegian ELECTRA language model.\nYou can read more about this project at KBLab, Scaleout, and AI Sweden."
  },
  {
    "objectID": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#what-is-electra",
    "href": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#what-is-electra",
    "title": "A Swedish-Norwegian Federated Language Model",
    "section": "What is ELECTRA?",
    "text": "What is ELECTRA?\nELECTRA is a transformer-based masked language model (MLM) similar to its predecessor BERT. In contrast to classical LMs, now often referred to as causal language models (CLMs), that are trained by predicting the next token in a sequence, an MLM is trained by reconstructing the original sequence given a corrupted input sequence. In the original BERT model this is done by randomly masking out 15% of the input:\n\nInput: The [MASK] sat on the mat.\nOutput: The cat sat on the mat.\n\nBy learning to predict missing tokens, the model learns to imitate not only the structure of language in form of fitting syntax, but also which words and phrases have similar meaning by the contexts they have been used in the dataset.\nGiven that only 15% of the input tokens are masked and thus used for training the model, this approach is somewhat inefficient. While the network structure of ELECTRA is essentially the same as BERT‚Äôs, its training objective promises to be more sample-efficient. Instead of training to predict some masked out tokens, ELECTRA learns to predict for each token whether it belongs to the original input sequence or if it was generated by a secondary model. This secondary model, the generator, is trained in tandem with the primary model, the discriminator, quite similar to generative adversarial networks (GANs).\n\nInput: The dog sat on the mat.\nOutput: ‚úîÔ∏è ‚ùå ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è\n\nWith this new objective ELECTRA is able to outperform BERT, essentially applying the MLM objective to every input token."
  },
  {
    "objectID": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#what-is-federated-machine-learning",
    "href": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#what-is-federated-machine-learning",
    "title": "A Swedish-Norwegian Federated Language Model",
    "section": "What is Federated Machine Learning?",
    "text": "What is Federated Machine Learning?\nFederated learning is a technique used when a model needs to be trained on multiple datasets that cannot be pooled. There are two general use-case scenarios for FL: Cross-silo and cross-device.\nCross-device is a scenario where there are too many small devices, such as mobile or edge devices, that provide a constant stream of outputs. In contrast to this, cross-silo involves few, more powerful machines that handle datasets that cannot be shared due to privacy concerns or legal restrictions.\n\n\n\n\n\n\nFigure¬†1: Schema for the hierarchical federated learning architecture implemented in FEDn.\n\n\n\nThe FL framework FEDn is designed to support scalable FL using a tiered, hierarchical architecture. It is based on services taking four principal roles: i) controller, ii) reducer, iii) combiner, and iv) client. At the lowest level of this hierarchical structure, local models with local data are trained on multiple geographically distributed client nodes. These local models are then, after a certain number of training updates, sent to one or more combiners that coordinate the updates from their own subset of clients. These partial model updates are then reduced into a single global model and redistributed to clients for the next training round, according to a reducer protocol (currently all-reduce). Finally, the controller‚Äôs responsibility is to coordinate the overall computation and to maintain the immutable trail of global models.\nThe update scheme used to combine the local models into one global model is called federated averaging (FedAvg), one of the most widely used methods for FL. In each round the current version of the global model is distributed to the clients that continue training using each their own data. After one local round of training the distributed clients‚Äô model-weights are sent back to the server that simply averages the weights, while taking the number of local updates into account."
  },
  {
    "objectID": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#experimental-setup",
    "href": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#experimental-setup",
    "title": "A Swedish-Norwegian Federated Language Model",
    "section": "Experimental Setup",
    "text": "Experimental Setup\nWith the future goal to train a large Scandinavian transformer-based language model, we downscale the size of the model and data to be able to efficiently test different hyper-parameter settings. We choose to train a small ELECTRA model using publicly available data from the OSCAR corpus and Wikipedia, for Swedish, and Norwegian bokm√•l and nynorsk. The Swedish corpus is 27 GB, about five times larger than the Norwegian corpus. This uneven distribution allows us to additionally investigate whether an LM built on little data can benefit from a similar language‚Äôs data.\nDue to the rather small size of the ELECTRA model, we are able to train using standard workstation GPUs. Our federated setup consists of three workstations plugged into the same network, two of which serving as local clients, doing the majority of computational work training the local model instances on GPU, and one workstation taking care of collecting, averaging, and redistributing the models.\nTraining large-scale transformer-based language models heavily relies on the correct choice of hyper-parameters, for the model as well as the optimizer. We follow the settings of the original small ELECTRA models in English, and focus only on choosing the correct federated learning strategy.\n\nConvergence as a function of local update steps\nIn order to obtain good FL performance, we need to balance communication overhead and convergence. This entails doing as many local model updates (i.e.¬†gradient steps) as possible (more update steps means fewer global rounds), without letting the local models diverge from one another too far (large divergence before aggregation leads to lower convergence rate). For example, updating after 100 gradient steps will keep divergence to a minimum and require fewer gradient steps in total to converge, but will, due the communication overhead in global rounds, need much longer actual wall-time to reach a certain accuracy level, compared to models communicating their updates after every 1000 local gradient steps. On the other hand, taking too many local gradient steps will manage to do more gradient steps in a shorter amount of time, but need many more updates and thus time to reach convergence.\n\n\nThe Role of the optimizer\nWith FedAvg we typically only consider model parameters, but large transformer neural networks generally need more advanced optimization methods than simple stochastic gradient descent. In most cases the Adam (Adaptive Moment Estimation) optimizer is used, which computes adaptive learning rates for each parameter, storing both the mean and the variance of the gradients. These additional parameters depend on the model parameters, meaning that they should be averaged as well and redistributed to the clients. This however increases the size of the data package that has to be sent by a factor of three, which can be significant when larger models are trained that ‚Äúweigh‚Äù multiple gigabytes. We test how the development of the loss is affected by keeping the optimizer specific parameters local versus averaging them the same way as regular model parameters."
  },
  {
    "objectID": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#results",
    "href": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#results",
    "title": "A Swedish-Norwegian Federated Language Model",
    "section": "Results",
    "text": "Results\nTo evaluate the impact of changing various hyper-parameters, we focus on the development of the loss function during training. While it seems easy to evaluate large language models, as one can simply use the GLUE or SuperGLUE benchmarks to get an overall performance evaluation, there are many tricks one needs to apply to gain better scores. Even simply changing the random seed can increase or decrease performance by multiple points.\nWhile we do not evaluate downstream model performance, we clearly see how the training is affected.\n\nNumber of local updates\nIn our first set of experiments we investigate how various local round lengths affect the training progress. We try four different local round lengths, with 100, 1000, 2000, and 5000 gradient steps before recombining the models.\n\n\n\nWhile the loss decreases the most per steps taken when the model is updated as often as possible (i.e.¬†100 steps), it takes far longer than in the other setups to reach the same loss values. Increasing the local round length to 5000 gradient steps allows us to do the most gradient steps in the shortest amount of time, but results in the loss not decreasing as quickly as with for example 1000 steps per round.\n\n\n\nIn this scenario we finally settle for 1000 steps per round, giving us the best speed-performance trade-off. With real-world models being much larger than the one used in our experiments, it could be interesting to change the round length during training. Longer round length in the beginning allows the model to see more data, while shorter round lengths towards the end will help the model to converge.\n\n\nLocal vs.¬†global optimizer\nUsing a more advanced optimizer such as Adam is necessary when training models with parameters now regularly surpassing multiple billions. This unfortunately means that the number of parameters that we need to federate triples, which increases the communication overhead. In order to test whether it is enough to only federate the model parameters themselves while keeping the optimizer states local, we train our small ELECTRA model with the additional Adam parameters retaining their local states, and averaging them just as the regular model parameters.\n\n\n\nWe can see that averaging the optimization-specific parameters allows the loss to decrease further, without taking much more time. While keeping the optimization parameters local increases the speed a little bit (the green curve in the figure above is slightly longer), it is not enough to counteract the decrease in learning.\n\n\n\nThese results show that keeping outdated optimization parameters to increase the overall speed is not desirable. For larger models we might see a significant increase in speed, but it might then be a better idea to change the optimization algorithm to regular stochastic gradient descent, to avoid faulty inputs. Similarly to dynamically changing the round lengths, adding a smarter optimization algorithm towards the end can be a possibility."
  },
  {
    "objectID": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#continuation",
    "href": "posts/2021-06-15-a-swedish-norwegian-federated-language-model/index.html#continuation",
    "title": "A Swedish-Norwegian Federated Language Model",
    "section": "Continuation",
    "text": "Continuation\nThis project has given us some promising first results towards training large language models such as ELECTRA. Using a federated black-box approach as implemented in FEDn, gives us the possibility to train models with other non-public data holders, but also gives others the possibility to train their models with our data.\nThe models we trained are however only of one type and relatively small. We are working on implementing an interface to the ü§ó Transformers library, that will allow users to train LMs from scratch in a federated fashion, but also fine-tune these models using the same functionalities. We hope that training models larger than our small ELECTRA, will give us more insights into how long we should train locally and whether to change the optimization strategy.\nWith these pieces in place, we finally hope to train a large Scandinavian language model that combines data sources that so far could not have been combined."
  },
  {
    "objectID": "posts/2023-01-16-sentence-transformer-20/index.html",
    "href": "posts/2023-01-16-sentence-transformer-20/index.html",
    "title": "Swedish Sentence Transformer 2.0",
    "section": "",
    "text": "We release an updated Swedish sentence transformer model. In addition to training the model on parallel sentences, we concatenate sentences from those parallel text corpora whose sentence orderings are sequential, training our model on longer text paragraphs. The new KB-SBERT v2.0 has an increased maximum sequence length of 384, up from the 256 maximum tokens of the previous model. The model performs only marginally worse on SuperLim‚Äôs SweParaphrase benchmark, while performing significantly better on SweFaQ."
  },
  {
    "objectID": "posts/2023-01-16-sentence-transformer-20/index.html#the-sequence-length-problem",
    "href": "posts/2023-01-16-sentence-transformer-20/index.html#the-sequence-length-problem",
    "title": "Swedish Sentence Transformer 2.0",
    "section": "The sequence length problem",
    "text": "The sequence length problem\nThe training of the vast majority of available non-English sentence transformer models involves the use of so called parallel corpora. These are text translations of the same source documents in two or more languages. Typically these datasets come in the form of sentence-aligned observations. As a result, the context of any single given observation is quite small.\n\n\nYou can read more about the first version of the model in another article on our blog: Introducing a Swedish Sentence Transformer.\nThe setup for training the first version of KB-SBERT involved parallel corpora and using knowledge distillation to transfer the knowledge of an English sentence transformer to a Swedish student BERT model. The library sentence-transformers provides a template for training models in this manner. Its default setting however recommends a maximum sequence length of \\(128\\) for knowledge distilled bi- or multilingual models. This is due to most training examples in parallel corpora being rather short. It is unclear whether models trained on only short inputs can perform well on longer inputs.\n\n\nThe limited maximum sequence length of multilingual models is discussed in a Github issue.\nIn this article, we set out to train a new model where we investigate and try to address the sequence length problem."
  },
  {
    "objectID": "posts/2023-01-16-sentence-transformer-20/index.html#longer-parallel-texts",
    "href": "posts/2023-01-16-sentence-transformer-20/index.html#longer-parallel-texts",
    "title": "Swedish Sentence Transformer 2.0",
    "section": "Longer parallel texts",
    "text": "Longer parallel texts\nThe new model is trained on a similar set of source datasets as the old one, assembled mostly from the Open Parallel Corpus (OPUS). We use JW300, Europarl, DGT-TM, EMEA, ELITR-ECA, TED2020, Tatoeba and OpenSubtitles. We discovered issues in alignments in the published files from OPUS for the DGT dataset. For this reason we downloaded all the raw DGT-TM data files from EU‚Äôs data portal and processed them ourselves (code). Some of the datasets have quality filters indicating the confidence of the alignments. You can see which thresholds we used in the following highlighted lines.\n\n\nRead more about the datasets in the original article.\nThe main difference when it comes to training data comes from\n\nidentifying the datasets where sentences are ordered consecutively.\nconcatenating consecutive sentences in both English and Swedish to longer parallel texts.\n\nA dataset with longer texts was created using DGT-TM, Europarl, EMEA, ELITR-ECA, JW300 and TED2020. Word counts were calculated for all sentences. Consecutive sentences were then concatenated until the cumulative number of words exceeded a maximum word limit determined by sampling from a truncated normal distribution:\n\\[\n\\mathcal{TN}(\\mu, \\sigma, a, b)\n\\]\n\n\n\\(\\mu\\) ‚Äì mean.\n\\(\\sigma\\) ‚Äì standard deviation.\n\\(a\\) ‚Äì minimum nr of words.\n\\(b\\) ‚Äì maximum nr of words.\nwhere \\(\\mu=270\\), \\(\\sigma=110\\), \\(a=60\\), \\(b=330\\). Generally each word is represented by an average of \\(1.3\\) to \\(1.4\\) tokens. Some of the concatenated texts will thus exceed the \\(384\\) max sequence length of the model and be truncated. We imagine truncation will be common in real world usage as well, and the pretraining schemes of some language models such as BERT even included too long sequences for this very reason. Therefore we don‚Äôt regard the occasional truncation as an issue."
  },
  {
    "objectID": "posts/2023-01-16-sentence-transformer-20/index.html#new-models",
    "href": "posts/2023-01-16-sentence-transformer-20/index.html#new-models",
    "title": "Swedish Sentence Transformer 2.0",
    "section": "New models",
    "text": "New models\nWe train two new models with \\(384\\) max sequence length. The one called v1.1 is trained with the same teacher model as our original v1.0 model. The new default model called v2.0 is trained with a more recently released teacher model all-mpnet-base-v2 that is supposedly better. An overview of available models and their differences are listed below.\n\n\n\nModel version\nTeacher Model\nMax Sequence Length\n\n\n\n\nv1.0\nparaphrase-mpnet-base-v2\n256\n\n\nv1.1\nparaphrase-mpnet-base-v2\n384\n\n\nv2.0\nall-mpnet-base-v2\n384\n\n\n\n\nYou can still access and use older version of the model with Huggingface via\n\nfrom transformers import AutoModel\nAutoModel.from_pretrained('KBLab/sentence-bert-swedish-cased', revision=\"v1.0\")\n\nor with sentence-transformers by cloning the repository to your computer with\ngit clone --depth 1 --branch v1.0 https://huggingface.co/KBLab/sentence-bert-swedish-cased\nand then pointing to that local folder when loading the model in:\n\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"path_to_model_folder/sentence-bert-swedish-cased\")"
  },
  {
    "objectID": "posts/2023-01-16-sentence-transformer-20/index.html#training",
    "href": "posts/2023-01-16-sentence-transformer-20/index.html#training",
    "title": "Swedish Sentence Transformer 2.0",
    "section": "Training",
    "text": "Training\nThe published models v1.1, and v2.0 were first trained on datasets without concatenations for about 380k steps, since this was 4 times faster than training with longer texts mixed in. After 48 hours (380k steps), they were trained for another ~100k steps (48 hours) with longer paragraphs mixed in.\nInitially, training and convergence was very slow when using all-mpnet-base-v2 as a teacher model. Our student model had difficulties and took much longer to converge to the same evaluation results compared to when it was trained with paraphrase-mpnet-base-v2. After some troubleshooting involving ablations such as:\n\nTraining only with parallel sentences.\nTraining with only longer paragraphs.\nTraining with a mix.\nRepeating training with all the above configurations using paraphrase-mpnet-base-v2 as a comparison.\n\neventually we turned to carefully comparing both of the above teacher models for differences. We discovered that all-mpnet-base-v2 has an L2-normalization layer at the end after pooling the embeddings. This normalization layer doesn‚Äôt involve any model parameters, it simply rescales the output vector based on the magnitude of its own values. We suspected this layer was making it harder for our student model to learn to output similar embeddings to the teacher model, as the student model now had to learn how to normalize a vector (a process that doesn‚Äôt involve model paramters) in addition to emulating the teacher model.\nWe removed this normalization layer and replaced it with an Identity()-layer (returns the input without any manipulation). After this, the model converged much faster and the evaluation results improved."
  },
  {
    "objectID": "posts/2023-01-16-sentence-transformer-20/index.html#results-on-superlim",
    "href": "posts/2023-01-16-sentence-transformer-20/index.html#results-on-superlim",
    "title": "Swedish Sentence Transformer 2.0",
    "section": "Results on SuperLim",
    "text": "Results on SuperLim\nTo evaluate whether whether training with longer sequences improves model performance, we evaluated the model on two datasets using SuperLim, a set of evaluation datasets for Swedish language models. An updated version v2.0 of SuperLim is in the works, and will be released publicly once it is ready. We luckily had access to a development version of v2.0, and evaluated our models on both v1.0 and v2.0 of SuperLim.\n\nSweParahrase v1.0\nThe models were evaluated on SweParahrase and SweFAQ. Results from SweParaphrase v1.0 are displayed below.\n\n\n\nModel version\nPearson\nSpearman\n\n\n\n\nv1.0\n0.9183\n0.9114\n\n\nv1.1\n0.9183\n0.9114\n\n\nv2.0\n0.9283\n0.9130\n\n\n\nv2.0 of our model inches out a slight win when evaluated on SweParaphrase v1.0. However, it should be noted the test set is quite small in this version of SuperLim.\n\n\nSweParaphrase v2.0\nBelow, we present zero-shot evaluation results on all data splits. They display the model‚Äôs performance out of the box, without any fine-tuning.\n\n\n\nModel version\nData split\nPearson\nSpearman\n\n\n\n\nv1.0\ntrain\n0.8355\n0.8256\n\n\nv1.1\ntrain\n0.8383\n0.8302\n\n\nv2.0\ntrain\n0.8209\n0.8059\n\n\nv1.0\ndev\n0.8682\n0.8774\n\n\nv1.1\ndev\n0.8739\n0.8833\n\n\nv2.0\ndev\n0.8638\n0.8668\n\n\nv1.0\ntest\n0.8356\n0.8476\n\n\nv1.1\ntest\n0.8393\n0.8550\n\n\nv2.0\ntest\n0.8232\n0.8213\n\n\n\nIn general, v1.1, the model trained using the same teacher model as the original model, but using longer texts, correlates the most with human assessment of text similarity on SweParaphrase v2.0.\n\n\nSweFAQ v2.0\nWhen it comes to retrieval tasks, v2.0 performs the best by quite a substantial margin. It is better at matching the correct answer to a question compared to v1.1 and v1.0. Notably v1.1 performs better than v1.0, with the only difference between the two models being that longer parallel texts were included in the training of v1.1.\n\n\n\nModel version\nData split\nAccuracy\n\n\n\n\nv1.0\ntrain\n0.5262\n\n\nv1.1\ntrain\n0.6236\n\n\nv2.0\ntrain\n0.7106\n\n\nv1.0\ndev\n0.4636\n\n\nv1.1\ndev\n0.5818\n\n\nv2.0\ndev\n0.6727\n\n\nv1.0\ntest\n0.4495\n\n\nv1.1\ntest\n0.5229\n\n\nv2.0\ntest\n0.5871\n\n\n\n\nExamples how to evaluate the newer model on some of the test sets of SuperLim v2.0 can be found on the following links: evaluate_faq.py (Swedish FAQ), evaluate_sweparaphrase.py"
  },
  {
    "objectID": "posts/2023-01-16-sentence-transformer-20/index.html#acknowledgements",
    "href": "posts/2023-01-16-sentence-transformer-20/index.html#acknowledgements",
    "title": "Swedish Sentence Transformer 2.0",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe gratefully acknowledge the HPC RIVR consortium (www.hpc-rivr.si) and EuroHPC JU (eurohpc-ju.europa.eu) for funding this research by providing computing resources of the HPC system Vega at the Institute of Information Science (www.izum.si)."
  },
  {
    "objectID": "posts/2023-01-16-sentence-transformer-20/index.html#code-availability",
    "href": "posts/2023-01-16-sentence-transformer-20/index.html#code-availability",
    "title": "Swedish Sentence Transformer 2.0",
    "section": "Code availability",
    "text": "Code availability\n\nThe code used to train and evaluate KBLab‚Äôs Swedish Sentence BERT is available at https://github.com/kb-labb/swedish-sbert."
  },
  {
    "objectID": "posts/2023-02-12-zero-shot-text-classification/index.html",
    "href": "posts/2023-02-12-zero-shot-text-classification/index.html",
    "title": "Swedish zero-shot classification model",
    "section": "",
    "text": "The general goal of text classification is to assign a label to a piece of writing ‚Äì whether it is a book review that we want to categorize as positive or negative, or a news article that we wish to check the topic of. Use cases are diverse and can range from topic classification and spam detection to sentiment analysis.\nUsually, text classification is conducted with help of supervised machine learning, which means that we have to gather a sufficient number of labeled examples in order to train a classification model. The model learns relations between the sentences and the target labels and in turn can extrapolate from the examples it was exposed to during training to the unseen data. For instance, if we want to use a sentiment classification model, we first have to feed it with a train portion of the data, which can contain movie or book reviews marked as negative, positive or neutral. Only after training, we can show new texts to the model and ask it to predict the sentiment of the previously unseen reviews.\nThe procedure is straightforward, however, one issue is that it requires a labeled dataset for training. If we don‚Äôt have one, or cannot create one, generating the model is impossible. A solution suggested by Yin et al.¬†in ‚ÄúBenchmarking zero-shot text classification: Datasets, evaluation and entailment approach‚Äù (2019) has its roots in changing perspective and treating classification task as a textual entailment problem. This approach allows us to use already available Natural Language Inference (NLI) models as zero-shot text classifiers. In zero-shot learning, models are not exposed to any task-specific examples, yet they are expected to generalize from the tasks they were trained on to others ‚Äì in our case NLI models and various classification tasks."
  },
  {
    "objectID": "posts/2023-02-12-zero-shot-text-classification/index.html#what-is-zero-shot-text-classification",
    "href": "posts/2023-02-12-zero-shot-text-classification/index.html#what-is-zero-shot-text-classification",
    "title": "Swedish zero-shot classification model",
    "section": "",
    "text": "The general goal of text classification is to assign a label to a piece of writing ‚Äì whether it is a book review that we want to categorize as positive or negative, or a news article that we wish to check the topic of. Use cases are diverse and can range from topic classification and spam detection to sentiment analysis.\nUsually, text classification is conducted with help of supervised machine learning, which means that we have to gather a sufficient number of labeled examples in order to train a classification model. The model learns relations between the sentences and the target labels and in turn can extrapolate from the examples it was exposed to during training to the unseen data. For instance, if we want to use a sentiment classification model, we first have to feed it with a train portion of the data, which can contain movie or book reviews marked as negative, positive or neutral. Only after training, we can show new texts to the model and ask it to predict the sentiment of the previously unseen reviews.\nThe procedure is straightforward, however, one issue is that it requires a labeled dataset for training. If we don‚Äôt have one, or cannot create one, generating the model is impossible. A solution suggested by Yin et al.¬†in ‚ÄúBenchmarking zero-shot text classification: Datasets, evaluation and entailment approach‚Äù (2019) has its roots in changing perspective and treating classification task as a textual entailment problem. This approach allows us to use already available Natural Language Inference (NLI) models as zero-shot text classifiers. In zero-shot learning, models are not exposed to any task-specific examples, yet they are expected to generalize from the tasks they were trained on to others ‚Äì in our case NLI models and various classification tasks."
  },
  {
    "objectID": "posts/2023-02-12-zero-shot-text-classification/index.html#how-can-natural-language-inference-models-be-used-with-zero-shot-classification",
    "href": "posts/2023-02-12-zero-shot-text-classification/index.html#how-can-natural-language-inference-models-be-used-with-zero-shot-classification",
    "title": "Swedish zero-shot classification model",
    "section": "How can Natural Language Inference models be used with zero-shot classification?",
    "text": "How can Natural Language Inference models be used with zero-shot classification?\nNatural Language Inference task is designed to test models‚Äô reasoning capabilities. The aim of the task is to decide if there is a connection between two sentences, so-called premise and hypothesis, by checking whether a premise entails, contradicts or is not related to a hypothesis. For instance, the premise sentence ‚ÄúHow do you know? All this is their information again.‚Äù entails the hypothesis ‚ÄúThis information belongs to them.‚Äù, while the premise ‚Äúbut that takes too much planning‚Äù is contradictory to the hypothesis ‚ÄúIt doesn‚Äôt take much planning.‚Äù. The idea behind using NLI models as zero-shot text classifiers is to look at input text that we want to categorize and its label as an entailment pair. If the premise entails the hypothesis, we can assume that the label is suitable for the text, otherwise we reject it.\nIn order to be able to evaluate entailment pairs, we first have to reformulate potential classes into hypotheses with help of a template. Templates can be adjusted to specific tasks - for topic classification we could use the templates ‚ÄúThis text is about {}.‚Äù or ‚ÄúThis example is {}.‚Äù, while ‚ÄúThis text expresses {}.‚Äù might be appropriate for sentiment analysis.\nThe pairs can be constructed in the following manner:\nPremise: ‚ÄúN√§r Tutankhamons grav uppt√§cktes f√∂r 100 √•r sedan blev han v√§rldsk√§nd. Faraon dog som 19-√•ring f√∂r √∂ver 3000 √•r sedan men n√§r graven hittades var den helt intakt. Forskarna trodde f√∂rst att Tutankhamon blivit m√∂rdad, men egyptologen Zahi Hawass tror sig vara n√§ra att kn√§cka g√•tan om den unga faraons d√∂d.‚Äù\nCategories: ‚Äúsport‚Äù, ‚Äúreligion‚Äù, ‚Äún√∂je‚Äù, ‚Äúpolitik‚Äù, ‚Äúvetenskap‚Äù\nTemplate: ‚ÄúDetta exempel handlar om {}.‚Äù\nFirst hypothesis: ‚ÄúDetta exempel handlar om sport.‚Äù\nBy inserting the labels into the template and evaluating each of the pairs, we can check which topic is most suitable for the premise."
  },
  {
    "objectID": "posts/2023-02-12-zero-shot-text-classification/index.html#training-the-swedish-model",
    "href": "posts/2023-02-12-zero-shot-text-classification/index.html#training-the-swedish-model",
    "title": "Swedish zero-shot classification model",
    "section": "Training the Swedish model",
    "text": "Training the Swedish model\nTo create the Swedish model for zero-shot text classification, we have experimented with fine-tuning models on different combinations of NLI tasks. From the models manually tested on a set of entailment pairs, the best results obtained Swedish BERT-large model with 340M parameters fine-tuned on two NLI datasets, which are part of the Swedish version of the GLUE benchmark (https://gluebenchmark.com/) ‚Äì OverLim (https://huggingface.co/datasets/KBLab/overlim). The fine-tuning was conducted in two steps, first on The Stanford Question Answering Dataset (QNLI) and then on The Multi-Genre Natural Language Inference Corpus (MNLI). The model obtained 91.23% and 84.71% accuracy respectively."
  },
  {
    "objectID": "posts/2023-02-12-zero-shot-text-classification/index.html#examples",
    "href": "posts/2023-02-12-zero-shot-text-classification/index.html#examples",
    "title": "Swedish zero-shot classification model",
    "section": "Examples",
    "text": "Examples\nThe model is available on the KBLab‚Äôs HuggingFace page and can be used with ü§ó zero-shot classification pipeline. The pipeline offers a convenient API for using NLI models for classification. Let‚Äôs have a look at some examples!\n\nAfter loading the model, we have to specify a premise sentence and a list of categories that we wish to evaluate together with the input text. The output is a list over classes and corresponding probabilities calculated from probabilities for entailment and contradiction.\n\nDefining a hypothesis template is not obligatory, however, setting a custom one can positively influence performance. In the example below, we can observe that the probability for the label ‚Äún√∂je‚Äù (entertainment) has increased when the template was changed to a more specific one.\n\nLet‚Äôs consider another example. Once the punctuation is removed, the probability decreases.\n\nWith zero-shot approach training or fine-tuning models for text classification is not necessary. It can be therefore convenient if we are not in possession of labeled data, as the already trained NLI models can be used off-the-shelf. In addition, since the categories are not solely limited to ones determined at the point of training of a model, using a NLI-based zero-shot text classification pipeline gives more flexibility in terms of choice of candidate labels.\n\nLinks about zero-shot text classification:\n‚ÄúBenchmarking zero-shot text classification: Datasets, evaluation and entailment approach‚Äù, Yin et al.¬†article : https://aclanthology.org/D19-1404/\nNLI model for Swedish: https://huggingface.co/KBLab/megatron-bert-large-swedish-cased-165-zero-shot\nInformation about HuggingFace pipelines: https://huggingface.co/docs/transformers/v4.26.0/en/main_classes/pipelines"
  },
  {
    "objectID": "posts/2021-11-15-kblabs-director-love-brjeson-nominated-as-ai-swede-of-the-year-2021/index.html",
    "href": "posts/2021-11-15-kblabs-director-love-brjeson-nominated-as-ai-swede-of-the-year-2021/index.html",
    "title": "KBLab‚Äôs director Love B√∂rjeson nominated as AI Swede of the year 2021",
    "section": "",
    "text": "About TechSverige\nFrom their own website:\n‚ÄúTechSverige is a member organization for companies of all sizes within the tech sector, that wish to join the largest industry network in Sweden in order to promote and further develop the tech market and conditions for tech enterprises. We represent about 1 400 member companies that between them have nearly 100 000 employees.‚Äù\n\n\nAbout the prize\n√Örets AI Svensk 2021 (AI Swede of the year 2021) is a prize awarded by TechSverige. The prize aims to promote efforts and achievements that lead to an advancement in Swedish AI. That is done by way of bringing forward and honoring people who have contributed to the development and visibility of Swedish AI.\n\n\nMotivation\n‚ÄúLove B√∂rjeson leads KBLab at the National Library of Sweden with great patience and engagement. Among other things, the lab builds large AI language models for Swedish and has a huge impact in the application of AI in the Swedish public sector. KBLab is considered to be a unique resource in the development of Swedish AI and thanks to Love‚Äôs leadership and willpower it even has the potential to position Sweden on the international AI map.‚Äù\nRead more in the official announcement here (in Swedish).\n\n\n\n\nCitationBibTeX citation:@online{fano2021,\n  author = {Fano, Elena},\n  title = {KBLab‚Äôs Director {Love} {B√∂rjeson} Nominated as {AI} {Swede}\n    of the Year 2021},\n  date = {2021-11-15},\n  url = {https://kb-labb.github.io/posts/2021-11-15-kblabs-director-love-brjeson-nominated-as-ai-swede-of-the-year-2021/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFano, Elena. 2021. ‚ÄúKBLab‚Äôs Director Love B√∂rjeson Nominated as AI\nSwede of the Year 2021.‚Äù November 15, 2021. https://kb-labb.github.io/posts/2021-11-15-kblabs-director-love-brjeson-nominated-as-ai-swede-of-the-year-2021/."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "",
    "text": "Sentence transformers are a useful class of models that make it easier to implement efficient textual search applications. In this article we explain how KBLab‚Äôs Swedish Sentence-BERT was trained, providing some motivations on the methods used and the training process.\nThe published model can be found on Huggingface via the following link: https://huggingface.co/KBLab/sentence-bert-swedish-cased ."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#meaningful-sentence-embeddings",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#meaningful-sentence-embeddings",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "Meaningful sentence embeddings",
    "text": "Meaningful sentence embeddings\nPre-trained transformer language models trained at scale on large amounts of data have shown great success when fine-tuned on tasks such as text classification, named entity recognition and question answering. They can in fact also be successfully fine-tuned to compare the similarity between two sentences. However, when trained on semantic textual similarity (STS) tasks, these models typically require that two sentences be passed together as one input sequence to the network. This convention of passing sentence pairs as single input sequences is also present in the pre-training of said models, where one of the network‚Äôs pre-training tasks commonly includes ‚Äúnext sentence prediction‚Äù .\nWhile this convention produces strong results because the model can draw and combine information from both sentences in solving a task, it unfortunately also leads to practical issues when a particular dataset does not come nicely arranged in the form of sentence pairs, but rather instead as an unordered set of sentences. Finding the top \\(k\\) most similar sentences in a set of \\(N\\) sentences requires \\(\\sum^N_{i=1} i = \\frac{N \\cdot (N-1)}{2}\\) similarity computations (Reimers and Gurevych 2019). In the case of BERT, every single one of these computations come with an additional overhead ‚Äì since in order to obtain a similarity score one must first pass every sentence pair through the neural network. A BERT base model consists of \\(110\\) million parameters. These parameters are all involved in the transformation of the input to obtain a single similarity score.\n\n\n    A list of 2000 sentences requires almost 1 million similarity computations.\n\n\n\n\n\n\nFigure¬†1: Sentence similarity models were traditionally trained as cross-encoders (right figure). Information from Sentence A and Sentence B became \"cross-encoded\" , since both were passed as input together to the model. While effective at producing a similarity score, this setup would not yield meaningful sentence embeddings for the respective sentences A or B in isolation. This \"cross-contamination\" was addressed by Reimers and Gurevych (2019) using two BERT models in the training process, passing only one sentence each to the models (left figure), producing unique embeddings u and v for the respective sentences. 1\n\n\n\nReimers and Gurevych (2019) estimated it would take approximately 65 hours to perform the required 50 million similarity inference computations for a set of 10000 sentences with a V100 GPU. As an alternative they proposed Sentence-BERT, which they gave the moniker a ‚Äúbi-encoder‚Äù . In the training process each sentence was passed independently to the model. The resulting sentence embeddings u and v were then used for different down stream tasks such as classification. Trained in this manner, BERT models are successfully able to produce meaningful semantic sentence embeddings for single sentences.\nOnce the embeddings are obtained, we can perform the \\(\\frac{N \\cdot (N-1)}{2}\\) similarity computations using only the embeddings and consequently avoid the overhead of involving the neural network in each of the computations. In this scenario we only need \\(N\\) inference passes through the network to obtain sentence embeddings for every sentence. As a result our inference time can be reduced from 65 hours to mere seconds."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#swedish-training-data",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#swedish-training-data",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "Swedish training data?",
    "text": "Swedish training data?\nAn ever-present concern when training Swedish language models tends to be the lack of training data for fine-tuning. The situation is no different in the area of semantic textual similarity, where training data is plentiful in English, but sorely lacking in Swedish. Below is an example of data sources used to train many of the English sentence transformers models in the sentence-transformers package 2 . The evaluation dataset most often used is STSb: the Semantic Textual Similarity benchmark (Cer et al. 2017). A subset of the STSb‚Äôs test set was translated into Swedish and included as part of SuperLim (Adesam, Berdicevskis, and Morger 2020) under the name of SweParaphrase.\n\n\n\n\nName\n\n\nSource\n\n\n#Sentence-Pairs\n\n\nSTSb-dev\n\n\n\n\n\n\nAllNLI.tsv.gz\n\n\nSNLI + MultiNLI\n\n\n277,230\n\n\n86.54\n\n\n\n\nsentence-compression.tsv.gz\n\n\nsentence-compression\n\n\n180,000\n\n\n84.36\n\n\n\n\nSimpleWiki.tsv.gz\n\n\nSimpleWiki\n\n\n102,225\n\n\n84.26\n\n\n\n\naltlex.tsv.gz\n\n\naltlex\n\n\n112,696\n\n\n83.34\n\n\n\n\nmsmarco-triplets.tsv.gz\n\n\nMS MARCO Passages\n\n\n5,028,051\n\n\n83.12\n\n\n\n\nquora_duplicates.tsv.gz\n\n\nQuora\n\n\n103,663\n\n\n82.55\n\n\n\n\ncoco_captions-with-guid.tsv.gz\n\n\nCOCO\n\n\n828,395\n\n\n82.25\n\n\n\n\nflickr30k_captions-with-guid.tsv.gz\n\n\nFlickr 30k\n\n\n317,695\n\n\n82.04\n\n\n\n\nyahoo_answers_title_question.tsv.gz\n\n\nYahoo Answers Dataset\n\n\n659,896\n\n\n81.19\n\n\n\n\nS2ORC_citation_pairs.tsv.gz\n\n\nSemantic Scholar Open Research Corpus\n\n\n52,603,982\n\n\n81.02\n\n\n\n\nyahoo_answers_title_answer.tsv.gz\n\n\nYahoo Answers Dataset\n\n\n1,198,260\n\n\n80.25\n\n\n\n\nstackexchange_duplicate_questions.tsv.gz\n\n\nStackexchange\n\n\n169,438\n\n\n80.37\n\n\n\n\nyahoo_answers_question_answer.tsv.gz\n\n\nYahoo Answers Dataset\n\n\n681,164\n\n\n79.88\n\n\n\n\nwiki-atomic-edits.tsv.gz\n\n\nwiki-atomic-edits\n\n\n22,980,185\n\n\n79.58\n\n\n\n\nwiki-split.tsv.gz\n\n\nwiki-split\n\n\n929,944\n\n\n76.59\n\n\n\n\n\nMachine translation\nDifferent ways of getting around the issue of data have been explored. Isbister and Sahlgren (2020) investigated whether simply machine translating the English NLI and STS training datasets to Swedish could yield competitive results. They used KB-BERT (Malmsten, B√∂rjeson, and Haffenden 2020) trained in a cross-encoder setting and found it outperformed all other evaluated options (\\(82.5\\) Pearson correlation on a machine translated version of STS-b test set). Ultimately the authors still recommended against using their model ‚Äúdue to a high prevalence of translation errors‚Äù in the data with unknown effects on downstream applications.\n\n\nSelf-supervised\nA second avenue for getting around the lack of training data has been by training completely self-supervised/unsupervised. Carlsson et al. (2021) trained their models on data dumps of Wikipedia. Contrastive tension was used to maximize similarity between identical sentences, and minimize it for differing sentences without any need for labels. Their model however performed somewhat worse for Swedish than other languages (Arabic, English, Russian, Spanish). The Swedish model achieved \\(61.69\\) Pearson correlation on the machine translated version of the STS-b test set created by Isbister and Sahlgren (2020) .\nWhile the results are encouraging, at this point there still existed a gap that had yet to be closed when compared to the best performing English models."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#data-parallel-corpus-training-data",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#data-parallel-corpus-training-data",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "Data: Parallel corpus training data",
    "text": "Data: Parallel corpus training data\nOne approach to training a model is through distilling the knowledge of a ‚Äúteacher‚Äù model to a student model. In this scenario where we want our student model to emulate the embeddings of a teacher model trained in another language, we need to make use of parallel (translated) data.\nWe have data in the form of one sentence from the source language (English) and one sentence from the target language (Swedish). An example of the first 100 sentences of the English-Swedish Europarl dataset can be found here.\nThe OPUS (Open Parallel Corpus) project (Tiedemann 2012) has made available a great collection of parallel corpora from the web from a diverse set of sources. We use a number of different data sources to ensure our translated parallel sentences cover a wide range of language. Below is a table of the number of total sentences from each dataset used in the training of our model. Each English and Swedish sentence is counted uniquely. The reason some datasets have an odd number of sentences, is because in rare occasions two (or more) different candidate translations may be attached to the same sentence. We filter out all sentences above 600 characters in length. Furthermore, single sentences without a suggested candidate translation or source sentence are not included in training.\n\n\n\n\n\n\nThe question of dataset bias may naturally arise as a result of our selection. However, it should be noted that we ‚Äì in our training ‚Äì are merely recreating and matching the knowledge of the original model. The training process is not meant to impart ‚Äúnew knowledge‚Äù to our student model. Biases in our datasets are unlikely to be reflected in any substantial manner in the final model. Rather the final model will reflect the biases already present in the teacher model.\nHere is a short description of each dataset:\n\nOpenSubtitles 18: Sourced from opensubtitles.org, a large database of movie and TV subtitles (Lison and Tiedemann 2016). Can be filtered by a variable that measures time-overlap of the subtitles. v1 of the model did not filter observations, but a future v2 of the model will be trained on a subset of data with a higher overlap threshold.\nEuroparl: A parallel corpus consisting of proceedings of the European parliament (Tiedemann 2012).\nJW300: Various texts and articles from different Jehovas Witnesses websites and magazines. A filtering variable denoting certainty of alignment exists. Filtering will be applied for v2 of the model.\nEMEA: Texts from the European Medicines Agency, extracted from PDFs.\nEUbookshop: Based on documents from the EU bookshop. This datasets xml alignment file was found to be corrupt after training the model, causing quality issues in the aligned sentences. Future versions of the model will likely omit this dataset unless the corrupted xml files can be fixed.\nTED2020: Based off of TED and TED-X transcripts from July 2020 (Reimers and Gurevych 2020).\nTatoeba: Translated sentences from a free collaborative platform for language learners.\n\n\n\nData download scripts can be found on the following links: get_parallel_data_opus.py get_parallel_data_tatoeba.py get_parallel_data_ted2020.py\n\nDev set validation during training\nWe split off 1000 sentence pairs each from TED2020 and Tatoeba to form a validation set. The model was validated against these every 1000 training steps. The best model was continually autosaved during training based on the lowest combined MSE (sum) of the student model‚Äôs English and Swedish sentence embeddings against the teacher model‚Äôs English sentence embedding."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#method-translating-models-via-knowledge-distillation",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#method-translating-models-via-knowledge-distillation",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "Method: Translating models via knowledge distillation",
    "text": "Method: Translating models via knowledge distillation\nThe method used for KBLab‚Äôs Sentence-BERT is described in the paper ‚ÄúMaking Monolingual Sentence Embeddings Multilingual using Knowledge Distillation‚Äù (Reimers and Gurevych 2020). In short it allows us to take existing sentence embeddings models of the Bi-Encoder type shown in figure @ref(fig:cross) and extend them to new languages. The general setup in our case is as follows:\n\nA strong pre-trained teacher model of the Bi-Encoder type maps sentences in a source language (English) to dense vectors (embeddings). Our goal is to make a student model learn to match the teacher‚Äôs embeddings.\n\nA student model, which may be of the Cross-Encoder type takes sentence pairs consisting of one sentence from the source language (English) and one sentence from the target language (Swedish). Its objective is to minimize the mean squared error (MSE) between the teacher‚Äôs embedding against both the source and target language embeddings generated by the student model. See figure @ref(fig:multilingual).\nThe input sentences to the student model are from parallel corpora, meaning they are translations of each other.\n\n\n\n\n\n\n\nFigure¬†2: Student model takes a sentence from a source language and a sentence from a target language and minimizes the mean squared error of each to the teacher model‚Äôs embedding. Image source from Reimers and Gurevych (2020).\n\n\n\nUpdate 2022-04-22: In a previous version of this post the author discussed and alluded to the English-Swedish input sentence pairs being cross-encoded in the student model. However, when training a bi-encoder sentence transformer the sentences from different languages are processed independently by the student model network.\n\nTeacher model\nWe chose paraphrase-mpnet-base-v2 as our teacher model. At the time of training this was the strongest available bi-encoder. It was trained on Paraphrase Data.\n\n\nStudent model\nOur student model was the Swedish pretrained KB-BERT, using the same vocabulary."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#evaluations-on-superlim",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#evaluations-on-superlim",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "Evaluations on SuperLim",
    "text": "Evaluations on SuperLim\nSuperLim is a Swedish evaluation suite for natural language understanding models (Adesam, Berdicevskis, and Morger 2020). It was inspired by the English SuperGLUE (Wang et al. 2020).\nWe chose the four resources most relevant to the tasks our model was trained on:\n\nSweParaphrase: A subset of the English STS benchmark (Cer et al. 2017) dataset translated to Swedish. It consists of 165 sentence pairs. Human evaluators ranked the sentences according to how similar the two sentences were deemed to be (from 0 meaning no meaning overlap, to 5 meaning equivalence). Assembled and translated to Swedish by Isbister and Sahlgren (2020). Update (correction, 2021-09-07): The English STSb was machine translated to Swedish by Isbister and Sahlgren (2020). A subset of 165 sentence pairs from this automatically translated dataset were manually corrected by a native speaker of Swedish as part of the SuperLim project.\nSwedish FAQ: A collection of questions and answers from various Swedish authorities websites (F√∂rs√§kringskassan, Skatteverket, etc). The questions are divided into categories, for example F√∂r√§lder :: Barnbidrag :: Vanliga fr√•gor. The task is to match questions within a category to the correct answer (among a set of candidate answers that have been shuffled within the category).\nSweSAT synonyms: Multiple choice word synonym task of H√∂gskoleprovet (Swedish equivalent to SAT). Test taker is presented with a question word, and needs to match it to the correct option (synonym) from 5 possible choices.\nSuperSim: It is not clear whether Sentence embedding models produce meaningful word embeddings. This is a similarity and relatedness test set for word pairs, where each word has been rated on both relatedness and similarity by five different annotators. We only evaluate against the similarity scores.\n\n\nSweParaphrase results\nWe compare our results (KB-SBERT) to the previous highest published scores on SweParaphrase reported by Isbister and Sahlgren (2020). Results reported as Correlation coefficient * 100.\nUpdate (article correction, 2021-09-07): Isbister and Sahlgren (2020) evaluated on a machine translated version of the STSb test set. SweParaphrase is a subset of this machine translated version consisting of 165 sentence pairs. The sentence pairs were manually corrected by a native Swedish speaker. The results are therefore not directly comparable.\n\n\n\n\n\n\n\n\nCode to replicate results: evaluate_sweparaphrase.py\n\n\nSwedish FAQ results\nThere‚Äôs a varying number of questions per category in this dataset. Randomly guessing or na√Øvely guessing a single candidate answer within each group would give us an expected accuracy of \\(9.55\\%\\) (average questions per category is \\(10.47\\)).\nKB-SBERT total accuracy: \\(50.49\\%\\).\n\n\nReplicate results: evaluate_faq.py\nKB-SBERT manages to match half of the questions with the correct answer.\n\n\nSweSAT synonyms\nKB-SBERT hasn‚Äôt been explicitly trained to generate meaningful embeddings on the word level. However, we are curious to see how the model performs. Randomly guessing a single answer alternative would yield an expected accuracy of \\(20\\%\\).\nKB-SBERT accuracy: \\(42.82\\%\\).\n\n\nReplicate results: evaluate_swesat.py\nThe model performs better than random, though the result is still quite weak.\n\n\nSuperSim\nHere we predict semantic word similarity between word pairs as opposed to sentence pairs. We compare results with baselines published by Hengchen and Tahmasebi (2021) on the word similarity task.\n\n\n\n\n\n\n\n\nReplicate results: evaluate_supersim.py\nIt appears KB-SBERT is not to recommend for word embeddings."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#acknowledgements",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#acknowledgements",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe gratefully acknowledge the HPC RIVR consortium (www.hpc-rivr.si) and EuroHPC JU (eurohpc-ju.europa.eu) for funding this research by providing computing resources of the HPC system Vega at the Institute of Information Science (www.izum.si)."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#closing-words",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#closing-words",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "Closing words",
    "text": "Closing words\nKB-SBERT appears to perform well on sentence similarity tasks. Training on parallel corpora using a teacher model seemingly leads to better results compared to machine translation. However, it remains to be seen whether the distilled model behaves well when fine-tuned on downstream tasks in Swedish without any English supervision.\nIf you use KB-SBERT in your work, and perhaps fine-tune it for specific tasks, please drop us a message and tell us how it went. You can find the lab‚Äôs e-mail address in the footer of this webpage."
  },
  {
    "objectID": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#footnotes",
    "href": "posts/2021-08-23-a-swedish-sentence-transformer/index.html#footnotes",
    "title": "Introducing a Swedish Sentence Transformer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage source: https://www.sbert.net/docs/pretrained_cross-encoders.html‚Ü©Ô∏é\nParaphrase datasets table source: https://www.sbert.net/examples/training/paraphrases/README.html#‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-06-16-a-robust-multi-label-sentiment-classifier-for-swedish/index.html",
    "href": "posts/2023-06-16-a-robust-multi-label-sentiment-classifier-for-swedish/index.html",
    "title": "A robust, multi-label sentiment classifier for Swedish",
    "section": "",
    "text": "Many researchers in the humanities and adjacent fields are interested in the tonality of texts, for which sentiment analysis is an excellent tool. KBLab presents a robust, transformer based sentiment classifier in Swedish. The model is available as a multi-class model (negative/neutral/positive). It is publicly available via the Hugging Face Hub, published under the Apache 2.0 license.\nThe model was developed in collaboration with KBLab researcher Nora Hansson Bitt√°r, who is a PhD student at the Stockholm School of Economics. She is currently studying the development of sentiments and emotional load in the Swedish media landscape over time, inspired by Rozado et al (2022)1. Nora‚Äôs project will be further documented in the blog.\nA particular requirement when studying tonality in news media, is the need of a category representing a neutral tone, as many news articles are neither inherently positive or negative. This is often overlooked in sentiment modeling, as it adds complexity to the task, which in turn affects model performance.\nAnother aspect of other available sentiment models available in the Swedish language that makes them difficult to use in a project like Nora‚Äôs is their poor generalization capabilities. Most, if not all, previously published sentiment models in Swedish are trained on one type of text exclusively (reviews), which consequently leads to poor performance in other linguistic domains. We have trained our models on multiple datasets of various types and sizes.\nRobustness, in this case, refers to a language model‚Äôs generalization capabilities. Since most, if not all, previously published sentiment models in Swedish are trained on only one type of text (reviews), the performance in other linguistic domains suffer. We have trained our models on five different datasets from different sources, of various sizes and quality. Note that these datasets do not have a consistent, underlying annotation schema. This is compensated by the relatively large size of the corpus.\nThe accuracy of the model is evaluated on a balanced test set and is measured at 0.80 for the multiclass version and 0.88 for the binary version. More extensive evaluation will be conducted at a later stage and included in Nora‚Äôs report (albeit not on a balanced test set, but in the news media domain specifically). Both models are finetuned on the Swedish BERT-large model with 340M parameters, developed here at KBLab."
  },
  {
    "objectID": "posts/2023-06-16-a-robust-multi-label-sentiment-classifier-for-swedish/index.html#acknowledgements",
    "href": "posts/2023-06-16-a-robust-multi-label-sentiment-classifier-for-swedish/index.html#acknowledgements",
    "title": "A robust, multi-label sentiment classifier for Swedish",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPart of this development work was carried out within HUMINFRA infrastructure project.\n\n\n\n\n\nHUMINFRA\n\n\nPreview photo by Bo Wing√•rd (1967)."
  },
  {
    "objectID": "posts/2023-06-16-a-robust-multi-label-sentiment-classifier-for-swedish/index.html#footnotes",
    "href": "posts/2023-06-16-a-robust-multi-label-sentiment-classifier-for-swedish/index.html#footnotes",
    "title": "A robust, multi-label sentiment classifier for Swedish",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRozado D, Hughes R, Halberstadt J (2022) Longitudinal analysis of sentiment and emotion in news media headlines using automated labeling with Transformer language models. PLoS ONE 17(10): e0276367. https://doi.org/10.1371/journal.pone.0276367‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html",
    "href": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html",
    "title": "For how long is a person recognisable by their voice?",
    "section": "",
    "text": "Speaker verification is the computational task of indicating whether two audio recordings (each containing only one person‚Äôs speech) come from the same person or not. This comparison is done by converting the audio recordings to voiceprints, an abstract representation of someone‚Äôs voice. Recent approaches have used deep-learning models to create these voiceprints. TitaNet has been a particularly successful model, which was trained on the speaker identification task (who is speaking), and, once it achieved satisfactory results, used to create voiceprints.\nRiksdagen has released a public database with all of its speeches, who was speaking when, and what they said. However, this data is not always accurate. Thus, it can be interesting to use the identity of the speakers where the data is accurate to identify them where it‚Äôs less accurate. But people are members of parliament for years. The speaker verification and identification tasks are usually performed in a smaller time-frame, and so it is unclear how the results will extend to when there are larger age-gaps between two voiceprints. Besides searching a database, this is also important when using someone‚Äôs voice to unlock, for instance, a device. It is important to know what the range is, so the user can be warned in due time when it is time to record a new voiceprint."
  },
  {
    "objectID": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#objective",
    "href": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#objective",
    "title": "For how long is a person recognisable by their voice?",
    "section": "Objective",
    "text": "Objective\nThe main objective of this project was to investigate to what extent speakers remain recognisable by their voice as the age-gap of the speaker between the current and comparison recording increases. Further goals were to examine the effect of the age at which the comparison recording was made, the length of the recordings used for the comparisons, and the gender of the speakers."
  },
  {
    "objectID": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#data",
    "href": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#data",
    "title": "For how long is a person recognisable by their voice?",
    "section": "Data",
    "text": "Data\nThe data for this project consisted of Riksdagen‚Äôs parliamentary speeches. In the blog ‚ÄúFinding Speeches in the Riksdag‚Äôs debates‚Äù (Rekathati 2023a) you can read about how the speeches were segmented and their precise timestamps were determined, and in the blog ‚ÄúRixVox: A Swedish Speech Corpus with 5500 Hours of Speech from Parliamentary Debates‚Äù (Rekathati 2023b) you can read about the resulting dataset.\n\nExtracting speeches\nSometimes speeches contained speakers other than the main one at the beginning and at the end of the audio file. Because of this, the first and last 10 seconds of each speech were excluded. Then, for each speech, the audio was extracted at 7 different lengths, namely at 1, 3, 5, 10, 30 and 60 seconds, and also at the full speech length. To ensure that it was possible to extract 60 seconds from each speech, only those speeches with a length of at least 80 seconds (to account for excluding the first and last 10 seconds) were used. Each of the segments was extracted from a random point, and only once for each speech.\n\n\nFiltering the data\n\nThe data was also filtered for other characteristics. First, I excluded speeches containing speakers that did not have a birthyear or speaker ID associated with them. For some speeches, it seemed to be the case that a speaker mentioned themselves in a speech. However, manual inspection of the recordings revealed that the speeches had simply been assigned the wrong speaker ID, and that it was another speaker mentioning the speaker in question. These speeches were also removed Additionally, I excluded speeches that were doubled, and where the content and speech ID was swapped, but not the speaker ID. This latter case otherwise often resulted in an audio segment being attributed to the wrong person.\nFinally, the speeches were only included if they met the following criteria: First, their length_ratio and overlap_ratio was between 0.7 and 1.3. The length_ratio indicates how much longer the segment predicted by diarisation was compared to its length as predicted by ASR, and the overlap_ratio indicates how much overlap there is in terms of time between the segment predicted by diarisation and ASR. In addition to this, a speech was only included if it was associated with one segment as per the diarisation. Finally, for each speaker, I only included them and their speeches if they had at least 3 speeches in a year (that could potentially all be from the same debate).\n\n\nConverting to voiceprints\nThe next step was to convert the extracted audio to voiceprints. For this I used TitaNet-large (Koluguri, Park, and Ginsburg 2022). TitaNet was trained on the speaker identification task, which meant its final layer was the size of the number of speakers it was trained to recognise. The way this model was trained, meant that the representations in the previous layer maximise the cosine similarity when they belong to different speakers, and minise it when they belong to the same speaker. It is this 192-dimensional 1D vector that is extracted as the voiceprint of a speaker. Some of the speeches overloaded the GPU, so these were excluded.\n\n\nDivision of data\nThe data was further divided into train, dev, and test. The test data is created first. I tested how voiceprints age for a total age-gap of 9 years, and so only kept those speakers that were active in each of those years from the start of their presence in parliament. After this, I bucketed the speakers into age-ranges of 5 years. Each bucket contained a maximum of 4 speakers (balanced for gender, unless not possible). After this, I paired up the voiceprints in 4 different manners. They can be first distinguished by whether they compare the same or a different speaker, and then by whether they compare them at the same age or different age. The table below shows what the age-gaps are for each of the combinations.\n\nAge-gaps for the 4 data groups\n\n\npairs \\ age\nsame age\ndifferent age\n\n\n\n\nsame speaker\nsame age\n0-9 years difference\n\n\ndifferent speaker\nmax 5 years difference\nrandom age-gap\n\n\n\nThe train and dev data were created by first excluding the speakers already included in the test. Then, they were also bucketed in age-ranges of 5 years, but I put no requirements on for how long they had to be active in parliament. The data between train and dev had an approximate 80:20 ratio. However, where a bucket in the dev data contained fewer than 4 speakers, those speakers were instead added to the train, and the dev bucket remained empty. Then, the same 4 pairings as for the test were made for the train and dev data. See the below table for a full description of the data for train, dev, and test.\n\nData distribution and characteristics per split\n\n\n\n\n\n\n\n\nMeasure\nTrain\nDev\nTest\n\n\n\n\nFirst debate\n2003-11-11\n2004-01-23\n2006-01-25\n\n\nLast debate\n2023-02-03\n2023-01-31\n2021-12-08\n\n\nNumber of debates\n3156\n743\n1191\n\n\nNumber of speeches\n7422\n1363\n2310\n\n\nNumber of speakers\n177\n36\n20\n\n\nYoungest age\n19\n24\n24\n\n\nOldest age\n78\n68\n58\n\n\nDebates per year, mean (std.)\n353 (222)\n68 (32)\n144 (75)\n\n\nLowest number of debates (year)\n6 (2003)\n7 (2023)\n15 (2021)\n\n\nHighest number of debates (year)\n694 (2016)\n123 (2015)\n238 (2016)\n\n\nNumber of speeches per speaker, mean (std.)\n42 (44)\n38 (32)\n116 (36)\n\n\n\nFinally, I grouped the data along the length of the audio used to create the voiceprints. I created 8 groups in total. 7 of the groups paired up voiceprints extracted from pairs with the same source audio length. In other words, the group of length 1 only contained pairs where both source audios were 1 second long. The final group contained pairs comparing all source audio length combinations. This meant it compared pairs of voiceprints coming from 1 and 3 second long audio, 30 and 10, two full speeches, and so forth. The first 7 groups are referred to by their length, while this last group is referred to as ‚Äúall‚Äù."
  },
  {
    "objectID": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#method",
    "href": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#method",
    "title": "For how long is a person recognisable by their voice?",
    "section": "Method",
    "text": "Method\n\nVoiceprint separability\nThe first thing I examined, was whether the voiceprints were separable at all. That is, can they be grouped by different speakers? For this, I used T-SNE to create a graph of all the voiceprints used. If speeches cluster together by speaker, this indicates that there are likely many similarities between them. One drawback is that if some speeches end up behind another group, we will not be able to see this.\n\n\nSetting a threshold\nTo perform speaker verification, we need some way to compare two voiceprints to each other. For voiceprints from TitaNet, this is done by computing the cosine similarity between two voiceprints. A score closer to 1 indicates that the voiceprints are very similar, and likely belong to the same person, while a score closer to -1 (or 0 depending on how it‚Äôs calculated) indicates they are very dissimilar, and likely belong to different people. However, only having this score is not enough for us to know whether two voiceprints belong to the same speaker or not. To this end, we can set a threshold: For every score above or equal to this threshold we say that the two voiceprints come from the same speaker, and otherwise they come from two different speakers. This threshold however, needs to be determined. This is where the training and dev data comes in. If we set the threshold on the same data we are testing on, we cannot be sure that the results we get are due to this threshold and the resulting aging of the voiceprints is generalisable. Because of this, I set the threshold on the training data. I use the dev data to verify that, no matter where I set the threshold, the accuracy scores between the training and dev scores are going to be similar.\nTo set a threshold, we need to know what the scores for the same speaker group and different speaker group look like, to be able to distinguish them. For the same-speaker group, I used the same-age division of data. This is for two reasons. First: in real-world applications, speaker recordings are likely to be made in a short period of time, thereby not varying greatly in age. Second, I am testing what the effect is of testing voiceprints against different ages, meaning we cannot already include the effect of the aging voice when setting the threshold. For the different-speaker group, I used the different-age division, as we do not care to distinguish between different speakers of the same age, but about being able to tell the difference between different speakers at all. The threshold is set using sklearn‚Äôs roc_curve, and scipy‚Äôs interp1d and brentq.\n\n\nInvestigating the effects of age, segment length, and gender\nThresholds are set and tested using the ‚Äúall‚Äù speech lengths group, unless stated otherwise.\n\nAge\nI investigated the effects of age in two ways. First, I tested the effect of an increase in the age-gap on the accuracy, False Positive Rate (FPR), and False Negative Rate (FNR). In addition to this, I also investigated the effect of when the first voiceprint was recorded. In other words, if a voiceprint is recorded for someone at 29 years of age and then used for the next 9 years, how does this compare to when it is, for instance, recorded at 39 years of age and used for the next 9? Do we have a more stable voice in certain spans of life than others?\n\n\nSegment length\nTo test the effect of segment length, I split the ‚Äúall‚Äù speech lengths group among the comparisons, to investigate the effect of each individual speech length on the accuracy, FPR, and FNR. In addition to this, I also set the threshold for each of the 8 speech length groups, to investigate how this affects the height of the threshold set, and in turn how that affects the accuracy.\n\n\nGender\nI investigated the effect of gender on how the voiceprint ages. In other words, do voices age differently for men and women?"
  },
  {
    "objectID": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#results-and-discussion",
    "href": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#results-and-discussion",
    "title": "For how long is a person recognisable by their voice?",
    "section": "Results and discussion",
    "text": "Results and discussion\n\n\n\n\n\n\nFigure¬†1: T-SNE plot of test speakers at all lengths\n\n\n\nFigure¬†1 shows the T-SNE graph for all the test speakers. As we can see, the speakers generally group together among themselves, indicating that, despite age differences and varying lengths of the speech data used to create the voiceprints, they are still globally recognisable as belonging to the same person. Nevertheless, we see that there are also a few speeches that end up in vastly different places, suggesting that their representation was not as robust as the rest of the group.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Accuracy, FPR, and FNR at different thresholds\n\n\n\n\n\n\n\n\n\n\n\n(b) Comparison of train and dev accuracy\n\n\n\n\n\n\n\nFigure¬†2: Accuracy, FPR, and FNR at different thresholds.\n\n\n\n\nFigure¬†2 shows the training accuracy, FNR, and FPR (left plot), and compares the training and dev accuracy (right plot). The vertical dashed line indicates the threshold. The threshold is set at 0.463, and we can see that there is no large difference in accuracy between the train and dev. This gives us confidence that the threshold we set is generalisable.\n\n\n\n\n\n\nFigure¬†3: Cosine similarity scores for 3 groups: same-speaker same-age, same-speaker different-age, and different-speaker different-age.\n\n\n\nFigure¬†3 shows the cosine similarity scores for the two same-speaker groups and the different-speaker different-age group. As we can see, the cosine similarity scores are generally higher when comparing the voiceprints of one speaker against each other. What we also see, however, is that when we introduce an age-gap between the voiceprints, that the cosine similarity scores drop slightly. This suggests that the voice aging does become slightly more dissimilar compared to the first recording.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Percentiles (at intervals of 10%) of cosine similarities for each age-gap, and comparing two different speakers.\n\n\n\n\n\n\n\n\n\n\n\n(b) Test accuracy, FNR, and FPR for each age-gap.\n\n\n\n\n\n\n\nFigure¬†4: Effect of age-gap on cosine similarity, accuracy, FNR, and FPR scores.\n\n\n\n\nFigure¬†4 shows how the cosine similarity scores develop as the age-gap between two voiceprints increases (Figure¬†4 (a)), and also when comparing the voiceprints of two different speakers. In general, the cosine similarity seems to drop as the age-gap increases, but the cosine similarity is much lower when comparing two different speakers. The Figure¬†4 (b) in the same figure shows how, as the age-gap between two voiceprints increases, the accuracy drops slightly, and experiences an even sharper drop around the 5-year age-gap. This is characterised by an increase in FNR as the age-gap increases.\nIn general, it seems to be the case that increasing the age-gap between two voiceprints does affect our ability to recognise that they come from the same speaker, and so some caution needs to be exercised when using someone‚Äôs voiceprint to recognise them at a very different point in time.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Effect of starting age on same-age accuracy.\n\n\n\n\n\n\n\n\n\n\n\n(b) Effect of starting age on accuracy for a 9 year age-span.\n\n\n\n\n\n\n\nFigure¬†5: Effect of starting age on same-age and different-age scores.\n\n\n\n\nFigure¬†5 shows that speakers in the 29-33 age-range are the easiest to recognise, and older and younger speakers are harder to recognise (Figure¬†5 (a)). However, the difference is not large. The Figure¬†5 (b) uses different-age group for the same-speaker group. That is, the speaker verification is tested for this entire 9-year age-range for each speaker. We see that accuracy is highest when someone‚Äôs voiceprint is recorded in the 29-33 age-range and used for the next 9 years, and that it is lower for other age groups. Presumably this could be the case due to younger speakers‚Äô voices still changing too much, and older speakers‚Äô having begun to change again.\n\n\n\n\n\n\nFigure¬†6: Heatmap of FNR and FPR scores for all speech length comparisons. Left plot: Heatmap of FNR for all speech length comparisons. Right plot: Heatmap of FPR for all speech length comparisons.\n\n\n\nFigure¬†6 shows that accuracy is very low when at least one of the two voiceprints in a pair comes from a 1-second long audio. It also shows that accuracy is slightly diminished when the pairs use longer audio.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Heatmap of FNR for all speech length comparisons.\n\n\n\n\n\n\n\n\n\n\n\n(b) Heatmap of FPR for all speech length comparisons.\n\n\n\n\n\n\n\nFigure¬†7: Heatmap of FNR and FPR scores for all speech length comparisons.\n\n\n\n\nFigure¬†7 sheds some light on the accuracy scores. The very low performance of the short speeches seems to be attributable to a high FNR (Figure¬†7 (a)), meaning that we were much more likely to miss when two voiceprints belonged to the same person. The lower performance for longer audio lengths is attributable to a reduction in FPR (Figure¬†7 (b)): we were more likely to accidentally mark two unrelated speakers as being the same person.\nGiven all this, it might seem best to use speeches at around 3 seconds long: after all, these gave the best results, right? But that does not paint the whole story.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The effect of speech length on threshold and the subsequent accuracy.\n\n\n\n\n\n\n\n\n\n\n\n(b) The effect of speech length and the age-gap on cosine similarity. Ranges represent 95% confidence intervals.\n\n\n\n\n\n\n\nFigure¬†8: The effect of speech length on the threshold and cosine similarity.\n\n\n\n\nFigure¬†8 shows how, as we increase the audio length from which the voiceprints were extracted, the threshold is set higher as well (Figure¬†8 (a)). However, the corresponding accuracy is quite high, especially for voiceprint pairs extracted from speeches ranging from 5 seconds long to full-length speeches. The biggest dip in performance can be seen for voiceprint pairs extracted from 1-second long audio, and for ‚Äúall‚Äù speech-length comparisons. When looking at Figure¬†8 (b), we see that the cosine similarity increases for both the same-speaker and different-speaker voiceprint pairs, although moreso for the former group. It seems that voiceprint quality increases as the audio length from which they were extracted increases, resulting in the need to set a higher threshold to distinguish between the two groups. This also explains the reduced performance for the shortest audio lengths and for the mixed audio-length comparisons. For the short audio, the cosine similarities between the same- and different-speaker groups are considerably closer to each other, resulting in a larger overlap, and thereby worse performance. For the ‚Äúall‚Äù group, the low threshold is still too high to correctly classify the voiceprints coming from 1-second long audio, but too low to correctly distinguish voiceprint pairs coming from longer audio. The overall impression these two graphs seem to give is that it is not necessarily that one needs longer audio for more accurate speaker verification, but rather that the threshold should be set and tested on voiceprints coming from audio of the same length.\n\n\n\n\n\n\nFigure¬†9: The effect of gender on speaker verification.\n\n\n\nFigure¬†9 shows that as the age-gap between two voiceprints for the same speaker increases, the speaker verification drops at different rates for men and women after roughly 5 years, suggesting that it decreases more strongly for men."
  },
  {
    "objectID": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#conclusion-and-future-research",
    "href": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#conclusion-and-future-research",
    "title": "For how long is a person recognisable by their voice?",
    "section": "Conclusion and future research",
    "text": "Conclusion and future research\nAll in all, the results from this research suggests that voiceprints do age, and experience a sharper drop after the age-gap between two voiceprints reaches about 5 years. If a voiceprint is recorded between 29-33 years of age and used for the next 9, speaker verification retains a higher accuracy than if it is recorded at a different age. Using longer audio results in higher quality voiceprints, but it is also important to simply use and set thresholds on audio coming from the same length. However, mixing audio-length combinations still yields strong accuracy scores. Using very short audio yielded poor results, possibly due to the audio being extracted at random points from the speeches. Finally, male voiceprints might age faster than female voiceprints.\nOne thing that should be highlighted in this investigation, is that each age group was very small. Future research should endeavour to use larger groups to solidify the results obtained. Additionally, future research could increase the age-gap investigated, to see whether the general trend continues beyond this range. It would also be good to investigate both younger and older speakers. Additionally, we saw that voiceprints remain stable at different rates when recorded at different starting ages. It would be interesting to investigate what this range looks like for these age groups. Finally, given that speaker verification is quite accurate for shorter age-ranges, it would be interesting to combine the voiceprint from multiple ages to see whether that extends the number of years for which speaker verification remains accurate."
  },
  {
    "objectID": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#code",
    "href": "posts/2023-07-04-for-how-long-is-a-person-recognisable-by-their-voice/index.html#code",
    "title": "For how long is a person recognisable by their voice?",
    "section": "Code",
    "text": "Code\n\nThe code for this project can be found at https://github.com/MKNachesa/masters_thesis."
  },
  {
    "objectID": "posts/2023-05-24-swedish-text-to-speech/index.html",
    "href": "posts/2023-05-24-swedish-text-to-speech/index.html",
    "title": "Swedish speech synthesis",
    "section": "",
    "text": "Creating realistic sounding speech with proper intonation, pitch and tone from text has long been a goal of speech synthesis systems. These systems have a wide range of applications, among which a few are:\nRecent developments in neural speech synthesis have allowed for the synthetization of voices with increasing natural fidelity. However, many of these high quality systems with support for smaller languages remain proprietary.\nAt KBLab we recently discovered a relatively user friendly option to train a neural speech synthesis model through the Piper library1. Luckily, the Norwegian Language Bank (Spr√•kbanken) maintains several speech datasets originally produced by the company NST (Nordisk Spr√•kteknologi). One of those datasets consists of 5300 recordings of a single Swedish speaker 2, recorded for the purposes of training speech synthesis systems."
  },
  {
    "objectID": "posts/2023-05-24-swedish-text-to-speech/index.html#have-a-listen",
    "href": "posts/2023-05-24-swedish-text-to-speech/index.html#have-a-listen",
    "title": "Swedish speech synthesis",
    "section": "Have a listen",
    "text": "Have a listen\nHave a listen to the output of KBLab‚Äôs model below. We use text from the wikipedia articles on Regnb√•ge and Europaparlementet as the source text. For each sample, we also compare our model to a recently released open source text-to-speech model from Meta AI. See the Massively Multilingual Speech project (MMS) 3 for further details about Meta‚Äôs model.\n\n  \n    \n      KBLab TTS (Piper)\n      \n         \n      \n    \n  \n  \n  \n    \n      Facebook/Meta TTS\n      \n        \n      \n    \n  \n\n¬†\n\nEn regnb√•ge √§r ett optiskt, meteorologiskt fenomen som upptr√§der som ett (n√§stintill) fullst√§ndigt ljusspektrum i form av en b√•ge p√• himlen d√• solen lyser p√• nedfallande regn. Regnb√•gen best√•r f√§rgm√§ssigt av en kontinuerlig √∂verg√•ng fr√•n r√∂tt (ytterst) via gula, gr√∂na och bl√• nyanser till violett innerst; ofta definieras antalet f√§rger som sju, inklusive orange och indigo.\n\n\n  \n    \n      KBLab TTS (Piper)\n    \n      \n    \n    \n  \n  \n  \n    \n      Facebook/Meta TTS\n      \n        \n      \n    \n  \n\n¬†\n\nEuropaparlamentet (EP), √§ven k√§nt som EU-parlamentet, √§r den ena lagstiftande institutionen inom Europeiska unionen; den andra √§r Europeiska unionens r√•d. Parlamentet, som best√•r av 705 ledam√∂ter, v√§ljs genom allm√§nna och direkta val vart femte √•r, och f√∂retr√§der unionsmedborgarna direkt p√• unionsniv√•. Parlamentet kan f√∂renklat liknas vid ett underhus i ett tv√•kammarsystem.\n\n\nOdd pronounciations\nPiper and the MMS model from Meta both use VITS to train the text-to-speech model. This model relies on espeak-ng to translate text to phonemes. The extent and comprehensiveness of espeak-ng‚Äôs pronounciation and prosody rules vary from language to language, as the software is largely reliant on volunteer contributions. The reason the words meteorologiskt, fenomen and kontinuerlig have such a strange pronounciations is because espeak-ng generates an incorrect text to phoneme conversion 4."
  },
  {
    "objectID": "posts/2023-05-24-swedish-text-to-speech/index.html#how-do-i-use-the-model",
    "href": "posts/2023-05-24-swedish-text-to-speech/index.html#how-do-i-use-the-model",
    "title": "Swedish speech synthesis",
    "section": "How do I use the model?",
    "text": "How do I use the model?\nTo try out KBLab‚Äôs TTS model yourself using Piper:\n\nDownload the Piper binary from Github (executable file that allows you to run the model in your terminal) 5. For Linux:\n\nwget https://github.com/rhasspy/piper/releases/download/v0.0.2/piper_amd64.tar.gz\n\nUnzip/Untar the downloaded archive in a directory of your choice.\n\n# The contents will be untarred to directory named piper/\ntar -xvf piper_amd64.tar.gz\n\nDownload the Svenska (Swedish) model weights from Piper‚Äôs voice samples.\n\nwget https://github.com/rhasspy/piper/releases/download/v0.0.2/voice-sv-se-nst-medium.tar.gz\n\nUnzip/Untar the downloaded archive file in a directory of your choice. We suggest doing it in the same directory where you untarred the piper binary: piper/.\n\ntar -xvf voice-sv-se-nst-medium.tar.gz --directory=\"piper\"\n\nGenerate speech via the terminal.\n\n# Let's first move in to the piper directory\ncd piper\n\n# Generate speech to the audio file min_talsyntes.wav\necho 'Jag genererar tal med hj√§lp av talsyntes.' | ./piper \\\n  --model sv-se-nst-medium.onnx \\\n  --output_file min_talsyntes.wav"
  },
  {
    "objectID": "posts/2023-05-24-swedish-text-to-speech/index.html#pretrained-model-checkpoints",
    "href": "posts/2023-05-24-swedish-text-to-speech/index.html#pretrained-model-checkpoints",
    "title": "Swedish speech synthesis",
    "section": "Pretrained model checkpoints",
    "text": "Pretrained model checkpoints\nFor anyone interested in using this model to finetune other voices, we have uploaded the pretrained checkpoint weights to Huggingface.\ngit clone https://huggingface.co/KBLab/piper-tts-nst-swedish"
  },
  {
    "objectID": "posts/2023-05-24-swedish-text-to-speech/index.html#footnotes",
    "href": "posts/2023-05-24-swedish-text-to-speech/index.html#footnotes",
    "title": "Swedish speech synthesis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://github.com/rhasspy/piper‚Ü©Ô∏é\nhttps://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-18/‚Ü©Ô∏é\nhttps://ai.facebook.com/blog/multilingual-model-speech-recognition/‚Ü©Ô∏é\nhttps://github.com/rhasspy/piper/issues/72#issuecomment-1550170779‚Ü©Ô∏é\nhttps://github.com/rhasspy/piper‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025-03-07-welcome-KB-Whisper/index.html",
    "href": "posts/2025-03-07-welcome-KB-Whisper/index.html",
    "title": "Welcome KB-Whisper, a new fine-tuned Swedish Whisper model!",
    "section": "",
    "text": "The team behind KB-Whisper. Back row: Agnes Toftg√•rd, Robin Kurtz, Justyna Sikora. Front row: Leonora Vesterbacka, Faton Rekathati. Photography: Lina L√∂fstr√∂m Baker/KB"
  },
  {
    "objectID": "posts/2025-03-07-welcome-KB-Whisper/index.html#improving-swedish-speech-recognition",
    "href": "posts/2025-03-07-welcome-KB-Whisper/index.html#improving-swedish-speech-recognition",
    "title": "Welcome KB-Whisper, a new fine-tuned Swedish Whisper model!",
    "section": "Improving Swedish speech recognition",
    "text": "Improving Swedish speech recognition\nKBLab proudly presents KB-Whisper, a speech to text model fine-tuned using 50,000 hours of transcribed speech. Traditionally, Automatic Speech Recognition (ASR) systems have been based on models that either require an extensive unsupervised pretraining or supervised training that demands very high-quality orthographic transcripts, which are rare and expensive to produce.\nThe Whisper model (Radford et al. 2022), originally released by OpenAI has revolutionized automatic speech recognition, by showing that high performance could be achieved with a slight decrease in quality of the transcript, thus unlocking large amounts of training data that has hitherto not been used. Subtitles for TV often use abbreviations to fit the text on the screen, and are not considered a gold standard transcription. However, Radford et al. (2022) showed that this training data, extracted from the web, was still good enough for Whisper to learn.\nThe massive improvement gain with Whisper has been shown for English, and this result is directly proportional to the amount of English training data available on the web. For languages with fewer speakers, this type of data is less represented on the web and leads to poorer performance. In order to bridge this performance gap, the team at KBLab have constructed a training dataset of transcribed Swedish speech of unprecedented size, which is used to fine-tune Whisper models.\n\n\n\nThe team behind KB-Whisper. Back row: Agnes Toftg√•rd, Robin Kurtz, Justyna Sikora. Front row: Leonora Vesterbacka, Faton Rekathati. Photography: Lina L√∂fstr√∂m Baker/KB"
  },
  {
    "objectID": "posts/2025-03-07-welcome-KB-Whisper/index.html#subtitles-parliament-recordings-and-dialect-archives",
    "href": "posts/2025-03-07-welcome-KB-Whisper/index.html#subtitles-parliament-recordings-and-dialect-archives",
    "title": "Welcome KB-Whisper, a new fine-tuned Swedish Whisper model!",
    "section": "Subtitles, Parliament recordings and dialect archives",
    "text": "Subtitles, Parliament recordings and dialect archives\nThe National Library of Sweden is responsible for collecting, preserving and giving access to everything that is published in Sweden. The collections include the audiovisual archives that hold TV broadcasted in Sweden. Swedish subtitles paired with spoken Swedish from TV broadcasts constitute a large portion of the training data. Parliament recordings paired with high quality transcripts in the form of protocols provide the second largest source of the training data. This dataset is made publicly available on Huggingface, and constitute 23,000 hours of transcribed Swedish speech.\nBoth of these data sources have the advantage of covering wide variations of spoken Swedish. In order to enhance KB-Whisper‚Äôs performance in transcribing rare variations of Swedish, dialect recordings from The Institute for language and folklore (Isof) are included. Subtitles are also extracted from YouTube channels with Swedish content. Finally, datasets collected as crowd sourced initiatives such as Mozillas CommonVoice, Googles FLEURS and the Nordic Speech Technology (NST) dataset are used partly in the training, and partly in the evaluation.\n\n\n\nFrom the audiovisual archives of the National Library of Sweden. Photography: Lina L√∂fstr√∂m Baker/KB"
  },
  {
    "objectID": "posts/2025-03-07-welcome-KB-Whisper/index.html#two-stages-of-data-quality",
    "href": "posts/2025-03-07-welcome-KB-Whisper/index.html#two-stages-of-data-quality",
    "title": "Welcome KB-Whisper, a new fine-tuned Swedish Whisper model!",
    "section": "Two stages of data quality",
    "text": "Two stages of data quality\nTo assess the quality of transcriptions, i.e.¬†how well the subtitles, protocols and other transcriptions match the spoken audio, we implemented a preprocessing pipeline. The training examples are split into small 30-seconds chunks and each audio chunk is transcribed using OpenAI‚Äôs Whisper-large-v3 and KBLabs VoxRex (Malmsten, Haffenden, and B√∂rjeson 2022). Then the overlap between the original transcript and the two AI-generated transcriptions are assessed using Character Error Rate (CER), BLEU and ROUGE scores.\nThe first quality assessment catches examples with low or no overlap, but still of sufficient quality for the model to learn from, yielding a large training dataset with an increased probability of covering rare Swedish words and names, denoted below as the Stage 1 data. The second quality assessment focuses on defining the style of transcription, aiming to teach the model how to transcribe rather than providing a large number of examples. Two styles of transcriptions are defined: one more subtitle-like (Stage 2-subtitle), and one more orthographic (Stage 2-standard) for more precise transcription.\nEach stage is defined by a set of criteria on CER, BLEU and ROUGE, which are outlined in Table 1.\n\n\n\n\n\nBLEU\nCER-head\nCER-tail\nROUGE\n\n\n\n\nStage 1\n&gt; 0.2\n-\n-\n-\n\n\nStage 2 standard\n&gt; 0.6\n&lt; 0.3\n&lt; 0.3\n&gt; 0.7\n\n\nStage 2 subtitle\n&gt; 0.6\n&lt; 0.4\n&lt; 0.4\n-\n\n\n\n\nThe resulting hours the fall into each category is presented in Table 2.\n\n\n\n\n\n\n\n\n\n\nDataset\nStage 1 (h)\nStage 2 standard (h)\nStage 2 subtitle (h)\n\n\n\n\nSubtitles\n34,261\n3,110\n6,928\n\n\nRiksdag\n21,949\n5,119\n8,710\n\n\nISOF\n54\n54\n54\n\n\nNST\n250\n250\n250\n\n\nTotal\n56,514\n8,533\n15,942"
  },
  {
    "objectID": "posts/2025-03-07-welcome-KB-Whisper/index.html#based-on-open-weights-from-openais-whisper",
    "href": "posts/2025-03-07-welcome-KB-Whisper/index.html#based-on-open-weights-from-openais-whisper",
    "title": "Welcome KB-Whisper, a new fine-tuned Swedish Whisper model!",
    "section": "Based on open weights from OpenAI‚Äôs Whisper",
    "text": "Based on open weights from OpenAI‚Äôs Whisper\nFollowing the excellent Whisper fine-tuning tutorial from Huggingface we fine-tune all sizes of Whisper models on our Swedish training dataset of unprecedented size. The training is performed in a two-stage approach, where the first stage leverages the large Stage 1 dataset, followed by two parallel training stages where the model is either trained on the Stage 2-subtitle data or the Stage 2-standard data.\nThe training is executed on the Leonardo Supercomputer hosted by CINECA (Italy), that we were granted access to through a EuroHPC JU AI and data-intensive applications call.\n\n\n\nThe Leonardo Supercomputer. Source CINECA"
  },
  {
    "objectID": "posts/2025-03-07-welcome-KB-Whisper/index.html#a-great-improvement-in-swedish-asr",
    "href": "posts/2025-03-07-welcome-KB-Whisper/index.html#a-great-improvement-in-swedish-asr",
    "title": "Welcome KB-Whisper, a new fine-tuned Swedish Whisper model!",
    "section": "A great improvement in Swedish ASR",
    "text": "A great improvement in Swedish ASR\nWe have evaluated the models on three datasets: FLEURS (train and test set), NST (test set), and Common Voice 16.0 (train, validation, and test set). The CommonVoice and FLEURS data has not been part of the training set and can therefore serve as a benchmark for the models‚Äô out-of-domain performance.\nTo compare our newly trained models with OpenAI‚Äôs models, we calculate Word Error Rate (WER) and BLEU scores for each of the mentioned datasets. WER measures transcription accuracy by calculating the percentage of words that are substituted, deleted, or inserted, while the BLEU score evaluates how well a transcription matches the reference text.\nThe results evaluated in terms of WER is presented in the table below.\n\n\n\n\nModel size\n\nFLEURS\nCommonVoice\nNST\n\n\n\n\ntiny\nKBLab\n13.2\n12.9\n11.2\n\n\n\nOpenAI\n59.2\n67.8\n85.2\n\n\nbase\nKBLab\n9.1\n8.7\n7.8\n\n\n\nOpenAI\n39.6\n52.1\n53.4\n\n\nsmall\nKBLab\n7.3\n6.4\n6.6\n\n\n\nOpenAI\n20.6\n26.4\n26.4\n\n\nmedium\nKBLab\n6.6\n5.4\n5.8\n\n\n\nOpenAI\n12.1\n15.8\n17.1\n\n\nlarge-v3\nKBLab\n5.4\n4.1\n5.2\n\n\n\nOpenAI\n7.8\n9.5\n11.3\n\n\n\n\nOur evaluations show that the best-performing model reduces the WER by an average of 47% compared to Whisper-large-v3.\nThe results evaluated in terms of BLEU is presented in the table below.\n\n\n\n\nModel size\n\nFLEURS\nCommonVoice\nNST\n\n\n\n\ntiny\nKBLab\n76.6\n73.7\n74.3\n\n\n\nOpenAI\n26.9\n21.1\n24.0\n\n\nbase\nKBLab\n83.2\n79.9\n78.3\n\n\n\nOpenAI\n41.1\n32.5\n36.9\n\n\nsmall\nKBLab\n86.6\n83.5\n79.6\n\n\n\nOpenAI\n64.0\n56.5\n58.2\n\n\nmedium\nKBLab\n87.6\n85.0\n80.2\n\n\n\nOpenAI\n77.1\n70.1\n68.9\n\n\nlarge-v3\nKBLab\n89.8\n87.2\n81.1\n\n\n\nOpenAI\n84.9\n79.1\n75.1\n\n\n\n\nThe most significant improvements are observed in smaller models, demonstrating that high-quality transcriptions can be achieved with fewer computational resources.The KB-whisper-small model outperforms OpenAI‚Äôs whisper-large-v3, a model six times its size. ¬¥This means that similar transcription quality can be obtained using a smaller model, making speech-to-text more accessible and less costly."
  },
  {
    "objectID": "posts/2025-03-07-welcome-KB-Whisper/index.html#where-to-find-the-models",
    "href": "posts/2025-03-07-welcome-KB-Whisper/index.html#where-to-find-the-models",
    "title": "Welcome KB-Whisper, a new fine-tuned Swedish Whisper model!",
    "section": "Where to find the models?",
    "text": "Where to find the models?\nAll models are freely available for download from KBLab‚Äôs page on HuggingFace.\nFor more information on ASR models and how to use KBLab‚Äôs Whisper models programmatically, we recommend exploring this notebook."
  },
  {
    "objectID": "posts/2025-03-07-welcome-KB-Whisper/index.html#acknowledgments",
    "href": "posts/2025-03-07-welcome-KB-Whisper/index.html#acknowledgments",
    "title": "Welcome KB-Whisper, a new fine-tuned Swedish Whisper model!",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWe acknowledge the EuroHPC Joint Undertaking for awarding this project access to the EuroHPC supercomputer LEONARDO, hosted by CINECA (Italy) and the LEONARDO consortium, through the Development Access call and AI and data intensive applications access call.\n\n\n\nThe development work to produce the notebook mentioned above was carried out within the HUMINFRA infrastructure project."
  },
  {
    "objectID": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html",
    "href": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html",
    "title": "Preserving the history of cultural heritage conservation",
    "section": "",
    "text": "This page spread from Jonas Haquini Rhezelius, Runestones in Uppland (n.d. before 1666) - available in full here: Fc 8 - is one of the many sketch illustrations waiting to be found in KB‚Äôs F-collection."
  },
  {
    "objectID": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#in-search-of-the-past",
    "href": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#in-search-of-the-past",
    "title": "Preserving the history of cultural heritage conservation",
    "section": "In search of the past",
    "text": "In search of the past\nHow do we best preserve historical traces from oblivion? This question might feel particularly urgent today, as war, cyberattacks, and climate change create ever more acute threats to cultural heritage material (Bergvall 2023; Frederikzon and Haffenden 2023). But the challenge is far from new. Historians like Peter Fritzsche have described how shifting perceptions of time after the French Revolution‚Äîand concerns about what was vanishing‚Äîcontributed to the establishment of memory institutions across Europe in the mid-19th century (Fritzsche 2004; Lowenthal 1985; Swenson 2013). In Sweden, the story of heritage management goes even further back (Jensen 2018). As early as the 17th century, organized efforts were underway to collect, document, and preserve physical cultural heritage, exemplified by Johannes Bureus (1568‚Äì1652), Sweden‚Äôs first antiquarian, national archivist, and royal librarian (√Östr√∂m 2023).\nReading Bureus‚Äô 400-year-old manuscripts about his journeys to document rune stones gives us a tangible sense of this particular history (K√§llstr√∂m 2024). His drawings, notebooks, and compilations of quotations remind those of us working with cultural heritage conservation today that we are part of a much longer tradition‚Äîone in which the similarities can often surprise, despite the obvious contrasts with our digital age. To paraphrase L. P. Hartley‚Äôs famous observation: the past may be a foreign country, but they didn‚Äôt always do things differently there."
  },
  {
    "objectID": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#digitizing-the-history-of-cultural-heritage-conservation",
    "href": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#digitizing-the-history-of-cultural-heritage-conservation",
    "title": "Preserving the history of cultural heritage conservation",
    "section": "Digitizing the history of cultural heritage conservation",
    "text": "Digitizing the history of cultural heritage conservation\nThanks to a recently started collaborative project between the National Library of Sweden (Kungliga biblioteket, KB) and the Swedish National Heritage Board (Riksantikvarie√§mbetet, RA√Ñ), it is now possible to explore Sweden‚Äôs early cultural heritage preservation efforts in an entirely new way. Funded by the Royal Swedish Academy of Letters, History, and Antiquities (Vitterhetsakademien), the project is digitizing and making accessible a significant portion of this history. For KB, this involves the entire F-collection of the library‚Äôs archival holdings, which includes Bureus‚Äô manuscripts, while RA√Ñ is contributing a large number of archival documents from the 17th century to the late 19th century, drawn from its official archives.\nThrough digitization, this material‚Äîconstituting the oldest history of RA√Ñ and the Academy of Letters‚Äîis being made available to researchers and the public alike. Via KB‚Äôs service Manuscripta and RA√Ñ‚Äôs Arkivs√∂k, researchers and the general public will be able to explore a newly reunited collection, previously scattered across several archives, that bears witness to the work of our predecessors in preserving the past.\n\n\n\nStorage capsules, covers and boxes from many generations coexist in the F-collection. Some have withstood the test of time well, while others need to be inspected and replaced."
  },
  {
    "objectID": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#preservation-measuresa-journey-of-discovery-in-itself",
    "href": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#preservation-measuresa-journey-of-discovery-in-itself",
    "title": "Preserving the history of cultural heritage conservation",
    "section": "Preservation measures‚Äîa journey of discovery in itself",
    "text": "Preservation measures‚Äîa journey of discovery in itself\nWhat does it mean in practice to digitize historical manuscripts and works? Image capture is obviously a central part of digitally preserving and making the material accessible, but significant work also needs to be carried out before the manuscripts even reach our photographers. The following post gives an insight into how we work at KB to conserve and prepare manuscript collections for digitization.\nHandling these materials in any way poses the risk of further damage, since the volumes and texts have been heavily used and are often extremely fragile. Many of them have lived rich and varied lives in the field before arriving in the library‚Äôs archives, and require careful and considered treatment. KB‚Äôs conservators and bookbinders therefore conduct thorough condition assessments and conservation measures before the material continues through the process. The close encounter with the material that this involves bears striking similarities to Bureus‚Äô earlier explorations of ancient monuments and rune stones.\n\n\n\nReattaching a detached wax and paper seal.\n\n\nFirst and foremost, each volume undergoes an evaluation before any further handling and image capture. Based on established criteria, the condition of the various components‚Äîbindings, sewing, text blocks, and media (e.g.¬†ink)‚Äîis examined and documented. Different types of damage are graded according to their severity. For example, one volume might be in good condition with minor tears, while another might have extensive damage that risks worsening with handling. This evaluation determines whether conservation actions are needed and provides the best conditions for those handling the material during the rest of the digitisation process.\nThe assessment also considers whether the material is sensitive to changes in temperature and humidity, influencing where image capture can take place. For example, parchment covers tend to expand or shrink with climate fluctuations, and degraded ink risks cracking if pages ‚Äúbreathe‚Äù too much. Decisions are also made about whether new storage solutions are needed or if a damaged box must be replaced. In some cases, the conservators assist during digitization, such as in opening bindings at risk of breaking, loosening tight clasps, or carefully turning fragile pages."
  },
  {
    "objectID": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#a-collection-full-of-surprises",
    "href": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#a-collection-full-of-surprises",
    "title": "Preserving the history of cultural heritage conservation",
    "section": "A collection full of surprises",
    "text": "A collection full of surprises\nKB‚Äôs F-collection is a diverse and heterogeneous assemblage of bound handwritten documents, sketches of buildings, runes and landscapes, prints, and traditional texts annotated with marginalia. Every object in the collection has the potential to surprise, whether with striking imagery or something more unusual.\nThe first time we opened Martin Aschaneus‚Äô Collectaneum monetalium seu monetoscopia Sweogothica (n.d. before 1641) - Fb 17:2 - we discovered coins bound between the pages on parchment strips. While books containing objects might seem more challenging to digitize, in this case, the binding method is ideal: the coins are as easy to leaf through as the pages! The thrill of discovering little treasures like this has been a recurring highlight throughout the project, and this coin book, one of the first volumes assessed, got us off to an exciting start.\n\n\n\nUnexpected coins nesting as part of the book. Aschaneus‚Äô Collectaneum monetalium seu monetoscopia Sweogothica (n.d. before 1641), Fb 17:2."
  },
  {
    "objectID": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#protecting-the-past-for-the-future",
    "href": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#protecting-the-past-for-the-future",
    "title": "Preserving the history of cultural heritage conservation",
    "section": "Protecting the past for the future",
    "text": "Protecting the past for the future\nWe cannot turn back time, but with the right conservation measures and storage, we can extend the lifespan of materials at critical moments. Water damage mars the edges of many books. Ink spills have eaten through pages. Many bindings were originally temporary but have become permanent over time‚Äîeasily mobile in the field but not particularly durable.\nEven though digitization is a method of preserving collections, it is essential that the physical objects themselves, the original, remain in the best possible condition for future generations and research opportunities. With that in mind, tears and creases are among the most common damages that have needed to be repaired in the F-collection.\n\n\n\nShoring these fragments from ruin. Peringski√∂ld‚Äôs Om Heliga Birgitta (n.d.,1680-1720), Fh 26. Top right: before intervention. Bottom right: highlighting the damage of time. Left: after preservation measures.\n\n\nA good example can be found in capsule Fh 26, Johan Peringski√∂ld‚Äôs Om Heliga Birgitta (n.d., between 1680 and 1720, yet to be digitized), which contains a letter where the same sheet of paper serves as both the writing surface and the envelope. At first glance, the letter appears somewhat worn, but as soon as we remove it from its capsule, we realize that it cannot be handled without falling apart (see red marking in figure 5). To make digitization possible, the folds need to be carefully smoothed out, and extremely thin Japanese paper is applied with adhesive to secure the different parts of the letter. This process is carried out with great care, and we avoid affecting the ink with the moisture introduced during treatment as much as possible.\nMany of the volumes contain foldouts or plates that are larger than the rest of the pages and therefore had to be folded to fit within the covers. Johan Peringski√∂ld‚Äôs Konungatal eller f√∂rteckning p√• Svea och G√∂ta Rikes konungar och drottningar (n.d. before 1720) - Fh 13 - is an example of a book that includes several large printed plates, one of which illustrates a procession and lists its participants. This particular plate is one of the largest, measuring a full 75 cm (see figure 6). Foldouts present several challenges both before and during digitization. The paper in the folds is often weakened, and long tears are common. These must be stabilized‚Äînot only to prevent the loss of material but also to ensure that text and images remain intact during digitization. It is also common for foldouts to have been improperly refolded over time, creating new creases that need to be smoothed out.\n\n\n\nMediating social order via extended diagrams. Peringski√∂ld, Konungatal eller f√∂rteckning p√• Svea och G√∂ta Rikes konungar och drottningar, (n.d. before 1720), Fh 13."
  },
  {
    "objectID": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#only-the-beginning",
    "href": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#only-the-beginning",
    "title": "Preserving the history of cultural heritage conservation",
    "section": "Only the beginning‚Ä¶",
    "text": "Only the beginning‚Ä¶\nWe are still in the early stages of the project. Some digitized items have already been published through Manuscripta, and at the time of writing, our conservators and bookbinders have assessed and treated 90 of approximately 300 volumes. Many challenges lie ahead, given the diversity of the collection. We are also exploring the possibility of applying the Handwritten Text Recognition (HTR) models developed by our colleagues at the Swedish National Archives‚Äô AI lab to further enhance the material‚Äôs searchability and accessibility. We suspect more surprises await and hope that as you explore the digitized collection, you make your own discoveries‚Äîand perhaps gain a deeper appreciation for the dedication with which Johannes Bureus and so many others worked to preserve the heritage that continues to enrich our landscapes, both physical and digital."
  },
  {
    "objectID": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#keen-to-explore-further",
    "href": "posts/2025-01-31-preserving-the-history-of-cultural-heritage-conservation/index.html#keen-to-explore-further",
    "title": "Preserving the history of cultural heritage conservation",
    "section": "Keen to explore further?",
    "text": "Keen to explore further?\nThe material that has been digitized thus far in the project is freely available via Manuscripta. Follow the link to browse the collection!\nRead more about the collaborative project that enables this digitization work, RiViH, via the Swedish National Heritage Board‚Äôs website (in Swedish).\nSee how the research team attached to the project is making use of the material with this essay on the National Heritage Board‚Äôs K-blogg (in Swedish)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About KBLab",
    "section": "",
    "text": "I work as a director for KBLab. I‚Äôm also a Research Fellow at the Stockholm School of Economics, Center for Educational Leadership and Excellence (CELE). To a large extent, my job involves collocating data, competence, and computational resources.\nI have a PhD in Industrial Organization and Economics and I have been a Postdoctoral Fellow and Research Fellow at the Computational Social Science Lab at Stanford University (School of Ed.). Academically/intellectually I would perhaps prefer to label myself as ‚Äúsome kind of sociologist‚Äù.\n\n\n\n\n\n\nI work as a Data scientist at the lab and as an IT architect for the National Library. In these two roles I mainly focus on corpus creation, large transformer models, infrastructure design and prototyping. Having one foot in tech and data science and one in the library allows me to work strategically with larger questions such as where machine learning can be used to provide new insights into digital collections.\nMy background is in computer science and software development. Having implemented and worked closely with numerous metadata standards and systems I am a strong supporter of Linked (Open) Data as a paradigm and a way to connect information. My main driver is getting as much information and tools into the hands of as many people as possible.\n\n\n\n\n\n\nAs one of the KBLab‚Äôs data scientists I work on developing models and datasets that are intended to be used not only internally at the National Library, but also for general use by industry, governmental agencies, and academia. With the recent rise of importance of transformer-based language models, we focus on making use of the library‚Äôs vast amounts of text data to train and publish these language models.\nI have a strong background in language technology, with degrees in natural language processing (NLP), computational linguistics and computer science. Before starting at the library in October 2020 I received my doctoral degree in computer science (datalogi) from Link√∂ping University, working on semantic dependency parsing, studying algorithms, machine learning methods, and potential applications.\n\n\n\n\n\n\nMy first contact with KBLab was as an external researcher. In the spring of 2020 I wrote a masters thesis on the subject of curating news sections in historical newspapers. Over the summer I continued work on the same project as a research assistant for Link√∂ping University, before eventually ending up as a Data scientist with KBLab in September 2020. At KBLab I work with making the library‚Äôs collections of visual materials searchable and navigable. In addition to this I also work with training language models and speech recognition models.\nMy background is in statistics and machine Learning. As a statistician I of course love the programming language R, though nowadays I spend most of my time working with Python.\n\n\n\n\n\n\nMy position involves helping researchers use the lab‚Äôs resources and the library‚Äôs digital collections. I assist in dealing with applications for research collaboration, in getting research projects up and running at the lab, and in fixing problems that arise as part of the research process. I also work with communicating and writing articles about the lab‚Äôs development projects, as well as running workshops and organizing outreach events to inform the academic community about our tools and resources. I‚Äôm always open to new initiatives for collaboration and outreach, so please get in touch!\nMy academic background is in the field of intellectual and cultural history. I have an MPhil in Political Thought and Intellectual history from Cambridge University, and a PhD in the History of Science and Ideas from Uppsala University. My doctoral thesis, Every Man His Own Monument (2018), examined novel practices of self-monumentalizing in nineteenth-century Britain to present a new argument about the interconnection of celebrity culture and posthumous fame in this period. Apart from working at KBLab, I have also begun work on a new, RJ-financed project that explores the emergence of self-erasure and the longer history of the right to be forgotten. My involvement with KBLab and my research interests are underpinned by a reflexive concern with the ways in which cultural heritage is produced and made use of.\n\n\n\n\n\n\nI am a developer at the National Library nearing two decades of working with the national Libris systems. Joined the KBLab team part-time in 2019 and would describe myself as a Semantic information modeler thinking about ‚Äúconnectedness‚Äù and usefulness of data. RDF is the language of expression and my current role at KBLab is being a helping hand in this field and integration of our metadata infrastructure. Special interest as some may already have surmised is linking entities and identity disambiguation.\nMy background is sprawling but started somewhere long ago in Computer Science classes at Stockholm University/DSV. Gravitated to musicology and sound engineering for a while but back hands on in the information/knowledge sphere again.\n\n\n\n\n\n\nMy contribution to the lab is within overall strategy and usability. In my work I try to understand who our users are, what their needs are, and how to reach them in the best possible way. I have driven the process of mapping the lab user journey, which changes continuously as the lab develops. I¬¥ve also had a leading part in identifying the core values for the lab. I only work part-time in the lab, but as a product manager I naturally take the lab‚Äôs questions into the various forums I participate in at KB.\nI have previously worked within the private sector as a business analyst and business developer in e-commerce. Throughout my career, I have always had a focus on the user experience, but with a commercial insight. I have a master‚Äôs degree in Computer and Systems Science from Stockholm University.\n\n\n\n\n\n\nI have always been passionate about languages and linguistics. However, having studied philology and worked in the language industry for couple years, I had a chance to stumble upon many uses of what I, at the time, did not know was NLP. I became fascinated by the possibilities of combining linguistic knowledge with computational methods. This interest has led me into pursuing a Master‚Äôs degree in Language Technology at Uppsala University. I graduated in 2022 and started working as a data scientist at KBLab.\n\n\n\n\n\n\nI work as a researcher at the KBLab and as curator of the medieval and early modern manuscript collection at the National Library. I am the creator of manuscripta.se, a research infrastructure for the study of medieval and early modern manuscripts preserved in Swedish libraries. My role at the lab is mainly focused on maintaining and developing this infrastructure.\nMy background is in Classics and I have a PhD in Greek from Uppsala University (2012). My main areas of expertise are textual criticism, manuscript studies, and digital manuscript cataloguing in TEI and IIIF. Since 2012 I have worked in several externally funded projects to catalogue and digitise Greek manuscripts, medieval Old Swedish manuscripts, and post-medieval Old Swedish manuscripts. Currently, I am working, part-time, in two large scale cataloguing and digitisation projects, one involving the medieval Latin theological manuscripts at the National Library, another the West Norse manuscripts at the National Library and Uppsala University Library.\n\n\n\n\n\n\nI work as a senior data scientist at KBLab. I first came into contact with the lab when I used KB-BERT in my previous job as a data scientist at another government agency. Being impressed by the quality and impact of the many large language models trained on swedish data, I applied for a job as soon as there was an opening and voil√†! I now lead a project training speech recognition models on the librarys audiovisual archives, using supercomputing facilities provided by the EuroHPC Joint Undertaking. I have a background in particle physics, with research conducted at CERN, and a PhD from ETH Z√ºrich, as well as a postdoc experience at NYU doing research in ML applied to particle physics.\n\n\n\n\n\n\nI am a data scientist at KBLab, where I work with training Swedish language models (such as our speech-to-text model kb-whisper), and with finding ways to integrate AI in the broader activity at the National Library (such as auto-transcribing radio-archives to make them searchable). My background is a MSE from Link√∂ping University in Data-driven Analysis and Machine Learning, where I found that natural language processing was a perfect mix of my interests in languages and computer science."
  },
  {
    "objectID": "about.html#people",
    "href": "about.html#people",
    "title": "About KBLab",
    "section": "",
    "text": "I work as a director for KBLab. I‚Äôm also a Research Fellow at the Stockholm School of Economics, Center for Educational Leadership and Excellence (CELE). To a large extent, my job involves collocating data, competence, and computational resources.\nI have a PhD in Industrial Organization and Economics and I have been a Postdoctoral Fellow and Research Fellow at the Computational Social Science Lab at Stanford University (School of Ed.). Academically/intellectually I would perhaps prefer to label myself as ‚Äúsome kind of sociologist‚Äù.\n\n\n\n\n\n\nI work as a Data scientist at the lab and as an IT architect for the National Library. In these two roles I mainly focus on corpus creation, large transformer models, infrastructure design and prototyping. Having one foot in tech and data science and one in the library allows me to work strategically with larger questions such as where machine learning can be used to provide new insights into digital collections.\nMy background is in computer science and software development. Having implemented and worked closely with numerous metadata standards and systems I am a strong supporter of Linked (Open) Data as a paradigm and a way to connect information. My main driver is getting as much information and tools into the hands of as many people as possible.\n\n\n\n\n\n\nAs one of the KBLab‚Äôs data scientists I work on developing models and datasets that are intended to be used not only internally at the National Library, but also for general use by industry, governmental agencies, and academia. With the recent rise of importance of transformer-based language models, we focus on making use of the library‚Äôs vast amounts of text data to train and publish these language models.\nI have a strong background in language technology, with degrees in natural language processing (NLP), computational linguistics and computer science. Before starting at the library in October 2020 I received my doctoral degree in computer science (datalogi) from Link√∂ping University, working on semantic dependency parsing, studying algorithms, machine learning methods, and potential applications.\n\n\n\n\n\n\nMy first contact with KBLab was as an external researcher. In the spring of 2020 I wrote a masters thesis on the subject of curating news sections in historical newspapers. Over the summer I continued work on the same project as a research assistant for Link√∂ping University, before eventually ending up as a Data scientist with KBLab in September 2020. At KBLab I work with making the library‚Äôs collections of visual materials searchable and navigable. In addition to this I also work with training language models and speech recognition models.\nMy background is in statistics and machine Learning. As a statistician I of course love the programming language R, though nowadays I spend most of my time working with Python.\n\n\n\n\n\n\nMy position involves helping researchers use the lab‚Äôs resources and the library‚Äôs digital collections. I assist in dealing with applications for research collaboration, in getting research projects up and running at the lab, and in fixing problems that arise as part of the research process. I also work with communicating and writing articles about the lab‚Äôs development projects, as well as running workshops and organizing outreach events to inform the academic community about our tools and resources. I‚Äôm always open to new initiatives for collaboration and outreach, so please get in touch!\nMy academic background is in the field of intellectual and cultural history. I have an MPhil in Political Thought and Intellectual history from Cambridge University, and a PhD in the History of Science and Ideas from Uppsala University. My doctoral thesis, Every Man His Own Monument (2018), examined novel practices of self-monumentalizing in nineteenth-century Britain to present a new argument about the interconnection of celebrity culture and posthumous fame in this period. Apart from working at KBLab, I have also begun work on a new, RJ-financed project that explores the emergence of self-erasure and the longer history of the right to be forgotten. My involvement with KBLab and my research interests are underpinned by a reflexive concern with the ways in which cultural heritage is produced and made use of.\n\n\n\n\n\n\nI am a developer at the National Library nearing two decades of working with the national Libris systems. Joined the KBLab team part-time in 2019 and would describe myself as a Semantic information modeler thinking about ‚Äúconnectedness‚Äù and usefulness of data. RDF is the language of expression and my current role at KBLab is being a helping hand in this field and integration of our metadata infrastructure. Special interest as some may already have surmised is linking entities and identity disambiguation.\nMy background is sprawling but started somewhere long ago in Computer Science classes at Stockholm University/DSV. Gravitated to musicology and sound engineering for a while but back hands on in the information/knowledge sphere again.\n\n\n\n\n\n\nMy contribution to the lab is within overall strategy and usability. In my work I try to understand who our users are, what their needs are, and how to reach them in the best possible way. I have driven the process of mapping the lab user journey, which changes continuously as the lab develops. I¬¥ve also had a leading part in identifying the core values for the lab. I only work part-time in the lab, but as a product manager I naturally take the lab‚Äôs questions into the various forums I participate in at KB.\nI have previously worked within the private sector as a business analyst and business developer in e-commerce. Throughout my career, I have always had a focus on the user experience, but with a commercial insight. I have a master‚Äôs degree in Computer and Systems Science from Stockholm University.\n\n\n\n\n\n\nI have always been passionate about languages and linguistics. However, having studied philology and worked in the language industry for couple years, I had a chance to stumble upon many uses of what I, at the time, did not know was NLP. I became fascinated by the possibilities of combining linguistic knowledge with computational methods. This interest has led me into pursuing a Master‚Äôs degree in Language Technology at Uppsala University. I graduated in 2022 and started working as a data scientist at KBLab.\n\n\n\n\n\n\nI work as a researcher at the KBLab and as curator of the medieval and early modern manuscript collection at the National Library. I am the creator of manuscripta.se, a research infrastructure for the study of medieval and early modern manuscripts preserved in Swedish libraries. My role at the lab is mainly focused on maintaining and developing this infrastructure.\nMy background is in Classics and I have a PhD in Greek from Uppsala University (2012). My main areas of expertise are textual criticism, manuscript studies, and digital manuscript cataloguing in TEI and IIIF. Since 2012 I have worked in several externally funded projects to catalogue and digitise Greek manuscripts, medieval Old Swedish manuscripts, and post-medieval Old Swedish manuscripts. Currently, I am working, part-time, in two large scale cataloguing and digitisation projects, one involving the medieval Latin theological manuscripts at the National Library, another the West Norse manuscripts at the National Library and Uppsala University Library.\n\n\n\n\n\n\nI work as a senior data scientist at KBLab. I first came into contact with the lab when I used KB-BERT in my previous job as a data scientist at another government agency. Being impressed by the quality and impact of the many large language models trained on swedish data, I applied for a job as soon as there was an opening and voil√†! I now lead a project training speech recognition models on the librarys audiovisual archives, using supercomputing facilities provided by the EuroHPC Joint Undertaking. I have a background in particle physics, with research conducted at CERN, and a PhD from ETH Z√ºrich, as well as a postdoc experience at NYU doing research in ML applied to particle physics.\n\n\n\n\n\n\nI am a data scientist at KBLab, where I work with training Swedish language models (such as our speech-to-text model kb-whisper), and with finding ways to integrate AI in the broader activity at the National Library (such as auto-transcribing radio-archives to make them searchable). My background is a MSE from Link√∂ping University in Data-driven Analysis and Machine Learning, where I found that natural language processing was a perfect mix of my interests in languages and computer science."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "",
    "text": "Recent advances in Handwritten Text Recognition (HTR) have opened up powerful new ways to access cultural heritage material. HTR models can now achieve high levels of accuracy in transcribing handwritten texts, making once difficult-to-decipher historical archives searchable in ways that resemble the granular digital search we take for granted online (Nockels, Gooding, and Terras 2024). At the Swedish National Archives‚Äô AI lab, our colleagues have spent the past few years significantly improving HTR models for modern and early modern Swedish. The technology is now being applied at scale, with over a million documents set to be transcribed and made digitally searchable.\nBut what about older handwritten archival holdings? The National Library of Sweden (KB) has over 300 medieval manuscripts, many of which remain challenging to search, analyze or even read due to the complexity and variability of the writing style. At KBLab we have begun exploring the possibilities of HTR to enhance the searchability of this material. In particular, we have been testing and comparing existing HTR models for Latin texts to establish the state of the art among openly available tools. This forms part of our broader mission to improve access to the library‚Äôs digital collections by harnessing the capacities of AI (B√∂rjeson et al. 2024).\nIn this post, we present some initial results from our experiments, highlight key challenges and share insights into the potential and limitations of HTR for Latin manuscripts. Before assessing these models, we begin with a brief overview of the library‚Äôs Latin manuscript holdings and an introduction to how HTR techniques work."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#improving-swedish-speech-recognition",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#improving-swedish-speech-recognition",
    "title": "From Parchment To Pixel",
    "section": "Improving Swedish speech recognition",
    "text": "Improving Swedish speech recognition\nKBLab proudly presents KB-Whisper, a speech to text model fine-tuned using 50,000 hours of transcribed speech. Traditionally, Automatic Speech Recognition (ASR) systems have been based on models that either require an extensive unsupervised pretraining or supervised training that demands very high-quality orthographic transcripts, which are rare and expensive to produce.\nThe Whisper model (Radford et al. 2022), originally released by OpenAI has revolutionized automatic speech recognition, by showing that high performance could be achieved with a slight decrease in quality of the transcript, thus unlocking large amounts of training data that has hitherto not been used. Subtitles for TV often use abbreviations to fit the text on the screen, and are not considered a gold standard transcription. However, Radford et al. (2022) showed that this training data, extracted from the web, was still good enough for Whisper to learn.\nThe massive improvement gain with Whisper has been shown for English, and this result is directly proportional to the amount of English training data available on the web. For languages with fewer speakers, this type of data is less represented on the web and leads to poorer performance. In order to bridge this performance gap, the team at KBLab have constructed a training dataset of transcribed Swedish speech of unprecedented size, which is used to fine-tune Whisper models.\n\n\n\nThe team behind KB-Whisper. Back row: Agnes Toftg√•rd, Robin Kurtz, Justyna Sikora. Front row: Leonora Vesterbacka, Faton Rekathati. Photography: Lina L√∂fstr√∂m Baker/KB"
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#subtitles-parliament-recordings-and-dialect-archives",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#subtitles-parliament-recordings-and-dialect-archives",
    "title": "From Parchment To Pixel",
    "section": "Subtitles, Parliament recordings and dialect archives",
    "text": "Subtitles, Parliament recordings and dialect archives\nThe National Library of Sweden is responsible for collecting, preserving and giving access to everything that is published in Sweden. The collections include the audiovisual archives that hold TV broadcasted in Sweden. Swedish subtitles paired with spoken Swedish from TV broadcasts constitute a large portion of the training data. Parliament recordings paired with high quality transcripts in the form of protocols provide the second largest source of the training data. This dataset is made publicly available on Huggingface, and constitute 23,000 hours of transcribed Swedish speech.\nBoth of these data sources have the advantage of covering wide variations of spoken Swedish. In order to enhance KB-Whisper‚Äôs performance in transcribing rare variations of Swedish, dialect recordings from The Institute for language and folklore (Isof) are included. Subtitles are also extracted from YouTube channels with Swedish content. Finally, datasets collected as crowd sourced initiatives such as Mozillas CommonVoice, Googles FLEURS and the Nordic Speech Technology (NST) dataset are used partly in the training, and partly in the evaluation.\n\n\n\nFrom the audiovisual archives of the National Library of Sweden. Photography: Lina L√∂fstr√∂m Baker/KB"
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#two-stages-of-data-quality",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#two-stages-of-data-quality",
    "title": "From Parchment To Pixel",
    "section": "Two stages of data quality",
    "text": "Two stages of data quality\nTo assess the quality of transcriptions, i.e.¬†how well the subtitles, protocols and other transcriptions match the spoken audio, we implemented a preprocessing pipeline. The training examples are split into small 30-seconds chunks and each audio chunk is transcribed using OpenAI‚Äôs Whisper-large-v3 and KBLabs VoxRex (Malmsten, Haffenden, and B√∂rjeson 2022). Then the overlap between the original transcript and the two AI-generated transcriptions are assessed using Character Error Rate (CER), BLEU and ROUGE scores.\nThe first quality assessment catches examples with low or no overlap, but still of sufficient quality for the model to learn from, yielding a large training dataset with an increased probability of covering rare Swedish words and names, denoted below as the Stage 1 data. The second quality assessment focuses on defining the style of transcription, aiming to teach the model how to transcribe rather than providing a large number of examples. Two styles of transcriptions are defined: one more subtitle-like (Stage 2-subtitle), and one more orthographic (Stage 2-standard) for more precise transcription.\nEach stage is defined by a set of criteria on CER, BLEU and ROUGE, which are outlined in Table 1.\n\n\n\n\n\nBLEU\nCER-head\nCER-tail\nROUGE\n\n\n\n\nStage 1\n&gt; 0.2\n-\n-\n-\n\n\nStage 2 standard\n&gt; 0.6\n&lt; 0.3\n&lt; 0.3\n&gt; 0.7\n\n\nStage 2 subtitle\n&gt; 0.6\n&lt; 0.4\n&lt; 0.4\n-\n\n\n\n\nThe resulting hours the fall into each category is presented in Table 2.\n\n\n\n\n\n\n\n\n\n\nDataset\nStage 1 (h)\nStage 2 standard (h)\nStage 2 subtitle (h)\n\n\n\n\nSubtitles\n34,261\n3,110\n6,928\n\n\nRiksdag\n21,949\n5,119\n8,710\n\n\nISOF\n54\n54\n54\n\n\nNST\n250\n250\n250\n\n\nTotal\n56,514\n8,533\n15,942"
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#based-on-open-weights-from-openais-whisper",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#based-on-open-weights-from-openais-whisper",
    "title": "From Parchment To Pixel",
    "section": "Based on open weights from OpenAI‚Äôs Whisper",
    "text": "Based on open weights from OpenAI‚Äôs Whisper\nFollowing the excellent Whisper fine-tuning tutorial from Huggingface we fine-tune all sizes of Whisper models on our Swedish training dataset of unprecedented size. The training is performed in a two-stage approach, where the first stage leverages the large Stage 1 dataset, followed by two parallel training stages where the model is either trained on the Stage 2-subtitle data or the Stage 2-standard data.\nThe training is executed on the Leonardo Supercomputer hosted by CINECA (Italy), that we were granted access to through a EuroHPC JU AI and data-intensive applications call.\n\n\n\nThe Leonardo Supercomputer. Source CINECA"
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#a-great-improvement-in-swedish-asr",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#a-great-improvement-in-swedish-asr",
    "title": "From Parchment to Pixels: Exploring HTR Models for KB‚Äôs Latin Manuscripts",
    "section": "A great improvement in Swedish ASR",
    "text": "A great improvement in Swedish ASR\nWe have evaluated the models on three datasets: FLEURS (train and test set), NST (test set), and Common Voice 16.0 (train, validation, and test set). The CommonVoice and FLEURS data has not been part of the training set and can therefore serve as a benchmark for the models‚Äô out-of-domain performance.\nTo compare our newly trained models with OpenAI‚Äôs models, we calculate Word Error Rate (WER) and BLEU scores for each of the mentioned datasets. WER measures transcription accuracy by calculating the percentage of words that are substituted, deleted, or inserted, while the BLEU score evaluates how well a transcription matches the reference text.\nThe results evaluated in terms of WER is presented in the table below.\n\n\n\n\nModel size\n\nFLEURS\nCommonVoice\nNST\n\n\n\n\ntiny\nKBLab\n13.2\n12.9\n11.2\n\n\n\nOpenAI\n59.2\n67.8\n85.2\n\n\nbase\nKBLab\n9.1\n8.7\n7.8\n\n\n\nOpenAI\n39.6\n52.1\n53.4\n\n\nsmall\nKBLab\n7.3\n6.4\n6.6\n\n\n\nOpenAI\n20.6\n26.4\n26.4\n\n\nmedium\nKBLab\n6.6\n5.4\n5.8\n\n\n\nOpenAI\n12.1\n15.8\n17.1\n\n\nlarge-v3\nKBLab\n5.4\n4.1\n5.2\n\n\n\nOpenAI\n7.8\n9.5\n11.3\n\n\n\n\nOur evaluations show that the best-performing model reduces the WER by an average of 47% compared to Whisper-large-v3.\nThe results evaluated in terms of BLEU is presented in the table below.\n\n\n\n\nModel size\n\nFLEURS\nCommonVoice\nNST\n\n\n\n\ntiny\nKBLab\n76.6\n73.7\n74.3\n\n\n\nOpenAI\n26.9\n21.1\n24.0\n\n\nbase\nKBLab\n83.2\n79.9\n78.3\n\n\n\nOpenAI\n41.1\n32.5\n36.9\n\n\nsmall\nKBLab\n86.6\n83.5\n79.6\n\n\n\nOpenAI\n64.0\n56.5\n58.2\n\n\nmedium\nKBLab\n87.6\n85.0\n80.2\n\n\n\nOpenAI\n77.1\n70.1\n68.9\n\n\nlarge-v3\nKBLab\n89.8\n87.2\n81.1\n\n\n\nOpenAI\n84.9\n79.1\n75.1\n\n\n\n\nThe most significant improvements are observed in smaller models, demonstrating that high-quality transcriptions can be achieved with fewer computational resources.The KB-whisper-small model outperforms OpenAI‚Äôs whisper-large-v3, a model six times its size. ¬¥This means that similar transcription quality can be obtained using a smaller model, making speech-to-text more accessible and less costly."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#where-to-find-the-models",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#where-to-find-the-models",
    "title": "From Parchment to Pixels: Exploring HTR Models for KB‚Äôs Latin Manuscripts",
    "section": "Where to find the models?",
    "text": "Where to find the models?\nAll models are freely available for download from KBLab‚Äôs page on HuggingFace.\nFor more information on ASR models and how to use KBLab‚Äôs Whisper models programmatically, we recommend exploring this notebook."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#acknowledgments",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#acknowledgments",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nPart of this development work was carried out within the HUMINFRA infrastructure project."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#introduction",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#introduction",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "",
    "text": "Recent advances in Handwritten Text Recognition (HTR) have opened up powerful new ways to access cultural heritage material. HTR models can now achieve high levels of accuracy in transcribing handwritten texts, making once difficult-to-decipher historical archives searchable in ways that resemble the granular digital search we take for granted online (Nockels, Gooding, and Terras 2024). At the Swedish National Archives‚Äô AI lab, our colleagues have spent the past few years significantly improving HTR models for modern and early modern Swedish. The technology is now being applied at scale, with over a million documents set to be transcribed and made digitally searchable.\nBut what about older handwritten archival holdings? The National Library of Sweden (KB) has over 300 medieval manuscripts, many of which remain challenging to search, analyze or even read due to the complexity and variability of the writing style. At KBLab we have begun exploring the possibilities of HTR to enhance the searchability of this material. In particular, we have been testing and comparing existing HTR models for Latin texts to establish the state of the art among openly available tools. This forms part of our broader mission to improve access to the library‚Äôs digital collections by harnessing the capacities of AI (B√∂rjeson et al. 2024).\nIn this post, we present some initial results from our experiments, highlight key challenges and share insights into the potential and limitations of HTR for Latin manuscripts. Before assessing these models, we begin with a brief overview of the library‚Äôs Latin manuscript holdings and an introduction to how HTR techniques work."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#kbs-latin-manuscripts",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#kbs-latin-manuscripts",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "KB‚Äôs Latin manuscripts",
    "text": "KB‚Äôs Latin manuscripts\nAround 60% of KB‚Äôs medieval manuscripts are written in Latin. These span from the early eighth century to the late sixteenth century and include texts on theology, law, grammar, philosophy, medicine, astronomy and rhetoric (B√∂ckerman 2025). The largest subgroup consists of theological manuscripts - about 200 in total. As part of an ongoing effort to improve access to these materials, a project is currently underway to provide detailed catalog descriptions and full digitization of all theological Latin manuscripts in the collection. Once digitized, the materials are made available through the library‚Äôs digital research infrastructure: manuscripta.se. The project, funded by Riksbankens Jubileumsfond, involves a team of Latin specialists at KB and is scheduled for completion in 2026.\n\n\nhttps://kb-labb.github.io/posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai\nFrom the 8th to the 15th century, Latin manuscripts were written in a variety of scripts that reflect shifts in writing styles. Between 700 and 1000, the dominant script was Carolingian minuscule - a relatively clear and legible hand. Yet KB holds only a few manuscripts from this early period. In the following centuries (1000‚Äì1200), the so-called protogothic script emerged. This transitional style is important but still relatively understudied, and the library‚Äôs manuscripts from this period are also limited. The majority of Latin manuscripts at KB -roughly 90 - date from 1200 to 1500 and are written in various Gothic scripts. These include the highly formal textualis (often used for liturgical purposes) and the more practical, everyday cursiva. In the 15th century, the humanist minuscule - developed in Italy as a deliberate return to earlier Carolingian and protogothic forms - appears as well. This style is notably more legible, and the library holds a few examples written in this hand.\n\n\nSaint Birgitta - or Bridget of Sweden, as she is often referred to in international contexts - lived in 14th-century Sweden and, beginning in the 1340s, is said to have received a series of divine visions. She initially recorded these revelations in Swedish, which were later translated into Latin. The Latin version became the standard text and circulated widely across Europe, contributing to Birgitta‚Äôs growing international influence.\nWe selected a few pages from five Latin manuscripts (A 32, A 66, A 68, A 69, A 70) containing The Revelations by St.¬†Birgitta of Sweden to explore how well automatic text recognition can be applied to Latin manuscripts in the library‚Äôs collections. The aim was to assess whether existing HTR models could handle the complexity and variation typical of medieval Latin texts, and to identify which models might serve as suitable starting points for future fine-tuning."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#htr-architecture",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#htr-architecture",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "HTR architecture",
    "text": "HTR architecture\nHTR is a form of optical character recognition (OCR) designed to convert handwritten text into machine-readable formats. In recent years, HTR has seen significant advancements - evolving from early rule-based and statistical approaches to deep learning and transformer-based architectures.\nEarly HTR systems relied heavily on handcrafted features, lexicons, and probabilistic models such as Hidden Markov Models (HMMs), often combined with n-gram language models. The advent of deep learning marked a major turning point. In particular, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) - especially Long Short-Term Memory (LSTM) networks - enabled models to extract features directly from raw image inputs and to capture the sequential nature of handwritten text lines. This architecture significantly improved recognition accuracy, especially for more challenging handwritten scripts and historical manuscripts.\nMore recently, transformer-based models - such as TrOCR - have pushed the boundaries of what HTR systems can achieve. Unlike traditional RNNs, transformer models rely on self-attention mechanisms that capture long-range dependencies in sequences more effectively (Vaswani et al. 2017).\nThese technical advances have been supported by the increasing availability of annotated training datasets, the synthetic generation of handwritten texts, and platforms such as eScriptorium, Transkribus, and the Hugging Face Model Hub, which enable the training, fine-tuning and deployment of HTR models."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#htr-process",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#htr-process",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "HTR process",
    "text": "HTR process\nHTR models can operate at different levels - character, word, line or page. The models we tested are mostly line-level, so we‚Äôll focus on how that process works.\nTo recognize text from a manuscript page using a line-level model, the first key step is segmentation. This means breaking the page down into parts the model can actually read - starting with identifying where the text is and then splitting it into individual lines. Segmentation has two main components:\n\nRegion detection: Identifying blocks of text on the page and separating them from non-textual elements such as images, margins and decorations etc (see Figure¬†1 (a)).\nLine extraction: Finding each individual line of text within those regions and cropping them out as individual images (see Figure¬†1 (b)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure¬†1: Segmentation involves first detecting region (left) and then lines of text (right)\n\n\n\nSegmentation is crucial because it also determines the reading order. Line-level HTR models process one line at a time and have no built-in understanding of sequence. So if the segmentation process gets the reading order wrong, the final output will not make sense.\nTools like eScriptorium or custom setups using object detection models like YOLO - You Only Look Once (Redmon et al. 2015) - can handle the segmentation process. These tools often allow manual adjustments, which is especially useful for complex layouts in historical documents.\nOnce the lines are segmented, they are passed to the HTR model, which reads each line image and outputs the recognized text. Some platforms also use language models to refine grammar, predict words or assign confidence scores that flag uncertain results for human review.\nFinally, the transcribed lines are reassembled to reconstruct the full page of handwriting as digital text. As suggested, this final output relies heavily on accurate segmentation from the start. Without the correct reading order, we cannot expect a legible text at the end of the process."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#challanges-with-htr",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#challanges-with-htr",
    "title": "From Parchment to Pixels: Exploring HTR Models for KB‚Äôs Latin Manuscripts",
    "section": "Challanges with HTR",
    "text": "Challanges with HTR"
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#challanges-with-htr-and",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#challanges-with-htr-and",
    "title": "From Parchment to Pixels: Exploring HTR Models for KB‚Äôs Latin Manuscripts",
    "section": "Challanges with HTR and",
    "text": "Challanges with HTR and\nOne of the main challenges in applying HTR to Latin manuscripts is the sheer diversity of handwriting styles, typescripts and scribal conventions, which can vary significantly even within a single document or collection. This variability makes generalization difficult‚Äîmodels trained on one set of manuscripts often struggle to adapt to others without retraining or fine-tuning.\nFor instance, a model trained on Carolingian minuscule‚Äîa relatively open and legible script used between the 9th and 12th centuries‚Äîmay perform poorly when applied to Gothic textualis, a denser script that emerged later and features tightly packed letters, numerous ligatures and frequent abbreviations. Even within the Gothic tradition, different writing styles such as the more formal textualis and the faster, less consistent cursiva present distinct challenges. Early modern humanist scripts, which deliberately revived classical letterforms and often resemble modern fonts, might seem easier to read at first glance. Yet even these show variation in letter shapes‚Äîsuch as long s versus short s or u versus v‚Äîand are often accompanied by marginal notes in entirely different hands.\nAnother complicating factor is the way manuscripts are transcribed. Transcriptions can vary significantly in their level of editorial intervention, which in turn influences how models are trained and evaluated. A diplomatic transcription preserves original spellings, abbreviations and letterforms, aiming to capture the text as it appears in the manuscript. A semi-diplomatic transcription may expand some abbreviations or regularise spelling for readability, whereas a normalised transcription modernises the text to align with contemporary orthographic standards. Abbreviations are expanded, non-standard characters are replaced with modern equivalents, and spacing, punctuation and capitalisation may be adjusted. While this makes the material more accessible for modern readers and analysis, it may strip away useful palaeographic information.\nAbbreviations themselves pose a specific challenge. Latin scribes used a wide array of abbreviation marks to save time and space, and these are not always easy for an HTR model to detect or interpret. Whether a model expands or retains them often depends on the transcription style it was trained on.\nTaken together, the variability in scripts, transcription practices and scribal habits means that a ‚Äúone-size-fits-all‚Äù approach to HTR rarely works for Latin manuscripts. Instead, models need to be adapted to the specific characteristics of the material‚Äîfor particular writing styles, time periods or even individual scribes. Though such fine-tuning of models is a time-consuming process, it is necessary to produce meaningful and accurate automatic transcriptions. Tools and platforms like eScriptorium, Transkribus, and Hugging Face now offer increasingly accessible ways to train and customize HTR models to meet these challenges."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#challenges-with-htr-and-latin-manuscripts",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#challenges-with-htr-and-latin-manuscripts",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Challenges with HTR and Latin manuscripts",
    "text": "Challenges with HTR and Latin manuscripts\nOne of the main challenges in applying HTR to Latin manuscripts is the sheer diversity of handwriting styles, typescripts and scribal conventions, which can vary significantly even within a single document or collection. This variability makes generalization difficult - models trained on one set of manuscripts often struggle to adapt to others without retraining or fine-tuning.\nFor instance, a model trained on Carolingian minuscule - a relatively open and legible script used between the 9th and 12th centuries - may perform poorly when applied to Gothic textualis, a denser script that emerged later and features tightly packed letters, numerous ligatures and frequent abbreviations. Even within the Gothic tradition, different writing styles such as the more formal textualis and the faster, less consistent cursiva present distinct challenges. Early modern humanist scripts, which deliberately revived classical letterforms and often resemble modern fonts, might seem easier to read at first glance. Yet even these show variation in letter shapes - such as long s versus short s or u versus v - and are often accompanied by marginal notes in entirely different hands.\nAnother complicating factor is the way manuscripts are transcribed. Transcriptions can vary significantly in their level of editorial intervention, which in turn influences how models are trained and evaluated. A diplomatic transcription preserves original spellings, abbreviations and letterforms, aiming to capture the text as it appears in the manuscript. A semi-diplomatic transcription may expand some abbreviations or regularise spelling for readability, whereas a normalised transcription modernises the text to align with contemporary orthographic standards. Abbreviations are expanded, non-standard characters are replaced with modern equivalents, and spacing, punctuation and capitalisation may be adjusted. While this makes the material more accessible for modern readers and analysis, it may strip away useful palaeographic information.\nAbbreviations themselves pose a specific challenge. Latin scribes used a wide array of abbreviation marks to save time and space, and these are not always easy for an HTR model to detect or interpret. Whether a model expands or retains them often depends on the transcription style it was trained on.\nTaken together, the variability in scripts, transcription practices and scribal habits means that a ‚Äúone-size-fits-all‚Äù approach to HTR rarely works for Latin manuscripts. Instead, models need to be adapted to the specific characteristics of the material - for particular writing styles, time periods or even individual scribes. Though such fine-tuning of models is a time-consuming process, it is necessary to produce meaningful and accurate automatic transcriptions. Tools and platforms like eScriptorium, Transkribus, and Hugging Face now offer increasingly accessible ways to train and customize HTR models to meet these challenges."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#different-htr-frameworks",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#different-htr-frameworks",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Different HTR frameworks",
    "text": "Different HTR frameworks\n\nTranskribus\nTranskribus is a platform maintained by READ-COOP SCE (Recognition and Enrichment of Archival Documents ‚Äì Cooperative Society), a European cooperative focused on advancing research and innovation in the digital humanities. The cooperative brings together a broad community of academic institutions, cultural heritage organisations, archives and individual researchers, with the goal of making historical documents more accessible and understandable through digital tools.\nThe platform offers tools for transcription, annotation and analysis of historical documents. One of its core features is support for HTR, enabling users to train and apply models tailored to specific handwriting styles in order to automate transcription.\nTranskribus operates on a credit-based system: users receive a number of free credits, and larger or more complex projects can be supported through a range of paid plans.\nThe platform relies on PyLaia, a recognition engine developed by the Universitat Polit√®cnica de Val√®ncia. Currently, Transkribus hosts more than 200 publicly available models, which can be used directly or fine-tuned to new handwriting styles or collections. However, it‚Äôs also worth noting that models trained within Transkribus are designed to work exclusively within the platform.\n\n\neScriptorium and Kraken\nLike Transkribus, eScriptorium is a platform designed to manage transcription workflows. It supports both manual and automated processes for annotation, segmentation and model training. The software was developed as part of the Scripta project at the √âcole Pratique des Hautes √âtudes, Universit√© Paris Sciences et Lettres (EPHE‚ÄìPSL).\neScriptorium is open source and can be installed locally. For light tasks such as annotation, segmentation or transcription, no specialised hardware is needed - standard consumer-grade computers are sufficient. Model training can also be done on CPUs, though the process is significantly faster when using GPUs, especially for large datasets or multi-user environments.\nThe underlying recognition engine in eScriptorium is the OCR software Kraken. Kraken uses deep learning, specifically recurrent neural networks (RNNs) with connectionist temporal classification (CTC), to recognise text in images. It offers flexible layout analysis and supports training on custom datasets, making it well suited for historical documents and complex scripts.\nPretrained models compatible with eScriptorium are often shared through repositories like Zenodo, typically alongside the training data used to create them. These resources - produced by researchers and institutions as part of larger transcription projects - can serve as helpful starting points for similar materials. Using such models can significantly reduce the time and effort required for training, particularly when working with rare or difficult scripts.\nOnce uploaded to eScriptorium, models can be used directly for transcription or fine-tuned on new data.\n\n\nAdvanced HTR training options\nPlatforms such as Transkribus and eScriptorium provide graphical user interfaces (GUIs) that lower the technical barrier, allowing users to train Kraken or PyLaia models without writing code. But for those who need greater control over the training process - hyperparameters, preprocessing pipelines or custom architectures - both Kraken and PyLaia can be run independently of these platforms. However, PyLaia models trained within Transkribus cannot currently be exported, so external training requires either building your own model from scratch or fine-tuning one from outside that ecosystem.\nFor an approach that works directly with Python packages, you can also leverage Microsoft‚Äôs transformer-based TrOCR models via the Hugging Face Transformers library. These models come pre-trained on a mix of printed and handwritten datasets and can be fine-tuned on your own Latin manuscript images to boost performance on complex, historical scripts. TrOCR‚Äôs transformer-based architecture allows them to handle complex input more robustly than some more traditional OCR approaches (Li et al. 2021).\nBecause most line-level HTR models assume correctly segmented inputs, they need to be paired with a region-detection step. Object-detection models like YOLO can locate and crop text regions or individual lines in a page image. Once each line has been detected and isolated, any of the above HTR models - whether trained via a GUI platform or directly in Python - can be applied to produce the transcription."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#testing-escriptorium",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#testing-escriptorium",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Testing eScriptorium",
    "text": "Testing eScriptorium\nHaving provided an overview of available HTR tools, we now turn to how these models perform in practice when applied to KB‚Äôs Latin manuscripts. In the remainder of this post, we discuss our experiences using eScriptorium, Transkribus and TrOCR‚Äôs models for automatic transcription.\n\nSegmentation\nBefore comparing the models themselves, let‚Äôs first examine the segmentation process. For our experiments, we used Kraken‚Äôs default segmentation model, available on Zenodo. This model predicts a single region class and is designed to work well on most non-fragmentary handwritten and machine-printed documents with moderate layout complexity. (It can also serve as a good starting point for fine-tuning when necessary.)\nWe applied the model to two single-column pages and three double-column pages. The images below show how the segmentation output highlights detected regions (in violet) and text lines (in blue). The yellow circles indicate the proposed reading order, which determines the sequence in which the transcribed lines will be arranged.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure¬†2: Outputs from Kraken‚Äôs segmentation model, when applied to A 69, f.¬†22r (left) and A 32, f.¬†2r (right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure¬†3: Outputs from Kraken‚Äôs segmentation model, when applied to A 68, f.¬†207v (left) and A 70 f.¬†1r (right).\n\n\n\n\n\nSingle-column manuscript pages\nOn the single-column pages, the segmentation model performed reasonably well. Almost all lines of text were correctly enclosed within a single region, with no major omissions (see Figure¬†2 (a) and Figure¬†2 (b)). This suggests good generalisation for straightforward layouts. In one case, however, the model mistakenly recognized regions on the previous page, which will later affect the reading order.\n\n\nReading order and line association\nAn important feature of Kraken‚Äôs layout analysis is the way it organizes regions and lines. While reading order typically proceeds from top to bottom, indentations or more complex layouts - such as double columns - can lead to unexpected results. A line can either be associated with a region or classified as an orphan. Regions themselves can contain multiple lines, a single line, or none at all. When lines are associated with a region, their reading order is calculated within that region, not across the entire page.\nThis makes segmentation a critical first step for producing a coherent transcription. If regions are too broad or incorrectly identified, the resulting reading order will become confused.\nDouble-column layouts illustrate this well. In Figure¬†3 (a), the model correctly identifies three distinct regions, assigning them a logical reading order. But in Figure¬†3 (b), the model encloses the entire page in a single region, ignoring the column structure. As a result, the lines are read left to right across the page, rather than top to bottom within each column - producing a disordered output.\nIn addition to problems with reading order, some lines are not properly segmented: a few are merged across both columns, while others are omitted altogether. These examples highlight the limitations of the default segmentation model when applied to complex or irregular layouts, and suggest that fine-tuning may be necessary for improved performance.\n\n\nTranscription\nOnce segmentation is complete, we can proceed with transcribing the documents. We tested several models available on Zenodo, each trained on different Latin manuscript datasets. The two best-performing models were TRIDIS and CATMuS, which also exemplify two different approaches to transcription and annotation.\nTRIDIS (Tria Digita Scribunt) is a multilingual HTR model trained on semi-diplomatic transcriptions from medieval and early modern documentary manuscripts. This means it retains much of the original spelling and character forms, while also expanding some common abbreviations and smoothing out inconsistencies to improve readability and support downstream analysis. TRIDIS is especially suited to legal, administrative and memorial documents from the 13th to 16th centuries, but it has also been shown to perform well on other genres, including literary texts, cartularies and scholarly treatises.\nIn contrast, CATMuS Medieval (Consistent Approach to Transcribing ManuScript) takes a stricter approach. It is trained on graphematic transcriptions that reproduce each character exactly as it appears in the manuscript - without expanding abbreviations - and represent diacritics, superscripts and ligatures separately using NFD Unicode normalization. CATMuS supports multiple languages, including Latin, Old and Middle French, Spanish and Italian, and reflects a consistent transcription standard developed across several collaborative projects, such as CREMMA, HTRomance and GalliCorpora.\nOne thing both models have in common is the scale and breadth of their training data. Compared to the other models we tested, TRIDIS and CATMuS were trained on significantly larger datasets: around 560,000 lines of Latin text for TRIDIS and 160,000 for CATMuS. By contrast, a third model we tested was trained on approximately 46,000 lines, of which fewer than 10,000 were in Latin. As expected, its performance was noticeably weaker. This highlights the importance of dataset size in enabling better HTR performance.\n\n\nResults of HTR with Kraken Models in eScriptorium\nAs noted above, off-the-shelf HTR models tend to perform best on material similar to their training data. Since we have not yet fine-tuned any models on KB‚Äôs manuscripts, perfect results are not to be expected at this stage. Instead, our aim is to survey the current capabilities of available Latin HTR models and determine which might be best suited for fine-tuning to better match the characteristics of KB‚Äôs collections.\n\n\n\n\n\n\nFigure¬†4: Recognized regions and text lines on a manuscript page in eScriptorium. Right: a view showing the generated text overlaid on the original layout for easier comparison.\n\n\n\n\n\n\n\n\n\nFigure¬†5: Comparing model outputs within eScriptorium\n\n\n\nThe user interface of eScriptorium makes it simple to compare the outputs of different HTR models. As Figure¬†5 shows, the GUI displays a manuscript excerpt alongside a manual line-by-line transcription and the results from various models (e.g., catmus_1.6.0 and tridis_v2). Insertions are highlighted in green, deletions in red. In our experiments, all manual transcriptions were done line by line, with hyphens marking words split across lines. Abbreviations were expanded - though without special notation - and the original orthography was preserved throughout.\nWe used this interface to compare transcriptions produced by CATMuS and TRIDIS, and we present a series of examples from that comparison below.\n\n\n\n\n\n\nFigure¬†6: Comparison of transcriptions from the CATMuS and TRIDIS models.\n\n\n\nVarious factors need to be considered when assessing these outputs. Because each model follows its own transcription conventions, red and green highlights don‚Äôt always signal true errors. CATMuS, for instance, emits graphematic output as opposed to expanding abbreviations - so in the example above ‚Äúxpi‚Äù remains unchanged rather than expanded to ‚Äúchristi‚Äù (see Figure¬†6), and diacritic-driven omissions (e.g.¬†‚óåÃÉ for an omitted ‚Äúm‚Äù) are left in their original form. Likewise, differences in capitalization or in rendering ‚Äúi‚Äù as ‚Äúj‚Äù and ‚Äúu‚Äù as ‚Äúv‚Äù reflect annotation choices rather than model failure.\n\n\n\n\n\n\nFigure¬†7: Transcription of a manuscript excerpt with the text ‚ÄúPrologus‚Äù.\n\n\n\n\n\n\n\n\n\nFigure¬†8: Transcription of a manuscript excerpt with the text ‚ÄúIncipit prologus in libros celestium reue-‚Äù.\n\n\n\nSegmentation errors can further complicate evaluation. In Figure¬†7 and Figure¬†8 above, the words ‚ÄúPrologus‚Äù and ‚ÄúIncipit‚Äù lie partially outside the detected text region. Because the model never ‚Äúsees‚Äù the full word image, it predictably fails to transcribe it correctly - underscoring how even the best HTR engine depends on accurate region detection.\n\n\n\n\n\n\nFigure¬†9: Transcription of a manuscript excerpt with the text ‚Äúintytulatur ex eo quod processus eius.\n\n\n\nLet‚Äôs consider a few more examples. Across several excerpts, we observed various model-specific quirks:\n\nThe LATIN word ‚Äúquod‚Äù (‚Äòthat‚Äô) was transcribed by CATMuS as the abbreviation ‚ÄúÍùô,‚Äù while TRIDIS misread the same sign as ‚ÄúÍùó‚Äù and expanded it to ‚Äúquam‚Äù (‚Äòhow‚Äô) (see Figure¬†9).\nThe symbol ‚ÄúÍù∞‚Äù intended as ‚Äúus‚Äù was rendered simply as ‚Äúo‚Äù in TRIDIS‚Äô expanded transcription, whereas CATMuS correctly preserved the meaning of both ‚Äúprocessus‚Äù and ‚Äúeius‚Äù.\nIn Figure¬†10 below, the scribal abbreviation for ‚Äúest‚Äù (‚Äòis‚Äô), ·∫Ω, was correctly recognized by CATMuS but not TRIDIS. However, neither model detected the ‚Äúper‚Äù abbreviation, pÃ±, leading to mistaken readings of ‚ÄúÍùì‚Äù or plain ‚Äúp.‚Äù\n\n\n\n\n\n\n\nFigure¬†10: Transcription of a manuscript excerpt with the text ‚Äúest per modum questionum ad quas‚Äù.\n\n\n\nOther errors - such as concatenated words or confusion between similar strings like ‚Äúquam‚Äù vs.¬†‚Äúquod‚Äù - further illustrate the gap between generic models and our specific manuscripts. Yet these results also point to a clear path forward: fine-tuning on annotated data from the same scribal hands should markedly improve accuracy. In sum, while base Kraken models offer a useful preview of HTR performance on KB‚Äôs Latin holdings, sustained fine-tuning is likely essential for reliable, high-quality transcriptions."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#testing-transkribus",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#testing-transkribus",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Testing Transkribus",
    "text": "Testing Transkribus\nNow, let us turn to our explorations with Transkribus. As in eScriptorium, the HTR workflow in Transkribus involves two main steps: detecting text regions and lines, followed by transcription. Segmentation can be carried out as a separate step or combined with transcription in a single process. By default, Transkribus uses a pre-trained segmentation model, though users can also train custom models tailored to their specific material. The recommended dataset size for training a new model is approximately 50 pages.\nWe tested the default segmentation mode in Transkribus and encountered segmentation challenges similar to those observed in eScriptorium. In particular, the default segmentation model performs best on single-column layouts. In the example shown below (see Figure¬†11), it incorrectly interpreted both columns of a page as a single region. As a result, the reading order became distorted: instead of reading the columns top-to-bottom, the engine processed the text line-by-line across the full page width from left to right. This produces transcriptions that are jumbled and difficult to interpret.\n\n\n\n\n\n\nFigure¬†11: Regions are shown in green and lines in blue, numbered from 1 within each region, A 70, f.¬†1r. (Beginning of Book 4 of St Birgitta‚Äôs Revelations. Top and bottom margin: later owner‚Äôs inscriptions (The Carthusians in Buxheim).\n\n\n\n\n\n\n\n\n\nFigure¬†12: As in eScriptorium, the model occasionally misinterprets decorative elements as text lines. A 32, f.¬†2r.\n\n\n\nHowever, unlike the segmentation model in eScriptorium, the Transkribus model appears to handle two-column layouts more effectively - provided no additional text elements, such as titles, are present.\n\n\n\n\n\n\nFigure¬†13: The results of segmentation of a multi-column page.\n\n\n\n\n\n\n\n\n\nFigure¬†14: The results of segmentation of a multi-column page with additional text.\n\n\n\nTranskribus also allows users to adjust parameters for layout analysis. Despite experimenting with these settings, we were unable to achieve the correct number of segments on a manuscript page that included a title. This suggests that more robust performance on complex layouts is likely only achievable by training a custom model tailored to the specific data. In our tests, we chose to force the model to detect ‚Äúmultiple‚Äù segments - see Figure¬†15 for the results.\n\n\n\n\n\n\nFigure¬†15: Detecting multiple segments.\n\n\n\nNow let us turn to our experiments with the transcription models in Transkribus. As in our tests with eScriptorium, we evaluated several HTR models to assess how well they perform on Latin manuscripts. One of the best-performing models was TrHtr Titan I bis, released in April. This was trained on a large, balanced dataset that includes both printed and handwritten documents, covering historical as well as modern sources. The platform classifies it as a ‚Äúsuper model,‚Äù meaning that access requires a subscription.\nTo explore a free alternative, we also tested a pyLaia-based model available in Transkribus: Medieval_Scripts_M2.4. Trained on 24,764 pages, this model was developed as a general-purpose HTR solution for medieval Latin scripts.\nIn terms of interface, Transkribus offers functionalities similar to those of eScriptorium, but it lacks some useful features. For instance, it does not currently support the direct comparison of more than two transcriptions, nor does it highlight the differences between them. Another helpful feature present in eScriptorium but missing in Transkribus is the close view option; in Transkribus, the only way to inspect individual lines is to manually zoom in on the highlighted text.\n\n\n\n\n\n\nFigure¬†16: Transkribus interface showing the transcription editor, with the corresponding line on the manuscript page highlighted during editing.\n\n\n\nTurning to the transcriptions themselves, we observed that the Titan I bis model handles abbreviation expansion inconsistently - likely reflecting the diversity of its training data. In one example (see Figure¬†17), the model successfully expands common abbreviations such as et and us, demonstrating its capacity to interpret these forms correctly. However, in another case (see Figure¬†18), the same abbreviation signs - such as Íù∞ (us) and Íù≠ (-us) - are left unexpanded, along with several others. This inconsistency suggests that the model does not apply a uniform rule for abbreviation expansion across different documents or contexts, which may require additional manual correction during post-processing.\n\n\n\n\n\n\nFigure¬†17: Titan I bis‚Äôs transcription with expanded abbreviations: ‚Äúipsa et confessores eius sepe oretenus testabantur. Nam semel conti‚Äù\n\n\n\n\n\n\n\n\n\nFigure¬†18: Titan I bis‚Äôs transcription without expanded abbreviations: ‚ÄúplogÍù∞ lib qÃÑstoÃÑm qÕ£ eÃÑ q·∂¶ntÍù≠ lib* celestium reuelacionum bteÃÑ BÀÄgitte‚Äù\n\n\n\nAnother notable feature of the Titan I bis model is its attempt to transcribe paragraph signs (¬∂) to reflect the original structure of the text. However, the model is not always consistent: it occasionally inserts the sign correctly, but at other times omits it or places it incorrectly.\n\n\n\n\n\n\nFigure¬†19: Correctly recognized paragraph sign.\n\n\n\n\n\n\n\n\n\nFigure¬†20: Incorrectly recognized paragraph sign.\n\n\n\nThis tendency can also result in transcriptions that include unexpected characters, such as &lt; or |, which are not present in the original manuscript.\n\n\n\n\n\n\nFigure¬†21: Example of a transcription containing unexpected characters such as &lt; and |.\n\n\n\nLike the Titan I bis model, M4.2 also shows inconsistency in handling abbreviations. For example, the abbreviation Íù∞ (used for us) is expanded in the word revelatus, but left unchanged in auitÍù∞, despite being the same abbreviation.\n\n\n\n\n\n\nFigure¬†22: Example of a transcription showing the abbreviation ‚ÄúÍù∞‚Äù left unexpanded.\n\n\n\n\n\n\n\n\n\nFigure¬†23: Example of a transcription where the abbreviation ‚ÄúÍù∞‚Äù is correctly expanded.\n\n\n\nIn other cases, the model fails to expand the abbreviation entirely and sometimes omits diacritics as well (see Figure¬†24). These inconsistencies can make it difficult to produce clean, uniform transcriptions - particularly when working with larger datasets or aiming to support full-text search across the material.\n\n\n\n\n\n\nFigure¬†24: Example of a transcription where the abbreviation ‚Äúdne‚Äù is neither expanded nor correctly marked with diacritics. The accurate form should be dnÃÑe or domine.\n\n\n\nThe challenges we encountered highlight how off-the-shelf models often fall short when applied to specialised manuscript collections with complex layouts and distinctive scribal practices. These limitations underscore the value of developing tailored models to improve transcription quality."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#testing-trocr",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#testing-trocr",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Testing TrOCR",
    "text": "Testing TrOCR\nWe also explored a third approach using transformer-based models outside the two major platforms. The TRIDIS model is available in a version built with Microsoft‚Äôs TrOCR framework, a transformer-based OCR system. This alternative, trained on the same dataset as the Kraken version, combines a Vision Transformer (ViT) encoder with a decoder based on a medieval Latin RoBERTa language model. According to reported evaluations, the TrOCR-based model is expected to slightly outperform its Kraken-based counterpart.\nTo use the TRIDIS TrOCR model for transcription, we first segmented the manuscript pages using the BigLAM YOLO model. Trained on the CATMuS Medieval Segmentation dataset, this model can distinguish between 18 different layout elements - such as heading lines, standard text lines, marginalia and numerical annotations.\nAt present, there is no graphical interface that fully integrates YOLO-based segmentation with TrOCR-based transcription. However, for those interested in exploring how the BigLAM model performs, a demo is available on Hugging Face.\n\n\n\n\n\n\nFigure¬†25: Example of YOLO segmentation output. In addition to regular text lines, the model labels other layout elements with distinct classes and assigns confidence scores to each detection, aiding in the filtering of regions unlikely to contain text.\n\n\n\nLike the other segmentation methods we tested, YOLO produces outputs consisting of regions and the elements contained within them. Unlike the models in eScriptorium and Transkribus - which recognize regions and text lines - YOLO also provides element-level classification, identifying components such as lines, marginalia or initials. In Transkribus, achieving this level of detail requires training a separate ‚ÄúField‚Äù model. Understanding the structural layout of a document is crucial, as it allows the detected lines to be organized into coherent text before the HTR step.\nAs noted, there is currently no interface that directly integrates TrOCR and YOLO models. As a result, users must programmatically extract lines from the manuscript pages, determine the correct reading order, and then run recognition on the extracted line images.\nIn our tests, we performed only steps 1 and 3 - line detection and text recognition - without automatically correcting the reading order. However, YOLO‚Äôs ability to label different parts of a page offers a valuable foundation for automating the reading order in future workflows.\nThe HTR model we tested is available on Hugging Face under the name: magistermilitum/tridis_v2_HTR_historical_manuscript. Compared to the Kraken model trained on the same dataset, the TrOCR version successfully corrected many transcription errors that Kraken failed to resolve. For example:\n\n\n\n\n\n\n\n\nExample\nSource\nTranscription\n\n\n\n\n1\nManual\n-cionem peccatorum de medio ignis zeli dei\n\n\n\nKraken\ncionem pecctorum de Medioignis zeli Dei\n\n\n\nTrOCR\ncionem peccatorum de medioignis zeli dei\n\n\n2\nManual\nLiber quintus\n\n\n\nKraken\nLiber quantus\n\n\n\nTrOCR\nliber quintus\n\n\n3\nManual\nde regno Swecie Qui liber questionum merito intulatur\n\n\n\nKraken\nde regno Girecie Rui liber questionim meito intytulatur\n\n\n\nTrOCR\nde regno Swecie Qui liber questionum merito intulatur\n\n\n4\nManual\nLiber quintus\n\n\n\nKraken\nRiber qntus\n\n\n\nTrOCR\nLiber q≈øtionu\n\n\n5\nManual\nLiber quintus\n\n\n\nKraken\nRiber qntus\n\n\n\nTrOCR\nLiber q≈øtionu\n\n\n\n\nIn some instances, TrOCR improved the transcription but did not fully correct the text:\n\n\n\n\n\n\n\nSource\nText\n\n\n\n\nManual\n-bantur Nam semel contigit quod\n\n\nKraken\nbautur Fulam semel contigit quom\n\n\nTrOCR\nvantur suam semel contigit quod\n\n\n\n\nIn a few cases, TrOCR introduced completely incorrect words. For example, it transcribed ‚Äúvidelicet‚Äù instead of ‚Äúsed‚Äù, which Kraken had correctly recognized:\n\n\n\n\n\n\n\nSource\nText\n\n\n\n\nManual\nvocem audiret Sed stupendius est quod\n\n\nKraken\nvotem audiret / sed scupendius est quod\n\n\nTrOCR\nvocem audiret / videlicet stipendius est / quod\n\n\n\n\nAs with other frameworks, the quality of segmentation has a major impact on HTR performance. When the manuscript text is relatively straight, YOLO‚Äôs output for a line typically includes only that line, with minimal noise (see Figure¬†26). In skewed documents, however, the detected line may accidentally include fragments from the lines above or below, as illustrated in Figure¬†27.\n\n\n\n\n\n\nFigure¬†26: Example of a YOLO output from a manuscript with a simple layout.\n\n\n\nTo reduce the impact of this added noise, an additional preprocessing step can be introduced before text recognition. For testing purposes, we used a model available on Hugging Face: Riksarkivet/yolov9-lines-within-regions-1. (Note: this model was trained on a different dataset and is included here solely for experimental purposes.)\nFigure¬†27 shows the ‚ÄúDefaultLine‚Äù region recognized by the medieval-manuscript-yolov11 model, while Figure¬†28 presents the same image cropped to remove surrounding noise. Although the model was not specifically trained on this type of manuscript, it performed adequately for testing purposes.\n\n\n\n\n\n\nFigure¬†27: ‚ÄúDefaultLine‚Äù region recognized by the medieval-manuscript-yolov11 model.\n\n\n\n\n\n\n\n\n\nFigure¬†28: The cropped image after removing additional noise. The model was not trained to recognize text on this type of documents, but it worked well enough for our testing purposes.\n\n\n\nComparison after masking:\n\nBefore (TrOCR): vocem audiret / videlicet stipendius est / quod\nAfter (cleaned TrOCR): yocem audiret / sed stipendius est quod\nGold: vocem audiret Sed stupendius est quod\n\nMasking redundant text helped clean up the transcription and corrected the previously mistranscribed word ‚Äúvidelicet‚Äù to the correct ‚Äúsed.‚Äù However, it also introduced a new error in the first word, which had previously been transcribed correctly. This highlights the importance of experimenting with different cropping and masking techniques, as well as the critical role of image preparation in the HTR pipeline.\nA valuable tool for managing this HTR pipeline is the HTRFlow Python package, developed by our colleagues at the National Archives of Sweden‚Äôs AI lab. HTRFlow simplifies the customization and management of HTR workflows by using a configuration file in which each step - such as segmentation or recognition - is defined as a modular component. This design allows users to flexibly adapt and experiment with different models or processing steps, without needing to rewrite code for every adjustment."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#comparing-outputs-kraken-pylaia-or-trocr",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#comparing-outputs-kraken-pylaia-or-trocr",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Comparing outputs: Kraken, pyLaia or TrOCR?",
    "text": "Comparing outputs: Kraken, pyLaia or TrOCR?\nThe performance of the HTR models we tested reflects the differences in their training data, transcription conventions and underlying architectures:\n\nTRIDIS TrOCR consistently delivered the most complete and accurate transcriptions. It handled abbreviations - such as ‚ÄúxpÃÑi‚Äù - by expanding them correctly to ‚ÄúChristi,‚Äù and even resolved more complex abbreviations into their full forms with remarkable consistency.\nKraken-based TRIDIS produced results similar to the TrOCR version but was more prone to occasional character-level errors, such as misrecognizing single letters or ligatures.\nKraken CATMuS faithfully preserved original abbreviations and special characters, making it suitable for researchers interested in palaeographic detail and scribal practices. However, its literal output may require additional editorial interpretation for those less familiar with medieval Latin conventions.\nTranskribus Titan I bis generally yielded readable, standardized text and attempted to encode layout features (e.g., paragraph marks or vertical bars). Although its transcriptions were often clear, the model sometimes introduced incorrect expansions or misreadings, and its layout markers were not always reliable.\nMedieval_Scripts_M2.4 (pyLaia) was the least reliable of the tested models, particularly in segmentation. It struggled with accurate line breaks and produced more errors compared to the other tools.\n\nAll models exhibited occasional spelling inaccuracies, spacing errors, misreadings and inconsistent abbreviation expansions. Overall, the TrOCR version of TRIDIS offered the best balance between accuracy and normalization - simplifying complex elements without introducing excessive distortions - which makes it a strong candidate for further fine-tuning in the future. The Kraken CATMuS model provides a closer visual match to the original manuscript, preserving intricate glyphs and diacritics, which may be especially valuable for manuscript-focused research. The Transkribus models tended to simplify special characters more aggressively, which improved readability but sometimes flattened palaeographic nuance and distorted visual elements.\nIn the appendix below you can observe exemplary outputs from the four models alongside the manually curated ‚Äúgold standard‚Äù transcription for direct comparison."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#next-steps-and-future-work",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#next-steps-and-future-work",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Next steps and future work",
    "text": "Next steps and future work\nOur experiments yield promising results, particularly with models like TRIDIS TrOCR, which strike a good balance between legibility and historical accuracy. However, variability in script styles, layouts and editorial conventions remains a significant challenge.\nMoving forward, we plan to explore the following options:\n\nFine-tune models on representative samples from KB‚Äôs collections.\nDevelop annotated datasets featuring both diplomatic and semi-diplomatic transcriptions.\nInvestigate hybrid workflows that combine HTR with expert human validation.\nExplore user-friendly tools for viewing, comparing and correcting transcriptions.\n\nBy continuing this work - and sharing both our successes and setbacks - we hope to contribute meaningfully to digital manuscript studies and improve access to KB‚Äôs medieval heritage collections. We welcome feedback from other researchers and institutions working with medieval Latin manuscripts and HTR. If you‚Äôre interested in collaboration, don‚Äôt hesitate to get in touch at kblabb@kb.se."
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#appendix-htr-model-results-with-manual-benchmark",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#appendix-htr-model-results-with-manual-benchmark",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "Appendix: HTR Model Results with Manual Benchmark",
    "text": "Appendix: HTR Model Results with Manual Benchmark\n\n\nTranscriptions of A 66\n\n\n\n\n\n\n\n\n\n\nTridis_v2\ntridis_v2_tr_ocr\nCatmus-medival-1.6.0\nTitan I bis\nm4.2\nManual (Gold)\n\n\n\n\nliber primus\nliber primus\nLiber primus\nLiberprimus\nLiber primus\nLiber primus\n\n\ntupor et muralia judita sunct in tram\nStupor et miralia vidita sunt in terra\ntupor et miralia uidita stÃæ iÃÉ tra\ntu por et miralia iudita sunt in tra\nStupor et miraliauidita ≈øti tra\nStupor et mirabilia audita sunt in terra\n\n\nnostra mirabile si quidem erat quod ze\nnostra mirabile siquidem erat quod ze\nnrÃÉa Mirabile si quidÃÉ erat Íùô ze\nnostra mirabile si quidem erat quod ze\nnostra mirabile siquidem erat quod ze\nnostra mirabile siquidem erat quod ze-\n\n\nlator legis morses igneam in vl\nlator legis morses ignea in vl\nlator legis moyses igneaÃÉ in ul\nlator legis moy≈øes igneƒÅ in vl\nDlator legis moyses ignea in vl\nlator legis moyses igneam in vl-\n\n\ncionem pecctorum de Medioignis zeli Dei\ncionem peccatorum de medioignis zeli dei\ntioÃÉnem pccÃÉoÍùµ de medro ignis zeli dei\ntionem pctoro de mediovinis zeli dei\ncionem patorum de medioigus zeli dei\ncionem peccatorum de medio ignis zeli dei\n\n\nvotem audiret / sed scupendius est quod\nvocem audiret / videlicet stipendius est / quod\nuocem audiret. S scupendiis est qd\n|| vocem audiret | Dz ¬ßtuyenerins e≈øt | quod\nvocem audiret E í ≈øtuprudius e≈øt quod\nvocem audiret Sed stupendius est quod\n\n\nhuiles hodie et mansueti spiritu vocem\nhumles hodie et mansueti spiritu vocem\nhiunles hodie et mansueti spuÃÉ uocem\nhuiusles hodie et mansueti spiritum vocem\nhunles hodie et mansueti spum vocem\nhumiles hodie et mansueti spiritu vocem\n\n\nibu Christi Dei et homum aveuit ut olim\nJhesu Christi dei et hominum audint ut olim\nihuÃÉ xpÃÉi dei et homu audiÃÉt ut olim\nSi huÃÑ xpi dei et homi auduit ut olim\nhu xxi dei et homi auduit ut olim\niesu christi dei et hominis audiunt vt olim\n\n\n\n\n\n\nTranscriptions of A 70\n\n\n\n\n\n\n\n\n\n\nTridis_v2\ntridis_v2_tr_ocr\nCatmus-medival-1.6.0\nTitan I bis\nm4.2\nManual (Gold)\n\n\n\n\nIste liber est Carthuser in Buchshein prope Meningen\nIste liber est Carthuses in Bouchshem prope Meningen\nIste liber est Carthus in Buchshem Íùìpe memugen\nI≈øte liber e≈øt CarthuÕ§√ü in Buch≈øheim ≈øpe wenigen\nIste liber est Carthusz in Buchshein prpe Meningen\nIste liber est Carthusiensis in Buchshem prope memingen\n\n\nliber quantus\nliber quintus\nRiber qntus\nTiberg‚Äôntus\nRiber qntus\nLiber quintus\n\n\nncipit quantus liber te\nIncipit quintus liber te\nncipit qntus liber ce\nincipit (qÕ•ntus liber te\nncipit quntus liber te\nIncipit quintus liber ce-\n\n\nlestis revelacionum Christi\nLestis revelacionum Christi\nlestis reuelacoÃÉnuÃÉ xpi\n¬∂ le≈øtis reuelatoÃÑnum xpÃÖi\nClestis revelacionum xxi\nlestis reuelacionum christi\n\n\nadbeatam Byigictam drequo\nad beatam Bycam dregno.\nadbeatam Bycgittam dregno.\nabbeatam bydgickam dÃæ regno\nadbeatam Byigittam dregno\nad beatam Byrgittam de regno\n\n\nsiretie qui liber questionum meico\nsroetie qui liber questionum merito\nsiretie qui liber qÃÉstionuÃÉ melco\n≈øwetie qui ≈øiber qÃÑ≈øtionu meito\nswetie am liber q≈øtionu meico\nswecie qui liber questionum merito\n\n\nintyntulatur exeo quam processo eius\nintyculatur ex eo quod processo ejus\nintyculatur exeo Íùô ÍùìcessÍù∞ eiÍù∞\nintyculatur exeo quod processo eius\nintyculatur exeo quod processo eius\nintytulatur ex eo quod processus eius\n\n\net prmodum questianum adquas\nest postmodum questionum adquas\neÃÉ Íùìmodum qÃÉstianuÃÉ adquas\nest primodum quaestionum adquas\npermodum questionum ut infra sequi\nest per modum questionum ad quas\n\n\nxhrist dominus dac nirabiles soluto\nsept dominus dati mirabiles soluto\nxpÃÉt dnÃÉs dac mirabiles solutoÃÉ\nXpt dominus dach mirabiles ≈øoluto\nxpc dominus dac mirabiles soluto\nchristus dominus dat mirabiles solucio-\n\n\nnes et revelatus fuit eidem\nnes et revelatus fuit eidem\nnes ¬∂Et reuelatus fuit eidem\nnes ¬∂ Et reuelatus fuit eidem\nnes set revelatus fuit eidem\nnes Et reuelatus fuit eidem\n\n\ndomine miro oson sicut ipsa et con\ndomine miro nostro sicut ipsa et con\ndnÃÉe miro moÃÉ situt ipsa et coÃÉ\ndnÃÑe miro moÃÑ ≈øitur ipÃÑa et to\ndne miro mo situc ipsa et co-\ndomine miro modo sicut ipsa et con-\n\n\nfessores ejus sepe oretenu resta\nfessores ejus sepe oretenus resta\nfessores eiÍù∞ sepe aretonÍù∞ resta\nfe≈ø≈øores eius ≈øepe oretem te≈øta\nfe≈ø≈øores ei ≈øepe cretens re≈øta\nfessores eius sepe oretenus testa-\n\n\nbautur Fulam semel contigit quom\nvantur suam semel contigit quod\nbantur Iam semel contigit Íùô\nbantur Nam semel contigit Íùô\nvantur Nam ≈øemel contigit q\n-bantur Nam semel contigit quod\n\n\n\n\n\n\nTranscriptions of A 32\n\n\n\n\n\n\n\n\n\n\nTridis_v2\ntridis_v2_tr_ocr\nCatmus-medival-1.6.0\nTitan I bis\nm4.2\nManual (Gold)\n\n\n\n\nEloga libero quesato que e quenta libe celestin reuela conun bete vegit\nPloga libras questionem, qui cum quintalibus celestium revelacionum beate Berengitte\nplogÍù∞ libÃæ qÃÉsto Õ´Õ´ q eÃÉ qntÍù∞lib celestiuÃÉ reuelacoÃÉnuÃÉ bte Bgitte\nplogÍù∞ lib qÃÑstoÃÑm qÕ£ eÃÑ q·∂¶ntÍù≠ lib* celestium reuelacionum bteÃÑ BÀÄgitte\nNots lib qstom q qutÍù∞ ib clestuÃÑ reuvelacdoinui bte rbgitt\nprologus libri questionum qui est quintus liber celestium revelacionum beate Birgitte\n\n\ncipit vitus liber celestium revelacionum Christi ad batam Botam\nPrecipit adecuitus liber celestivi revelacionum Christi ad vestram bergertum\nAcipit aduÃÉitÍù∞ liber celestiuÃÉ reuelacoÃÉnuÃÉ xpÃÉi ad btÃÉa EgtaÃÉ\nMcipit a QuiÃÉtÍù∞ liber cele≈øtuiÃÉ reue la coÃÉnuÃÉ xpÃÑi ad btÃÑaz SgÕ£taÃÑ\nbcipit auitÍù∞ liber celestiuÃÑ revelaconuÃÑ xpÃÑi ad utÃÑ∆∫ sgta\nINcipit Quintus liber celestium reuelacionum christi ad beatam Birgittam\n\n\nde regno Girecie Rui liber questionim meito intytulatur\nde regno Swecie Qui liber questionum merito intulatur\nde regno Groecie Qui liber questionuÃÉ meito iÃÉtytulatur\nde regno swecie Qui liber questionuÃÑ meito iÃÑtytula tur\nde regno siecie Qui liber questionum merito intytilaturs.\nde regno Swecie Qui liber questionum merito intytulatur\n\n\nexra eo quod processus eius est per modum questionum ad quas x dominus dat nostri et\nex eo quod processus eius est per modum questionum ad quas xl. dominus dat iii et\nex eo Íùô ÍùìcessÍù∞ eiÍù∞ eÃÉ Íùë moduÃÉ qÃÉstionuÃÉ ad qÕ£s x dnÃÉs dat miÃÉr\nex eo quod processus eius est per modum questionum ad quas Cristus dominus datum im\nex eo quo processus eius en per modum questionum ad quas e domins dat mi\nex eo quod processus eius est per modum questionum ad quas christus dominus dat mira-\n\n\nbiles soluciones . Et revelatus fuit eidem domine miro modo sicud\nviles soluciones / Et revelatus fuit eidem domine miro modo situd\nbiles solucoÃÉes. Et reuelatÍù∞ fuit eideÃÉ dnÃÉe miro modo Sicud\nbiles solucciones Et reuelatus fuit eidem domine miro modo Sicud\nbiles soluciones Et revelatus fuit eidem domine miro modo Sicud\n-biles soluciones Et reuelatus fuit eidem domine miro modo Sicud\n\n\nipsa et confessores eius sepe oretenus testabantur Ma semel conti\nipsa et confessores eius sepe oretenus testabantur / Wansemel\nipÃÉa ‚Åä ÍùØfessores eiÍù∞ sepe oretenÍù∞ testabant NaÃÉ semel ÍùØti ‚Åä\nipsa et confessores eius sepe oretenus testabantur. Nam semel conti\nipsia et confessores eius sepe oretenus testabantur Nam semel conti e\nipsa et confessores eius sepe oretenus testabantur Nam semel conti-\n\n\ngit quod cum ipsa quadam die equataret in equo itermerando ad suum\nsit quod cum ipsa quadam die equitaret in equo Itermerando ad suum\ngit Íùô cuÃÉ ipÃÉa quada die cqÕ•taret iÃÉ equo itmerando ad suuÃÉ\nEgit quod cum ipsa quadam die equitaret in equo itinerando ad suum\ngit quo cu ipsa quadea die equataret in equo itmerando ad suum\n-git quod cum ipsa quadam die equitaret in equo itinerando ad suum\n\n\ncastrum wadzsta plribus fralidibus cum ea equitantibus sociata tunc illa\ncastrum Wadzsten pluribus frantibus cum ea equitantibus sociata, tunc illa\ncastruÃÉ wadste p≈ÇibÍù∞ fraÃÉliaiÃæbÍù∞ cuÃÉ ea eqÃætaÃÉtibÍùØ sociÃæata. tuÃÉc illa\nca≈øtiuÃÑ wadÍù´ stÍù≠ plÃÑibro faÃÑlicaibÍù∞ cuÃÑ ea eÍùótciÃÑti bo ≈øociata. tuÃÑc illa\ncastum wadstuis plibs frailiaribus cum ea cstanti bius socirata. tumc illa\ncastrum wadzsteni pluribus familiaribus cum ea equitantibus sociata tunc illa\n\n\nsic eutando per viam Incep orando ad Deum erige mentem suam\nsic equitando per viam. Incept orando ad Deum erigeretur mentem suam.\nsic eqÃætando Íùë uiaÃÉ. IncepÕ≠ orando ad deuÃÉ crige Õ® menteÃÉ suam.\nsic equitando per viciis incepto orando ad deum eriget mentem suam\nsic eostando pruia. inceps orando ad deum erigeus mentem suam\nsic equitando per viam Incepit orando ad deum erigere mentem suam\n\n\nque illico rapta fuit in spiritu etibat quai extra se alienata a\nque illico rapta fuit in spiritu et ibat quam extra extit se alienata a\nque illico rapta fuit iÃÉ spuÃÉ. ‚Åä ibat qÕ£i extÃæ se alienata a\nque illico rapta fuit iÃÉ spirituÃÑ. ‚Åä ibat qÕ£i ext se alienata a\nque illico rapta fuit in sptiu. Rwat quam exter se alienata a\nque illico rapta fuit in spiritu et ibat quasi extra se alienata a"
  },
  {
    "objectID": "posts/2025-06-11-from-parchment-to-pixel/index.html#references",
    "href": "posts/2025-06-11-from-parchment-to-pixel/index.html#references",
    "title": "From Parchment to Pixels: Testing HTR for Medieval Latin Manuscripts at KBLab",
    "section": "References",
    "text": "References\n\n\nB√∂ckerman, Robin. 2025. ‚ÄúKungliga Bibliotekets Latinska Medeltida Handskrifter : Proveniens Och Katalogisering.‚Äù In, 11‚Äì34. Uppsala universitet. https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-556596.\n\n\nB√∂rjeson, Love, Chris Haffenden, Martin Malmsten, Fredrik Klingwall, Emma Rende, Robin Kurtz, Faton Rekathati, Hillevi H√§ggl√∂f, and Justyna Sikora. 2024. ‚ÄúTransfiguring the Library as Digital Research Infrastructure: Making KBLab at the National Library of Sweden.‚Äù College & Research Libraries 85 (4): 564. https://doi.org/10.5860/crl.85.4.564.\n\n\nLi, Minghao, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. 2021. ‚ÄúTrOCR: Transformer-Based Optical Character Recognition with Pre-Trained Models.‚Äù arXiv.org. https://arxiv.org/abs/2109.10282v5.\n\n\nNockels, Joseph, Paul Gooding, and Melissa Terras. 2024. ‚ÄúThe Implications of Handwritten Text Recognition for Accessing the¬†Past at Scale.‚Äù Journal of Documentation 80 (7): 148‚Äì67. https://doi.org/10.1108/JD-09-2023-0183.\n\n\nRedmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2015. ‚ÄúYou Only Look Once: Unified, Real-Time Object Detection.‚Äù arXiv.org. https://arxiv.org/abs/1506.02640v5.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz Kaiser, and Illia Polosukhin. 2017. ‚ÄúAttention Is All You Need.‚Äù In Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html."
  }
]